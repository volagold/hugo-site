<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Better Transformers | lifei.ai | AI blogs</title><meta name=keywords content="llm,attention,transformer,llama"><meta name=description content="In this post I will walk through the transformer layer and several improvements over this architecture that are commonly employed in many popular open source large language models (LLMs) today, for example Llama. Discussed include SwiGLU and RMSNorm layers, RoPE and ALiBi position embeddings; and finally Flash Attention for scaling attention calculation to long sequences. We will use Llama source code as example implementation, and toward the end I&rsquo;ll go through the rest of Llama&rsquo;s source code."><meta name=author content="Fei Li"><link rel=canonical href=https://lifei.ai/posts/2023-08-31-better-transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.415e3a8d51db2ff1dd1daeb6ed47da1d29ea7215172df9a95da9a546c6c5ba84.css integrity="sha256-QV46jVHbL/HdHa627UfaHSnqchUXLfmpXamlRsbFuoQ=" rel="preload stylesheet" as=style><link rel=icon href=https://lifei.ai/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lifei.ai/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lifei.ai/favicon-32x32.png><link rel=apple-touch-icon href=https://lifei.ai/apple-touch-icon.png><link rel=mask-icon href=https://lifei.ai/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet type=text/css href=/hugo-cite.css><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.css integrity="sha512-mQwom8Ns4op+H29oDkD/LXO/OsXPvCFfkgZkFAVrhhePzRLU8NUI3Nkm43NhWUSmj3p5Cca2HTEkMQmXQRwDQQ==" crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/mhchem.min.js integrity=sha384-F2ptQFZqNJuqfGGl28mIXyQ5kXH48spn7rcoS0Y9psqIKAcZPLd1NzwFlm/bl1mH crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin=anonymous referrerpolicy=no-referrer onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-QV43D2GXK2"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QV43D2GXK2",{anonymize_ip:!1})}</script><meta property="og:title" content="Better Transformers"><meta property="og:description" content="In this post I will walk through the transformer layer and several improvements over this architecture that are commonly employed in many popular open source large language models (LLMs) today, for example Llama. Discussed include SwiGLU and RMSNorm layers, RoPE and ALiBi position embeddings; and finally Flash Attention for scaling attention calculation to long sequences. We will use Llama source code as example implementation, and toward the end I&rsquo;ll go through the rest of Llama&rsquo;s source code."><meta property="og:type" content="article"><meta property="og:url" content="https://lifei.ai/posts/2023-08-31-better-transformers/"><meta property="og:image" content="https://lifei.ai/imgs/transformers/cover.jpeg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-31T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-31T00:00:00+00:00"><meta property="og:site_name" content="lifei.ai | AI blogs"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://lifei.ai/imgs/transformers/cover.jpeg"><meta name=twitter:title content="Better Transformers"><meta name=twitter:description content="In this post I will walk through the transformer layer and several improvements over this architecture that are commonly employed in many popular open source large language models (LLMs) today, for example Llama. Discussed include SwiGLU and RMSNorm layers, RoPE and ALiBi position embeddings; and finally Flash Attention for scaling attention calculation to long sequences. We will use Llama source code as example implementation, and toward the end I&rsquo;ll go through the rest of Llama&rsquo;s source code."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lifei.ai/posts/"},{"@type":"ListItem","position":2,"name":"Better Transformers","item":"https://lifei.ai/posts/2023-08-31-better-transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Better Transformers","name":"Better Transformers","description":"In this post I will walk through the transformer layer and several improvements over this architecture that are commonly employed in many popular open source large language models (LLMs) today, for example Llama. Discussed include SwiGLU and RMSNorm layers, RoPE and ALiBi position embeddings; and finally Flash Attention for scaling attention calculation to long sequences. We will use Llama source code as example implementation, and toward the end I\u0026rsquo;ll go through the rest of Llama\u0026rsquo;s source code.","keywords":["llm","attention","transformer","llama"],"articleBody":"In this post I will walk through the transformer layer and several improvements over this architecture that are commonly employed in many popular open source large language models (LLMs) today, for example Llama. Discussed include SwiGLU and RMSNorm layers, RoPE and ALiBi position embeddings; and finally Flash Attention for scaling attention calculation to long sequences. We will use Llama source code as example implementation, and toward the end I’ll go through the rest of Llama’s source code.\nTransformers we can think of a transformer layer as a cube. The transformation preserves the shape of the input tensor. Each input vector will be updated according to its interactions with other vectors. Image source: pinterest\nA Transformer block consists of an attention layer, two normalization layers, residual connections and a feedforward layer. Many models like Llama are usually made of 32 transformer blocks stacked together, plus normalization and linear projection at the end. A transformer layer is a “cube”, meaning that the input and the output have the same shape: N vectors in, N vectors out.\nFor language modeling (next-token prediction), each output vector $i=1,\\ldots,N$ will be mapped to an unnormalized logit vector with length vocab_size that represents the predicted distribution over the next token, given the attention with previous tokens. During training, the logit for ground truth will be maximized for each token in the sentence. For example, with “I eat apple with happiness”, we will maximize “eat” given “I”, maximize “apple” given “I eat”, etc. In inference though, only the final vector (\"out[-1]\") in the output tensor is needed, and we map the final output vector to logits to predict the next token.\nAmong transformer’s components, the attention layer is the most important one.\nThe attention layer As with a transformer layer, we can also think of an attention layer as a cube, because the output of an attention layer has the same shape as its input: it takes a sequence of embedding vectors as input, and output an updated version of those input embeddings, where each updated vector now attends to all other vectors in the sequence (including itself), or in the case of autoregressive modeling, attends to all previous vectors in the sequence (including itself).\nAn attention layer has three matrices as parameters: query matrix, key matrix, and value matrix, each has shape $d\\times d$, where $d$ is the token embedding dimension. Let’s take $d=2$, so that we can see the essence more clearly. In this case, we have three $2\\times2$ matrices, for a total of 12 parameters. We have:\nthe 2x2 query matrix $Q$ transforms any input vector $v\\in\\mathbb{R}^2$ to a query vector $q_v\\in \\mathbb{R}^2$. the 2x2 key matrix $K$ transforms any input vector to a key vector in $\\mathbb{R}^2$. the 2x2 query matrix $V$ transforms any input vector to a value vector in $\\mathbb{R}^2$. Let’s walk through the attention calculation. The input is a sequence of $N$ tokens, each embedded as a 2d vector. The input tensor is then an $N\\times2$ matrix. Apply the three matrix multiplications, we get\nNx2 query tensor, Nx2 key tensor, Nx2 value tensor, where each row is a 2d embedding vector.\nFor each vector, we want to update itself as a weighted sum of all the vectors in the sequence (including itself), weighted by cosine similarity (dot product).\n$$v = \\sum\\alpha_jv_j$$ $$\\alpha_j =v_i \\cdot v_j \\quad\\Rightarrow\\quad \\alpha_j=e^{v_i \\cdot v_j}/\\sum e^{v_i \\cdot v_j}\\,\\text{(normalize)}$$\nThe query tensor (Nx2) times the transpose of the key tensor (2xN) is an NxN matrix of dot products between each pair of vector in the sequence of length N.\nAttention calculation - query and key tensor product to score matrix. Image source: Fei Li (author)\nEach row is a list of similarity measures between the token $i$ with all other tokens in the sequence. If we don’t want token $i$ to attend to subsequent tokens, then we change those similarity scores to $-\\infty$, since $e^{-\\infty} = 0$. Otherwise, as in BERT, we leave them unchanged. Applying softmax row-wise, we get normalized weights for each token $i$.\nNote that it is in this sense that the attention layer can handle sequences of arbitrary length: the N just gets larger, the score matrix gets larger (grow O(N^2)), though there are only three 2x2 parameter matrices Q, K and V for a total of 12 parameters.\nNote that softmax per se is not a linear operation. For an input vector $v$, $\\mathrm{sm}(v)$ apply $e^{(\\cdot)}$ to each element in $v$ and then normalize them by the sum of the exponentials. It is not true that $\\mathrm{sm}(v + w) = \\mathrm{sm}(v) + \\mathrm{sm}(w)$.\nAfter that, we just multiply the score matrix with the value tensor. Each row $i$ in the output is then an updated version of token $i$’s’ embedding.\nAttention calculation - multiply score matrix with value tensor to get updated embeddings. Image source: Fei Li (author)\nSo this is the vanilla attention. In practice there are several additional variants:\nSince we multiply an $N\\times N$ square matrix with the $N\\times d_v$ value tensor, the dimension of the value vectors $d_v$ can be different, i.e. they don’t have to be 2, but can be 3, 4, or whatever. This means the value matrix $V$ can have shape $2\\times d_v$, that maps each 2d token embedding to a $d_v$-dimensional vector. The queriy and key vectors though, should have the same dimension for dot product. The attention layer we just illustrated is called single head attention, with one 2x2 key matrix, one 2x2 query matrix and one 2x2 value matrix as parameters. Similar to mapping an image with 3 channels to multiple channels with multiple kernels in convolutional layer in CNN, one may want to use multiple Q, K and V matrices to perform the attention calculation multiple times and concatenate the output from multiple calculations, in order to capture different aspects of an input sequence. This is called “multi-head attention”. It is also called self-attention because each token attends to other tokens within the same sequence. If, for example, the key vectors come from outside sources, for example image embeddings obtained from an image encoder, then it is called “cross attention”. Better Transformers SwiGLU layer SwiGLU ( Citation: Shazeer, 2020 Shazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202. ) is an improvement over the feed forward layer in transformers. A feed forward layer (also called a fully connected (FC) layer, or multi-layer perceptron (MLP)) is defined as\n$$ \\mathrm{FFN}(x, W_1, W_2, b_1, b_2) = W_2f(W_1x + b_1) + b_2, $$\nwhere $f$ is the nonlinear activation function applied element-wise to its input vector. The most commonly used one is the ReLU activation function $f(x) = \\max(0, x)$.\nThe Swish function ( Citation: Ramachandran, Zoph \u0026 al., 2017 Ramachandran, P., Zoph, B. \u0026 Le, Q. (2017). Searching for activation functions. arXiv preprint arXiv:1710.05941. ) , also called Sigmoid Linear Unit (SiLU), is defined as $f_\\beta(x) = x\\cdot\\sigma(\\beta x)$, where $\\sigma(x)$ is the sigmoid function\n$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}. $$\nAs $\\beta\\to0$, the function approaches the linear function $\\frac{1}{2}x$. As $\\beta\\to\\infty$, $\\sigma(x)$ approaches either $0$ or $1$, so the Swish function approaches the ReLU function. It is common to set $\\beta=1$. The most unique aspect of the Swish function is the non-monotonic “bump” at $x\u003c0$.\nSwish functions and their derivatives. Image source: Fei Li (author)\nGated Linear Units (GLU) refers to multiplying the vector coming out of the nonlinear activation function $f(W_1x)$ by a linear transformation of the input vector $x$, in element-wise fashion. This is denoted as\n$$ f(W_1x) \\otimes W_3x. $$\nNow, the SwiGLU layer is\n(1) feed forward layer without bias, plus\n(2) GLU, plus\n(3) the Swish function $f(x)=x\\cdot\\sigma(x)$ as activation function:\n$$ \\mathrm{SwiGLU}(x,W_1, W_2, W_3) = W_2(\\mathrm{Swish}_1(W_1x) \\otimes W_3x) $$\nBelow is the implementation of SwiGLU layer from Llama. I have deleted some non-essential code so that we can focus on the main part.\n1 2 3 4 5 6 7 8 9 class FeedForward(nn.Module): def __init__(self, dim: int, hidden_dim: int): super().__init__() self.w1 = ColumnParallelLinear(dim, hidden_dim, bias=False, ...) self.w2 = RowParallelLinear(hidden_dim, dim, bias=False, ...) self.w3 = ColumnParallelLinear(dim, hidden_dim, bias=False, ...) def forward(self, x): return self.w2(F.silu(self.w1(x)) * self.w3(x)) The code is very straightforward. The ColumnParallelLinear and RowParallelLinear layers are from the fairscale library that is designed for high performance and parallel training, and here we can simply regard them as linear layers (fully connected layers). The PyTorch F.silu function stands for “Sigmoid Linear Unit (SiLU)”, and it is exactly the Swish function $x\\cdot\\sigma(x)$ with $\\beta=1$.\nRMSNorm layer Neural network training is sensitive to parameter initialization and hyperparameter tuning. Batch normalization ( Citation: Ioffe \u0026 Szegedy, 2015 Ioffe, S. \u0026 Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. pmlr. ) was proposed to make the training process more robust and less dependent on such hyperparameter tuning. The method can make training converge faster, thus reducing training time. When it was first proposed it achieved the best accuracy on ImageNet classification task. For each mini-batch, batch normalization first calculates mean and variance statistics over the mini-batch, then subtracts the mean and scales by the variance for each data point in the mini-batch. We can represent the batch normalization layer as\n$$ \\mathrm{BN}(x) = \\frac{x - \\mathbb{E}_{x\\sim\\mathcal{B}}x}{\\sqrt{\\mathrm{var}(x) + \\epsilon}}, $$\nwhere the mean and variance statistics are calculated over a batch $\\mathcal{B}$.\nLayer normalization ( Citation: Ba, Kiros \u0026 al., 2016 Ba, J., Kiros, J. \u0026 Hinton, G. (2016). Layer normalization. arXiv preprint arXiv:1607.06450. ) was first proposed to apply the same idea to recurrent neural networks (RNN), where an layer may take only one input vector at a time. Layer normalization is “orthogonal” to batch normalization. Instead of looking at the data distribution across a batch, it calculates mean and variance statistics across all dimensions of an input vector, and normalize each dimension with the calculated mean and variance. Layer normalization can be represented as\n$$ \\mathrm{LN}(x) = \\frac{x - \\mathbb{E}x_j}{\\sqrt{\\mathrm{var}(x_j) + \\epsilon}}g, $$\nwhere the mean and variance statistics are calculated over $x=(x_1,\\ldots, x_j, \\ldots, x_n)\\in\\R^n$. $g\\in\\R^n$ is a trainable parameter used for scaling that is set to $(1,\\ldots,1)$ at the beginning.\nRooted Mean Square Normalization (RMSNorm) ( Citation: Zhang \u0026 Sennrich, 2019 Zhang, B. \u0026 Sennrich, R. (2019). Root mean square layer normalization. Advances in Neural Information Processing Systems, 32. ) proposed to focusing only on scaling invariance:\n$$ \\mathrm{RMSNorm}(x) = \\frac{x}{\\|x\\|/\\sqrt{n}}g, $$\nwhere $\\|x\\| = \\sqrt{x_1^2 + \\cdots + x_n^2}$. The output is thus confined to a sphere with radius $\\sqrt{n}$.\nIn Llama, RMSNorm is implemented as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 class RMSNorm(nn.Module): def __init__(self, dim: int, eps: float = 1e-6): super().__init__() self.eps = eps self.weight = nn.Parameter(torch.ones(dim)) def _norm(self, x): return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) def forward(self, x): output = self._norm(x) return output * self.weight x.pow(2).mean(-1) computes the squared norm of $x$. torch.rsqrt computes $y = 1/\\sqrt{x}$, the reciprocal square root. The weight parameter tensor has the same dimension as input $x$, and is initialized to $(1,\\ldots,1)$.\nRotary Position Embedding (RoPE) When modeling a token’s dependencies over previous tokens in a sentence, an issue to consider is how to encode the order of the sequence. Transformer (or self-attention) models each word vector as a weighted sum over all previous word vectors in a sequence. To see the issue of token positioning, let’s briefly review the self-attention layer. Let $(x_1,\\ldots,x_N)$ be a sequence of word vectors, and suppose we want to predict the next vector $x$. The self-attention calculation can be summarized as the following equation: $$ f(x) = \\frac{\\exp(q_x\\cdot k_1)}{\\sum_{n=1}^N\\exp(q_x\\cdot k_n)}v_1 + \\cdots + \\frac{\\exp(q_x\\cdot k_N)}{\\sum_{n=1}^N\\exp(q_x\\cdot k_n)}v_N $$ where $$ \\begin{align*} q_x \u0026= W^qx,\\newline k_i \u0026= W^kx_i, i=1,\\ldots, N\\newline v_i \u0026= W^vx_i, i=1, \\ldots, N \\end{align*} $$ and $W^q, W^k, W^v$ are three weight matrices to be learned. Since the addition operation is commutative, We can see that arbitrarily permuting the sequence $(x_1,\\ldots,x_N)$ would not affect the output, which is not desired. So we have to somehow encode position information into either the embedded vector $x_i$ or the query, key and value vectors. In the original Transformer paper ( Citation: Vaswani, Shazeer \u0026 al., 2017 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, Ł. \u0026 Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. ) , the authors proposed adding absolute position encoding to each input vector: $$ x_i = x_i + p_i $$ where $$ \\begin{cases} p_{i, 2t}\u0026= \\sin(i/10000^{2t/d}),\\newline p_{i, 2t+1}\u0026= \\cos(i/10000^{2t/d}). \\end{cases} $$ Note that both sine and cosine functions have range in $[-1, 1]$, so each value in each position vector $p_i$ is also confined within $[-1, 1]$. The large number 10000 was chosen in order to encompass large context lengths, but theoretically, the uniqueness of the position vectors is bounded, since we have the following two basic trigonometric properties $$ \\sin(x + 2k\\pi) = \\sin(x) $$ and $$ \\cos(x + 2k\\pi) = \\cos(x). $$ Let’s denote the large scaling factor in the denominator by $S=10000$, and choose a position $m$. Then the position vector for $m$ will be the same for the word that is $2\\pi S$ far away, since $$ \\sin\\left(\\frac{m + 2\\pi\\cdot S}{S}\\right) = \\sin\\left(\\frac{m}{S} + 2\\pi\\right) = \\sin\\left(\\frac{m}{S}\\right). $$ This means the maximum context length within which the position vector is unique for each position is around 62K. Beyond that, the position vectors will repeat. 62K is a very large length even according to today (2023)’s standard. Larger sequence lengths put significantly more pressure on GPU memories when using the vanilla Transformer, as we will see below when we discuss Flash Attention.\nRotary Position Embedding. Image source: Fei Li (author)\nRotary position embedding ( Citation: Su, Lu \u0026 al., 2021 Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B. \u0026 Liu, Y. (2021). Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864. ) is an alternative to additive position encoding. It is multiplicative, instead of additive. And it applies to the key and query vectors, rather than the input vectors. Let’s first see it in 2D case. Suppose we embedded each word token to a 2d vector, and calculated the query and key vectors. The RoPE method rotates each query or key vector with a rotation matrix. Take the query vector $q = (q_1, q_2)^T\\in\\mathbb{R}^2$ as an example:\n$$\\tag{🌟} \\hat{q} = \\begin{pmatrix}\\cos m\\theta \u0026 -\\sin m\\theta \\newline \\sin m\\theta \u0026 \\cos m\\theta\\end{pmatrix}\\begin{pmatrix}q_1\\newline q_2\\end{pmatrix} = \\begin{pmatrix} q_1\\cdot\\cos m\\theta - q_2\\cdot\\sin m\\theta \\newline q_1\\cdot\\sin m\\theta + q_2\\cdot\\cos m\\theta\\end{pmatrix} $$ where $m$ is the position and $\\theta$ is a preset small non-zero constant. Word vectors in different positions are rotated with different amounts according to $m\\theta$. To generalize to $d$ dimensions, where $d$ has to be an even number, the method applies rotation matrices to each 2 dimension pairs, for a total of $d/2$ pairs. Specifically, the transformation matrix is a block matrix where each block is a 2d rotation matrix: $$ \\begin{pmatrix} \\cos m\\theta_1 \u0026 -\\sin m\\theta_1 \u0026 0 \u0026 0 \u0026 \\cdots \u0026 0 \u0026 0 \\newline \\sin m\\theta_1 \u0026 \\cos m\\theta_1 \u0026 0 \u0026 0 \u0026 \\cdots \u0026 0 \u0026 0 \\newline \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \u0026 \\vdots \\newline 0 \u0026 0 \u0026 \\cdots \u0026 0 \u0026 0 \u0026 \\cos m\\theta_{d/2} \u0026 -\\sin m\\theta_{d/2} \\newline 0 \u0026 0 \u0026 \\cdots \u0026 0 \u0026 0 \u0026 \\sin m\\theta_{d/2} \u0026 \\cos m\\theta_{d/2}\\newline \\end{pmatrix} $$ and the angles are pre-defined with $\\{\\theta_i = 10000^{-2(i-1)/d}, i=1,\\ldots,d/2\\}$.\nImplementing RoPE as matrix multiplication takes lots of memories when $d$ is large. Representing each 2d sub-vector pair as a complex number will let us avoid matrix computation. Let’s re-focus on the 2d case. The 2d rotation matrix has close connections with arithmetics in the 2d complex plane. Recall the Euler’s formula $$ e^{i\\theta} = \\cos\\theta + i\\sin\\theta. $$ The core observation that is very useful for our implementation is the following: for a complex number $q_1 + q_2i$, we have $$ \\begin{split} (q_1 + q_2i)e^{im\\theta} \u0026= (q_1 + q_2i)(\\cos m\\theta + i\\sin m\\theta)\\newline \u0026= (q_1\\cdot\\cos m\\theta - q_2\\cdot\\sin m\\theta) + (q_1\\cdot\\sin m\\theta + q_2\\cdot\\cos m\\theta)i. \\end{split} $$ The real and imaginary parts of this complex number are exactly the first and second entry of the transformed query vector above (🌟)! So to get the rotary embedding for vector $q$, we first view it as a complex number, multiply it by $e^{im\\theta}$, then assemble the real and imaginary parts as the position encoded vector.\nSo, here are the implementation steps for one single 2d query/key vector $q$ with position $m$:\nView the 2d vector $q=(q_1, q_2)^T$ as a complex number $q_1 + q_2i$, where the first entry is the real part, and the second entry is the imaginary part. In PyTorch, this is 1 q = torch.view_as_complex(q) Multiply the vector by $e^{im\\theta}$. In PyTorch, $e^{im\\theta}$ is torch.polar(1, mθ). So we do 1 q = q * torch.polar(1, mθ) Get the real part and the imaginary part of this complex number. In PyTorch, this is 1 q_out = torch.view_as_real(q) The 2d implementation above showed the essence of RoPE. If you already understand the 2d case, then it is now easy to understand the code for general case. Here are the steps for $d$ dimensional query/key vectors , for all positions $m=0,\\ldots,N-1$, with reference to Llama source code. The input will have dimension (batch_size, seqlen, n_heads, head_dim).\ndim meaning 0 batch size 1 sequence length 2 number of heads 3 head dimension View each vector as a group of $d/2$ complex numbers. View dimension $(1, 2)$ as a complex number, dimension $(3, 4)$ as a complex number……and so on. torch.view_as_complex expects the last dimension of its input to be of size 2, so we first reshape the input tensor as a list of 2 number pairs 1 q.reshape(*q.shape[:-1], -1, 2) then get the list of complex numbers via\n1 q_ = torch.view_as_complex(q.reshape(*q.shape[:-1], -1, 2)) Multiply the first complex number by $e^{im\\theta_1}$, multiply the second complex number by $e^{im\\theta_2}$, and so on, until multiply the final complex number by $e^{im\\theta_{d/2}}$. In llama source code, the complex numbers $\\{e^{im\\theta_1},e^{im\\theta_2},\\ldots,e^{im\\theta_{d/2}}\\}$ for all $m=0,\\ldots,N-1$ are prepared by 1 2 3 4 5 6 def compute_exps(dim: int, N: int): thetas = 1.0 / (10000.0 ** (torch.arange(0, dim, 2) / dim)) m = torch.arange(N, device=thetas.device) angles = torch.outer(m, thetas).float() exps = torch.polar(torch.ones_like(angles), angles) # complex64 return exps To avoid potential confusions, I have renamed several variables so as to match the corresponding variable names in the original paper. Now let’s examine the this function line by line:\n(1)\n1 thetas = 1.0 / (10000.0 ** (torch.arange(0, dim, 2) / dim)) This line prepares all the thetas $\\{\\theta_i = 10000^{-2(i-1)/d}, i=1,\\ldots,d/2\\}$. The dimension of the output tensor thetas will be $d/2$, half the dimension of input $x$.\n(2)\n1 m = torch.arange(end, device=thetas.device) This line prepares all the positions $(0, 1, \\ldots, N-1)$.\n(3)\n1 angles = torch.outer(m, thetas).float() This line prepares all the angles $m\\theta_i$, for all $m$ and all $i$. For two tensors $a$ and $b$ with size $N$ and $d/2$, torch.outer(a, b) is $a^Tb$, so the output has dimension $N\\times(d/2)$.\n(4)\n1 exps = torch.polar(torch.ones_like(angles), angles) # complex64 This line computes $\\{e^{im\\theta_i}\\}$, for all $m$ and all $\\theta_i$. Thus the output of the compute_exps function is a matrix, where each row is $\\{e^{im\\theta_1},e^{im\\theta_2},\\ldots,e^{im\\theta_{d/2}}\\}$ for a particular position $m$, for all positions $m=0,\\ldots,N-1$.\nSuppose now we have prepared a query vector q_ as a list of complex numbers and we also have complex exponentials as exps. We simply do\n1 q_ * exps to multiply them together.\nFor each 2d dimension pair $(1, 2), (3,4), \\ldots, (d-1, d)$, we have computed a complex number $q\\cdot e^{im\\theta}$, for a total number of $d/2$. Now we get the real and imaginary part of each complex number, and stack all of them together to get the output. torch.view_as_real is the inverse operation of torch.view_as_complex. That means the last dimension of the output from torch.view_as_real will be 2. That’s why we need to call .flatten(3) to flatten the last dimension (remember from the input dimension table above that 3 is the last input shape position). 1 2 3 4 5 6 7 8 9 10 11 def apply_rotary_emb( q: torch.Tensor, k: torch.Tensor, exps: torch.Tensor, ) -\u003e Tuple[torch.Tensor, torch.Tensor]: q_ = torch.view_as_complex(q.float().reshape(*q.shape[:-1], -1, 2)) k_ = torch.view_as_complex(k.float().reshape(*k.shape[:-1], -1, 2)) exps = reshape_for_broadcast(exps, q_) q_out = torch.view_as_real(q_ * exps).flatten(3) k_out = torch.view_as_real(k_ * exps).flatten(3) return q_out, k_out We mention that, although RoPE is claimed to be a relative position embedding method, meaning that the similarity measure $\\hat{q}_m\\cdot\\hat{k}_n$ between the position encoded query $\\hat{q}_m$ (with position $m$) and position encoded key $\\hat{k}_n$ (with position $n$) only depends on $m-n$, their relative position, not their absolute positions in a sentence, we see that when implementing RoPE we still have to apply rotations to each query vector and each key vector in a sequence according to their absolute positions $m$ and $n$ in the sequence, obtaining $\\hat{q}_m$ and $\\hat{k}_n$, before computing $\\hat{q}_m\\cdot\\hat{k}_n$. This is because, query and key vectors are needed to compute the attention matrix. In this sense, RoPE is not true relative position embedding.\nAlso, RoPE has long term decay property: the dot product of two position encoded vectors approaches zero as their distance increases. The assumption that a pair of long distance tokens should have less connection is not desirable in light of LLM applications to long texts, e.g. contracts, legal documents, and financial reports. For example, users could give an instruction at the end of a prompt to extract some information from a long piece of text, where such information could be located right at the very beginning of the text. The long term decay property could lead to degraded performance in such cases.\nALiBi position embedding Attention with Linear Biases (ALiBi) ( Citation: Press, Smith \u0026 al., 2021 Press, O., Smith, N. \u0026 Lewis, M. (2021). Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409. ) is an even simpler method to encode position information.\nALiBi positional embedding. Image source: paper\n$m$ is a fixed scalar and is not learned. Ignoring $m$ for a second, the above figure has clear illustration of the method: biases $(-(N-1), \\ldots, -2, -1, 0)$ are added to query and key dot products before applying softmax. For example, $-5$ is added to $q_5\\cdot k_1$, $-1$ is added to $q_5\\cdot k_4$, no bias is added to $q_5\\cdot k_5$, and so on. There is no other positional encoding added elsewhere. Clearly, as distance gets larger, the negative bias will get large, and $\\exp\\{q\\cdot k - N\\}$ will decay to $0$ very fast as $N\\to\\infty$, resulting in 0 score and thus no lookups to distant tokens when generating the next token. As for RoPE, this property may not be desirable in real applications where one wants an LLM to process professional, long and complicated documents.\nFlash Attention Flash Attention ( Citation: Dao, Fu \u0026 al., 2022 Dao, T., Fu, D., Ermon, S., Rudra, A. \u0026 Ré, C. (2022). FlashAttention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35. 16344–16359. ) is a technique for speeding up attention computation on Nvidia GPUs for long sequences. There exists a memory heirarchy on GPU: high bandwidth memory (HBM) takes the bulk of GPU memory, but it is slower compared to SRAM, which is far smaller in size but is a dozen times faster than HBM.\nGPU memory hierarchy. Image source: Fei Li (author)\nFor a sequence with length $N$, the attention computation defined by the authors is $$ \\mathrm{O} = \\mathrm{dropout}(\\mathrm{softmax}(\\mathrm{mask}(QK^T)))V $$\nAttention is Bottlenecked by Memory Reads/Writes. Source: https://www.youtube.com/live/gMOAud7hZg4\nMemory usage grows quadratically with the sequence length $N$. As $N$ goes to thousands and beyond, the attention matrix $QK^T$ will occupy a lot of space in GPU memory. Masking, softmax and dropout computation all require reading and writing the $N\\times N$ attention matrix. It turns out that those memory reads/writes take up most of the computation time, much more than the actual matrix multiplication $QK^T$.\nTo reduce memory i/o, and to take advantage of SRAM’s fast speed, the authors proposed two techniques:\nTiling. This refers to loading partial blocks of the attention matrix from HBM to SRAM, computing attention on SRAM for such blocks, then finally concatenating the results by the correct scaling factors. This technique utilizes that fact that for a matrix $A = [A_1, A_2]$ that consists of two sub-matrices $A_1$ and $A_2$, the softmax of $A$ can be written as $$ \\mathrm{softmax}([A_1, A_2]) = [\\alpha\\cdot\\mathrm{softmax}(A_1), \\beta\\cdot\\mathrm{softmax}(A_2)] $$ for some scaling factors $\\alpha$ and $\\beta$, and $$ \\mathrm{softmax}([A_1, A_2])\\begin{bmatrix}V_1\\newline V_2\\end{bmatrix} = \\alpha\\cdot\\mathrm{softmax}(A_1)V_1 + \\beta\\cdot\\mathrm{softmax}(A_2)V_2, $$\nRecomputation. During the backward pass, the attention matrix is needed for computing gradients of the weights, and again, reading this large matrix from HBM memory would be slow. The recomputation technique is straightforward: do not store the $N\\times N$ attention matrix from forward pass (only store $N$ softmax normalizing factors), but recompute it in the backward pass. This incurs additional floating point operations (flops), but the runtime is reduced even with increased flops.\nAttention Standard FlashAttention GFLOPs 66.6 75.2 HBM R/W (GB) 40.3 4.4 Runtime (ms) 41.7 7.3 Source: ( Citation: Dao, Fu \u0026 al., 2022 Dao, T., Fu, D., Ermon, S., Rudra, A. \u0026 Ré, C. (2022). FlashAttention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35. 16344–16359. ) For the same sequence length $N$, Flash Attention allows faster computation compared with traditional attention. In other words, this means that for the same training time budget, one can train models that can deal with longer contexts. In this aspect, Flash Attention is very useful, because such models are very much needed in industrial applications.\nLlama source code We have already walked through some of Llama’s source code, including the feedforward layer, the RMSNorm layer, and the apply_rotary_emb function. In this last section, I will walk through the rest of Llama’s source code. First let’s look at the model.py file. We are left with three classes:\n1 2 3 4 5 6 7 8 class Attention(nn.Module): ... class TransformerBlock(nn.Module): ... class Transformer(nn.Module): ... Now let’s study them one by one.\nThe Attention layer The attention layer is implemented as follows. Again, I remind the reader that I have refactored the code and removed non-essential parts. What the Attention layer does is to multiply the query matrix with the key matrix, apply softmax to get the scores, then multiply the scores with values, and finally apply a linear layer to get the output.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Attention(nn.Module): def __init__(self, args: ModelArgs): super().__init__() self.head_dim = args.dim // args.n_heads self.wq = Linear(args.dim, args.n_heads * self.head_dim, bias=False, ...) self.wk = Linear(args.dim, args.n_heads * self.head_dim, bias=False, ...) self.wv = Linear(args.dim, args.n_heads * self.head_dim, bias=False, ...) self.wo = Linear(args.n_heads * self.head_dim, args.dim, bias=False, ...) def forward( self, x: torch.Tensor, start_pos: int, exps: torch.Tensor, mask: Optional[torch.Tensor], ): xq, xk, xv = self.wq(x), self.wk(x), self.wv(x) xq, xk = apply_rotary_emb(xq, xk, exps=exps) self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv keys = self.cache_k[:bsz, : start_pos + seqlen] values = self.cache_v[:bsz, : start_pos + seqlen] scores = torch.matmul(xq, keys) / math.sqrt(self.head_dim) if mask is not None: scores = scores + mask # (bs, n_heads, seqlen, cache_len + seqlen) scores = F.softmax(scores.float(), dim=-1) output = torch.matmul(scores, values) # (bs, n_heads, seqlen, head_dim) output = output.transpose(1, 2) return self.wo(output) The TransformerBlock layer A TransformerBlock layer in Llama is:\nx ---\u003eRMSNorm ---\u003eAttention ------\u003eRMSNorm ---\u003eFeedForward ---\u003eout | | | | | | | | ·------------ + ------------· ·------------ + ------------· Different from the original transformer architecture, the normalization layer is placed at the beginning, rather than the end. Below is the simplified pseudocode implementation. The self.attention attribute is Attention layer, self.feed_forward is SwiGLU layer. self.ffn_norm and self.attention_norm are both RMSNorm layers.\n1 2 3 4 5 6 7 8 9 10 11 12 class TransformerBlock(nn.Module): def __init__(self, layer_id: int, args: ModelArgs): super().__init__() self.layer_id = layer_id self.head_dim = args.dim // args.n_heads def forward(self, x, start_pos): h = x + self.attention.forward( self.attention_norm(x), start_pos ) out = h + self.feed_forward.forward(self.ffn_norm(h)) return out The Transformer layer Finally, the Llama model is defined as a Transformer class. It is a for loop of TransformerBlocks. The last dimension of Transformer output is un-normalized probability scores over the vocabulary. It has size vocab_size. Different sampling methods make different use of the scores. The basic greedy sampling chooses the one with the largest output score. Top-k sampling samples from top k highest score tokens.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Transformer(nn.Module): def __init__(self, params: ModelArgs): super().__init__() self.tok_embeddings = Embedding(params.vocab_size, params.dim) self.layers = torch.nn.ModuleList() for layer_id in range(params.n_layers): self.layers.append(TransformerBlock(layer_id, params)) self.norm = RMSNorm(params.dim, eps=params.norm_eps) self.output = Linear(params.dim, params.vocab_size, bias=False) @torch.inference_mode() def forward(self, tokens: torch.Tensor, start_pos: int): h = self.tok_embeddings(tokens) for layer in self.layers: h = layer(h, start_pos) h = self.norm(h) output = self.output(h) return output The Transformer class above is the Llama model, which is pretty concise and clean. Having looked at the Llama model, now let’s look at how inference is implemented.\nGeneration There is one Llama class defined in generation.py. First, the build method loads weights from local paths, and initializes the Llama model. It is used in official examples example_chat_completion.py and example_text_completion.py to build a generator. The generator exposes two interfaces, text_completion and chat_completion. Both of the two are variants of the generate method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from llama.model import ModelArgs, Transformer from llama.tokenizer import Tokenizer class Llama: @staticmethod def build( ckpt_dir: str, tokenizer_path: str, max_seq_len: int, max_batch_size: int, ) -\u003e \"Llama\": tokenizer = Tokenizer(model_path=tokenizer_path) model = Transformer(model_args) model.load_state_dict(checkpoint, strict=False) return Llama(model, tokenizer) def __init__(self, model: Transformer, tokenizer: Tokenizer): self.model = model self.tokenizer = tokenizer def generate(self, ...): pass The core part of the generate method is the following lines of code: get the logits from model’s forward method, then sample the next token from the logits. As mentioned before, greedy sampling chooses the token with the largest output score; top-k sampling samples from tokens with k largest scores; and top-p sampling is top-k sampling where k summed probabilities exceeds a certain threshold, so that k is dynamically adjusted.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # ...... @torch.inference_mode() def generate( self, prompt_tokens: List[List[int]], max_gen_len: int, temperature: float = 0.6, top_p: float = 0.9, logprobs: bool = False, echo: bool = False, ) -\u003e Tuple[List[List[int]], ...]: params = self.model.params bsz = len(prompt_tokens) tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\") for cur_pos in range(min_prompt_len, total_len): logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos) if temperature \u003e 0: probs = torch.softmax(logits[:, -1] / temperature, dim=-1) next_token = sample_top_p(probs, top_p) else: next_token = torch.argmax(logits[:, -1], dim=-1) Note that the temperature parameter appears as the denominator of the softmax function input. If temperature is zero, then we simply have greedy sampling. If temperature is small, this would magnify those logits that are already large, so that other tokens will have even slimmer chance of being selected. 1.0 is a neutral choice. At values larger than 1.0, temperature will reduce all the logits, reducing their differences after exponentials, so that sampling will be more random.\nText completion:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # ...... def text_completion( self, prompts: List[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None, ) -\u003e List[str]: prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts] generation_tokens, generation_logprobs = self.generate( prompt_tokens=prompt_tokens, max_gen_len=max_gen_len, temperature=temperature, top_p=top_p, ) return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens] Chat completion:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class Message(TypedDict): role: Role content: str Dialog = List[Message] B_INST, E_INST = \"[INST]\", \"[/INST]\" B_SYS, E_SYS = \"\u003c\u003e\\n\", \"\\n\u003c\u003e\\n\\n\" DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant...\" # ...... def chat_completion( self, dialogs: List[Dialog], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None, logprobs: bool = False, ) -\u003e List[Message]: if max_gen_len is None: max_gen_len = self.model.params.max_seq_len - 1 prompt_tokens = [] for dialog in dialogs: if dialog[0][\"role\"] != \"system\": dialog = [ { \"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT, } ] + dialog dialog = [ { \"role\": dialog[1][\"role\"], \"content\": B_SYS + dialog[0][\"content\"] + E_SYS + dialog[1][\"content\"], } ] + dialog[2:] assert all([msg[\"role\"] == \"user\" for msg in dialog[::2]]) and all( [msg[\"role\"] == \"assistant\" for msg in dialog[1::2]] ), ( \"model only supports 'system', 'user' and 'assistant' roles, \" \"starting with 'system', then 'user' and alternating (u/a/u/a/u...)\" ) dialog_tokens: List[int] = sum( [ self.tokenizer.encode( f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \", bos=True, eos=True, ) for prompt, answer in zip( dialog[::2], dialog[1::2], ) ], [], ) assert ( dialog[-1][\"role\"] == \"user\" ), f\"Last message must be from user, got {dialog[-1]['role']}\" dialog_tokens += self.tokenizer.encode( f\"{B_INST} {(dialog[-1]['content']).strip()} {E_INST}\", bos=True, eos=False, ) prompt_tokens.append(dialog_tokens) generation_tokens, generation_logprobs = self.generate( prompt_tokens=prompt_tokens, max_gen_len=max_gen_len, temperature=temperature, top_p=top_p, logprobs=logprobs, ) return [ {\"generation\": {\"role\": \"assistant\", \"content\": self.tokenizer.decode(t)}} for t in generation_tokens ] As with text_completion, chat_completion is also a wrapper around the generate method. Chat histories are concatenated to a string, before feeding into the generate method. To distinguish between users’ prompts and assistant’s answers, special tokens like [INST] and [/INST] are inserted:\n1 f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \" A pre-trained model is fine-tuned on chat data with such format, so that it should “recognize” such format in inference. Here is the description from the Llama 2 paper ( Citation: Touvron, Martin \u0026 al., 2023 Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S. \u0026 (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. ) :\nFor the fine-tuning process, each sample consists of a prompt and an answer. To ensure the model sequence length is properly filled, we concatenate all the prompts and answers from the training set. A special token is utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.\nself.tokenizer.encode will encode the input string to a list of integers. So the type of\n1 2 3 4 5 6 7 8 9 10 11 [ self.tokenizer.encode( f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \", bos=True, eos=True, ) for prompt, answer in zip( dialog[::2], dialog[1::2], ) ] will be list[list[int]], a list of integer lists. sum(lst, []) where lst is of type list[list[int]] will flatten the list (remove all inner lists), so the output of that will be list[int]. For example,\n1 2 In [1]: sum([[1,2,3], [4,5,6], [7,8,9]], []) Out[1]: [1, 2, 3, 4, 5, 6, 7, 8, 9] This dialog_tokens list (type list[int]) is then appended to prompt_tokens (type list[list[int]]), and finally this list is fed to self.generate method to get the generated tokens. This is how text_completion and chat_completion work under the hood.\nSummary In this post, I talked about various techniques for improving the transformer architecture, including SwiGLU, RMSNorm, Rotary Position Embedding (RoPE), ALiBi and Flash Attention. I have also walked through Llama’s source code. My post could help AI practitioners better understand LLMs’ behaviors as well as how to use them properly. With more open source LLMs coming out, this post could also help provide a direction for finding models that best suit various application scenarios.\nReferences ◎ Press, O., Smith, N. \u0026 Lewis, M. (2021). Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409. ◎ Ioffe, S. \u0026 Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. pmlr. ◎ Dao, T., Fu, D., Ermon, S., Rudra, A. \u0026 Ré, C. (2022). FlashAttention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35. 16344–16359. ◎ Ba, J., Kiros, J. \u0026 Hinton, G. (2016). Layer normalization. arXiv preprint arXiv:1607.06450. ◎ Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S. \u0026 (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. ◎ Zhang, B. \u0026 Sennrich, R. (2019). Root mean square layer normalization. Advances in Neural Information Processing Systems, 32. ◎ Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B. \u0026 Liu, Y. (2021). Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864. ◎ Shazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202. ◎ Ramachandran, P., Zoph, B. \u0026 Le, Q. (2017). Searching for activation functions. arXiv preprint arXiv:1710.05941. ◎ Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, Ł. \u0026 Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. ","wordCount":"6192","inLanguage":"en","image":"https://lifei.ai/imgs/transformers/cover.jpeg","datePublished":"2023-08-31T00:00:00Z","dateModified":"2023-08-31T00:00:00Z","author":{"@type":"Person","name":"Fei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://lifei.ai/posts/2023-08-31-better-transformers/"},"publisher":{"@type":"Organization","name":"lifei.ai | AI blogs","logo":{"@type":"ImageObject","url":"https://lifei.ai/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lifei.ai/ accesskey=h title="lifei.ai (Alt + H)"><img src=https://lifei.ai/f.png alt aria-label=logo height=40>lifei.ai</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://lifei.ai/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://lifei.ai/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://lifei.ai/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://lifei.ai/>Home</a>&nbsp;»&nbsp;<a href=https://lifei.ai/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Better Transformers</h1><div class=post-meta><span title='2023-08-31 00:00:00 +0000 UTC'>August 31, 2023</span>&nbsp;·&nbsp;30 min&nbsp;·&nbsp;Fei Li</div></header><figure class=entry-cover><img loading=eager src=https://lifei.ai/imgs/transformers/cover.jpeg alt="post cover"><p>source: midjourney</p></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#transformers>Transformers</a><ul><li><a href=#the-attention-layer>The attention layer</a></li></ul></li><li><a href=#better-transformers>Better Transformers</a><ul><li><a href=#swiglu-layer>SwiGLU layer</a></li><li><a href=#rmsnorm-layer>RMSNorm layer</a></li><li><a href=#rotary-position-embedding-rope>Rotary Position Embedding (RoPE)</a></li><li><a href=#alibi-position-embedding>ALiBi position embedding</a></li><li><a href=#flash-attention>Flash Attention</a></li></ul></li><li><a href=#llama-source-code>Llama source code</a><ul><li><a href=#the-attention-layer-1>The Attention layer</a></li><li><a href=#the-transformerblock-layer>The TransformerBlock layer</a></li><li><a href=#the-transformer-layer>The Transformer layer</a></li><li><a href=#generation>Generation</a></li></ul></li><li><a href=#summary>Summary</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>In this post I will walk through the transformer layer and several improvements over this architecture that are commonly employed in many popular open source large language models (LLMs) today, for example <a href=https://github.com/facebookresearch/llama>Llama</a>. Discussed include SwiGLU and RMSNorm layers, RoPE and ALiBi position embeddings; and finally Flash Attention for scaling attention calculation to long sequences. We will use Llama source code as example implementation, and toward the end I&rsquo;ll go through the rest of Llama&rsquo;s source code.</p><h2 id=transformers>Transformers<a hidden class=anchor aria-hidden=true href=#transformers>#</a></h2><figure class=align-center><img loading=lazy src=/imgs/transformers/cube.jpeg#center alt="we can think of a transformer layer as a cube. The transformation preserves the shape of the input tensor. Each input vector will be updated according to its interactions with other vectors. Image source: pinterest"><figcaption><p>we can think of a transformer layer as a cube. The transformation preserves the shape of the input tensor. Each input vector will be updated according to its interactions with other vectors. Image source: pinterest</p></figcaption></figure><p>A Transformer block consists of an attention layer, two normalization layers, residual connections and a feedforward layer. Many models like Llama are usually made of 32 transformer blocks stacked together, plus normalization and linear projection at the end. A transformer layer is a &ldquo;cube&rdquo;, meaning that the input and the output have the same shape: N vectors in, N vectors out.</p><p>For language modeling (next-token prediction), each output vector $i=1,\ldots,N$ will be mapped to an unnormalized logit vector with length <code>vocab_size</code> that represents the predicted distribution over the next token, given the attention with previous tokens. During training, the logit for ground truth will be maximized for each token in the sentence. For example, with &ldquo;I eat apple with happiness&rdquo;, we will maximize &ldquo;eat&rdquo; given &ldquo;I&rdquo;, maximize &ldquo;apple&rdquo; given &ldquo;I eat&rdquo;, etc. In inference though, only the final vector ("<code>out[-1]</code>") in the output tensor is needed, and we map the final output vector to logits to predict the next token.</p><p>Among transformer&rsquo;s components, the attention layer is the most important one.</p><h3 id=the-attention-layer>The attention layer<a hidden class=anchor aria-hidden=true href=#the-attention-layer>#</a></h3><p>As with a transformer layer, we can also think of an attention layer as a cube, because the output of an attention layer has the same shape as its input: it takes a sequence of embedding vectors as input, and output an updated version of those input embeddings, where each updated vector now attends to all other vectors in the sequence (including itself), or in the case of autoregressive modeling, attends to all previous vectors in the sequence (including itself).</p><p>An attention layer has three matrices as parameters: query matrix, key matrix, and value matrix, each has shape $d\times d$, where $d$ is the token embedding dimension. Let&rsquo;s take $d=2$, so that we can see the essence more clearly. In this case, we have three $2\times2$ matrices, for a total of 12 parameters. We have:</p><ul><li>the 2x2 query matrix $Q$ transforms <em>any</em> input vector $v\in\mathbb{R}^2$ to a query vector $q_v\in \mathbb{R}^2$.</li><li>the 2x2 key matrix $K$ transforms <em>any</em> input vector to a key vector in $\mathbb{R}^2$.</li><li>the 2x2 query matrix $V$ transforms <em>any</em> input vector to a value vector in $\mathbb{R}^2$.</li></ul><p>Let&rsquo;s walk through the attention calculation. The input is a sequence of $N$ tokens, each embedded as a 2d vector. The input tensor is then an $N\times2$ matrix. Apply the three matrix multiplications, we get</p><ul><li>Nx2 query tensor,</li><li>Nx2 key tensor,</li><li>Nx2 value tensor,</li></ul><p>where each row is a 2d embedding vector.</p><p>For each vector, we want to update itself as a weighted sum of all the vectors in the sequence (including itself), weighted by cosine similarity (dot product).</p><p>$$v = \sum\alpha_jv_j$$
$$\alpha_j =v_i \cdot v_j \quad\Rightarrow\quad \alpha_j=e^{v_i \cdot v_j}/\sum e^{v_i \cdot v_j}\,\text{(normalize)}$$</p><p>The query tensor (Nx2) times the transpose of the key tensor (2xN) is an NxN matrix of dot products between each pair of vector in the sequence of length N.</p><figure class=align-center><img loading=lazy src=/imgs/transformers/attention.png#center alt="Attention calculation - query and key tensor product to score matrix. Image source: Fei Li (author)"><figcaption><p>Attention calculation - query and key tensor product to score matrix. Image source: Fei Li (author)</p></figcaption></figure><p>Each row is a list of similarity measures between the token $i$ with all other tokens in the sequence. If we don&rsquo;t want token $i$ to attend to subsequent tokens, then we change those similarity scores to $-\infty$, since $e^{-\infty} = 0$. Otherwise, as in BERT, we leave them unchanged. Applying softmax row-wise, we get normalized weights for each token $i$.</p><blockquote><p>Note that it is in this sense that the attention layer can handle sequences of arbitrary length: the N just gets larger, the score matrix gets larger (grow O(N^2)), though there are only three 2x2 parameter matrices Q, K and V for a total of 12 parameters.</p></blockquote><blockquote><p>Note that softmax per se is not a linear operation. For an input vector $v$, $\mathrm{sm}(v)$ apply $e^{(\cdot)}$ to each element in $v$ and then normalize them by the sum of the exponentials. It is not true that $\mathrm{sm}(v + w) = \mathrm{sm}(v) + \mathrm{sm}(w)$.</p></blockquote><p>After that, we just multiply the score matrix with the value tensor. Each row $i$ in the output is then an updated version of token $i$&rsquo;s&rsquo; embedding.</p><figure class=align-center><img loading=lazy src=/imgs/transformers/attention2.png#center alt="Attention calculation - multiply score matrix with value tensor to get updated embeddings. Image source: Fei Li (author)"><figcaption><p>Attention calculation - multiply score matrix with value tensor to get updated embeddings. Image source: Fei Li (author)</p></figcaption></figure><p>So this is the vanilla attention. In practice there are several additional variants:</p><ol><li>Since we multiply an $N\times N$ square matrix with the $N\times d_v$ value tensor, the dimension of the value vectors $d_v$ can be different, i.e. they don&rsquo;t have to be 2, but can be 3, 4, or whatever. This means the value matrix $V$ can have shape $2\times d_v$, that maps each 2d token embedding to a $d_v$-dimensional vector. The queriy and key vectors though, should have the same dimension for dot product.</li><li>The attention layer we just illustrated is called single head attention, with one 2x2 key matrix, one 2x2 query matrix and one 2x2 value matrix as parameters. Similar to mapping an image with 3 channels to multiple channels with multiple kernels in convolutional layer in CNN, one may want to use multiple Q, K and V matrices to perform the attention calculation multiple times and concatenate the output from multiple calculations, in order to capture different aspects of an input sequence. This is called &ldquo;<strong>multi-head attention</strong>&rdquo;.</li><li>It is also called self-attention because each token attends to other tokens within the same sequence. If, for example, the key vectors come from outside sources, for example image embeddings obtained from an image encoder, then it is called &ldquo;<strong>cross attention</strong>&rdquo;.</li></ol><h2 id=better-transformers>Better Transformers<a hidden class=anchor aria-hidden=true href=#better-transformers>#</a></h2><h3 id=swiglu-layer>SwiGLU layer<a hidden class=anchor aria-hidden=true href=#swiglu-layer>#</a></h3><p>SwiGLU
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#swiglu><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Noam"><span itemprop=familyName>Shazeer</span></span>, <span itemprop=datePublished>2020</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shazeer</span>, <meta itemprop=givenName content="Noam">N.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>Glu variants improve transformer</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2002.05202</span></i>.</span>
</span></span>)</span>
is an improvement over the feed forward layer in transformers. A feed forward layer (also called a fully connected (FC) layer, or multi-layer perceptron (MLP)) is defined as</p><p>$$
\mathrm{FFN}(x, W_1, W_2, b_1, b_2) = W_2f(W_1x + b_1) + b_2,
$$</p><p>where $f$ is the nonlinear activation function applied element-wise to its input vector. The most commonly used one is the ReLU activation function $f(x) = \max(0, x)$.</p><p>The Swish function
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#swish><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Prajit"><span itemprop=familyName>Ramachandran</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Barret"><span itemprop=familyName>Zoph</span></span>
<em>& al.</em>, <span itemprop=datePublished>2017</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ramachandran</span>, <meta itemprop=givenName content="Prajit">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zoph</span>, <meta itemprop=givenName content="Barret">B.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Le</span>, <meta itemprop=givenName content="Quoc V">Q.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Searching for activation functions</span>.<i>
<span itemprop=about>arXiv preprint arXiv:1710.05941</span></i>.</span>
</span></span>)</span>
, also called Sigmoid Linear Unit (SiLU), is defined as $f_\beta(x) = x\cdot\sigma(\beta x)$, where $\sigma(x)$ is the sigmoid function</p><p>$$
\sigma(x) = \frac{1}{1 + e^{-x}}.
$$</p><p>As $\beta\to0$, the function approaches the linear function $\frac{1}{2}x$. As $\beta\to\infty$, $\sigma(x)$ approaches either $0$ or $1$, so the Swish function approaches the ReLU function. It is common to set $\beta=1$. The most unique aspect of the Swish function is the non-monotonic “bump” at $x&lt;0$.</p><figure class=align-center><img loading=lazy src=/imgs/transformers/swish.png#center alt="Swish functions and their derivatives. Image source: Fei Li (author)"><figcaption><p>Swish functions and their derivatives. Image source: Fei Li (author)</p></figcaption></figure><p>Gated Linear Units (GLU) refers to multiplying the vector coming out of the nonlinear activation function $f(W_1x)$ by a linear transformation of the input vector $x$, in element-wise fashion. This is denoted as</p><p>$$
f(W_1x) \otimes W_3x.
$$</p><p>Now, the SwiGLU layer is</p><p>(1) feed forward layer without bias, plus</p><p>(2) GLU, plus</p><p>(3) the Swish function $f(x)=x\cdot\sigma(x)$ as activation function:</p><p>$$
\mathrm{SwiGLU}(x,W_1, W_2, W_3) = W_2(\mathrm{Swish}_1(W_1x) \otimes W_3x)
$$</p><p>Below is the implementation of SwiGLU layer from Llama. I have deleted some non-essential code so that we can focus on the main part.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>w1</span> <span class=o>=</span> <span class=n>ColumnParallelLinear</span><span class=p>(</span><span class=n>dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>w2</span> <span class=o>=</span> <span class=n>RowParallelLinear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>w3</span> <span class=o>=</span> <span class=n>ColumnParallelLinear</span><span class=p>(</span><span class=n>dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>w2</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>silu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>w1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>w3</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>The code is very straightforward. The <code>ColumnParallelLinear</code> and <code>RowParallelLinear</code> layers are from the <a href=https://github.com/facebookresearch/fairscale>fairscale</a> library that is designed for high performance and parallel training, and here we can simply regard them as linear layers (fully connected layers). The PyTorch <code>F.silu</code> function stands for <a href=https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html>&ldquo;Sigmoid Linear Unit (SiLU)&rdquo;</a>, and it is exactly the Swish function $x\cdot\sigma(x)$ with $\beta=1$.</p><h3 id=rmsnorm-layer>RMSNorm layer<a hidden class=anchor aria-hidden=true href=#rmsnorm-layer>#</a></h3><p>Neural network training is sensitive to parameter initialization and hyperparameter tuning. Batch normalization
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#batchnorm><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Sergey"><span itemprop=familyName>Ioffe</span></span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Christian"><span itemprop=familyName>Szegedy</span></span>, <span itemprop=datePublished>2015</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ioffe</span>, <meta itemprop=givenName content="Sergey">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Szegedy</span>, <meta itemprop=givenName content="Christian">C.</span>
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name><i>Batch normalization: Accelerating deep network training by reducing internal covariate shift</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>pmlr</span></span>.</span>
</span></span>)</span>
was proposed to make the training process more robust and less dependent on such hyperparameter tuning. The method can make training converge faster, thus reducing training time. When it was first proposed it achieved the best accuracy on ImageNet classification task. For each mini-batch, batch normalization first calculates mean and variance statistics over the mini-batch, then subtracts the mean and scales by the variance for each data point in the mini-batch. We can represent the batch normalization layer as</p><p>$$
\mathrm{BN}(x) = \frac{x - \mathbb{E}_{x\sim\mathcal{B}}x}{\sqrt{\mathrm{var}(x) + \epsilon}},
$$</p><p>where the mean and variance statistics are calculated over a batch $\mathcal{B}$.</p><p>Layer normalization
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#layernorm><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jimmy Lei"><span itemprop=familyName>Ba</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jamie Ryan"><span itemprop=familyName>Kiros</span></span>
<em>& al.</em>, <span itemprop=datePublished>2016</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ba</span>, <meta itemprop=givenName content="Jimmy Lei">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kiros</span>, <meta itemprop=givenName content="Jamie Ryan">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Hinton</span>, <meta itemprop=givenName content="Geoffrey E">G.</span>
 
(<span itemprop=datePublished>2016</span>).
 <span itemprop=name>Layer normalization</span>.<i>
<span itemprop=about>arXiv preprint arXiv:1607.06450</span></i>.</span>
</span></span>)</span>
was first proposed to apply the same idea to recurrent neural networks (RNN), where an layer may take only one input vector at a time. Layer normalization is &ldquo;orthogonal&rdquo; to batch normalization. Instead of looking at the data distribution across a batch, it calculates mean and variance statistics across all <em>dimensions</em> of an input vector, and normalize each dimension with the calculated mean and variance. Layer normalization can be represented as</p><p>$$
\mathrm{LN}(x) = \frac{x - \mathbb{E}x_j}{\sqrt{\mathrm{var}(x_j) + \epsilon}}g,
$$</p><p>where the mean and variance statistics are calculated over $x=(x_1,\ldots, x_j, \ldots, x_n)\in\R^n$. $g\in\R^n$ is a trainable parameter used for scaling that is set to $(1,\ldots,1)$ at the beginning.</p><p>Rooted Mean Square Normalization (RMSNorm)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#rmsnorm><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Biao"><span itemprop=familyName>Zhang</span></span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Rico"><span itemprop=familyName>Sennrich</span></span>, <span itemprop=datePublished>2019</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Biao">B.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sennrich</span>, <meta itemprop=givenName content="Rico">R.</span>
 
(<span itemprop=datePublished>2019</span>).
 <span itemprop=name>Root mean square layer normalization</span>.<i>
<span itemprop=about>Advances in Neural Information Processing Systems</span>, 32</i>.</span>
</span></span>)</span>
proposed to focusing only on scaling invariance:</p><p>$$
\mathrm{RMSNorm}(x) = \frac{x}{\|x\|/\sqrt{n}}g,
$$</p><p>where $\|x\| = \sqrt{x_1^2 + \cdots + x_n^2}$. The output is thus confined to a sphere with radius $\sqrt{n}$.</p><p>In Llama, RMSNorm is implemented as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RMSNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>eps</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1e-6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_norm</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>rsqrt</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight</span>
</span></span></code></pre></td></tr></table></div></div><p><code>x.pow(2).mean(-1)</code> computes the squared norm of $x$. <code>torch.rsqrt</code> computes $y = 1/\sqrt{x}$, the reciprocal square root. The weight parameter tensor has the same dimension as input $x$, and is initialized to $(1,\ldots,1)$.</p><h3 id=rotary-position-embedding-rope>Rotary Position Embedding (RoPE)<a hidden class=anchor aria-hidden=true href=#rotary-position-embedding-rope>#</a></h3><p>When modeling a token&rsquo;s dependencies over previous tokens in a sentence, an issue to consider is how to encode the order of the sequence. Transformer (or self-attention) models each word vector as a weighted sum over all previous word vectors in a sequence. To see the issue of token positioning, let&rsquo;s briefly review the self-attention layer. Let $(x_1,\ldots,x_N)$ be a sequence of word vectors, and suppose we want to predict the next vector $x$. The self-attention calculation can be summarized as the following equation:
$$
f(x) = \frac{\exp(q_x\cdot k_1)}{\sum_{n=1}^N\exp(q_x\cdot k_n)}v_1 + \cdots + \frac{\exp(q_x\cdot k_N)}{\sum_{n=1}^N\exp(q_x\cdot k_n)}v_N
$$
where
$$
\begin{align*}
q_x &= W^qx,\newline
k_i &= W^kx_i, i=1,\ldots, N\newline
v_i &= W^vx_i, i=1, \ldots, N
\end{align*}
$$
and $W^q, W^k, W^v$ are three weight matrices to be learned. Since the addition operation is commutative, We can see that <strong>arbitrarily permuting the sequence $(x_1,\ldots,x_N)$ would not affect the output, which is not desired</strong>. So we have to somehow encode position information into either the embedded vector $x_i$ <em>or</em> the query, key and value vectors. In the original Transformer paper
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#transformer><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Ashish"><span itemprop=familyName>Vaswani</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Noam"><span itemprop=familyName>Shazeer</span></span>
<em>& al.</em>, <span itemprop=datePublished>2017</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Vaswani</span>, <meta itemprop=givenName content="Ashish">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shazeer</span>, <meta itemprop=givenName content="Noam">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Parmar</span>, <meta itemprop=givenName content="Niki">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Uszkoreit</span>, <meta itemprop=givenName content="Jakob">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jones</span>, <meta itemprop=givenName content="Llion">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gomez</span>, <meta itemprop=givenName content="Aidan N">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kaiser</span>, <meta itemprop=givenName content="Łukasz">Ł.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Polosukhin</span>, <meta itemprop=givenName content="Illia">I.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Attention is all you need</span>.<i>
<span itemprop=about>Advances in neural information processing systems</span>, 30</i>.</span>
</span></span>)</span>
, the authors proposed adding absolute position encoding to each input vector:
$$
x_i = x_i + p_i
$$
where
$$
\begin{cases}
p_{i, 2t}&= \sin(i/10000^{2t/d}),\newline
p_{i, 2t+1}&= \cos(i/10000^{2t/d}).
\end{cases}
$$
Note that both sine and cosine functions have range in $[-1, 1]$, so each value in each position vector $p_i$ is also confined within $[-1, 1]$. The large number 10000 was chosen in order to encompass large context lengths, but theoretically, the uniqueness of the position vectors is bounded, since we have the following two basic trigonometric properties
$$
\sin(x + 2k\pi) = \sin(x)
$$
and
$$
\cos(x + 2k\pi) = \cos(x).
$$
Let&rsquo;s denote the large scaling factor in the denominator by $S=10000$, and choose a position $m$. Then the position vector for $m$ will be the same for the word that is $2\pi S$ far away, since
$$
\sin\left(\frac{m + 2\pi\cdot S}{S}\right) = \sin\left(\frac{m}{S} + 2\pi\right) = \sin\left(\frac{m}{S}\right).
$$
This means the maximum context length within which the position vector is unique for each position is around 62K. Beyond that, the position vectors will repeat. 62K is a very large length even according to today (2023)&rsquo;s standard. Larger sequence lengths put significantly more pressure on GPU memories when using the vanilla Transformer, as we will see below when we discuss Flash Attention.</p><figure class=align-center><img loading=lazy src=/imgs/transformers/RoPE.jpeg#center alt="Rotary Position Embedding. Image source: Fei Li (author)"><figcaption><p>Rotary Position Embedding. Image source: Fei Li (author)</p></figcaption></figure><p>Rotary position embedding
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#rope><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jianlin"><span itemprop=familyName>Su</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Yu"><span itemprop=familyName>Lu</span></span>
<em>& al.</em>, <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Su</span>, <meta itemprop=givenName content="Jianlin">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, <meta itemprop=givenName content="Yu">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Pan</span>, <meta itemprop=givenName content="Shengfeng">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Murtadha</span>, <meta itemprop=givenName content="Ahmed">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wen</span>, <meta itemprop=givenName content="Bo">B.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Liu</span>, <meta itemprop=givenName content="Yunfeng">Y.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Roformer: Enhanced transformer with rotary position embedding</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2104.09864</span></i>.</span>
</span></span>)</span>
is an alternative to additive position encoding. It is multiplicative, instead of additive. And it applies to the key and query vectors, rather than the input vectors. Let&rsquo;s first see it in 2D case. Suppose we embedded each word token to a 2d vector, and calculated the query and key vectors. The RoPE method rotates each query or key vector with a rotation matrix. Take the query vector $q = (q_1, q_2)^T\in\mathbb{R}^2$ as an example:</p><p>$$\tag{🌟}
\hat{q} = \begin{pmatrix}\cos m\theta & -\sin m\theta \newline \sin m\theta & \cos m\theta\end{pmatrix}\begin{pmatrix}q_1\newline q_2\end{pmatrix} = \begin{pmatrix} q_1\cdot\cos m\theta - q_2\cdot\sin m\theta \newline q_1\cdot\sin m\theta + q_2\cdot\cos m\theta\end{pmatrix}
$$
where $m$ is the position and $\theta$ is a preset small non-zero constant. Word vectors in different positions are rotated with different amounts according to $m\theta$. To generalize to $d$ dimensions, where $d$ has to be an even number, the method applies rotation matrices to each 2 dimension pairs, for a total of $d/2$ pairs. Specifically, the transformation matrix is a block matrix where each block is a 2d rotation matrix:
$$
\begin{pmatrix}
\cos m\theta_1 & -\sin m\theta_1 & 0 & 0 & \cdots & 0 & 0 \newline
\sin m\theta_1 & \cos m\theta_1 & 0 & 0 & \cdots & 0 & 0 \newline
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \newline
0 & 0 & \cdots & 0 & 0 & \cos m\theta_{d/2} & -\sin m\theta_{d/2} \newline
0 & 0 & \cdots & 0 & 0 & \sin m\theta_{d/2} & \cos m\theta_{d/2}\newline
\end{pmatrix}
$$
and the angles are pre-defined with $\{\theta_i = 10000^{-2(i-1)/d}, i=1,\ldots,d/2\}$.</p><p>Implementing RoPE as matrix multiplication takes lots of memories when $d$ is large. Representing each 2d sub-vector pair as a complex number will let us avoid matrix computation. Let&rsquo;s re-focus on the 2d case. The 2d rotation matrix has close connections with arithmetics in the 2d complex plane. Recall the Euler&rsquo;s formula
$$
e^{i\theta} = \cos\theta + i\sin\theta.
$$
The core observation that is very useful for our implementation is the following: for a complex number $q_1 + q_2i$, we have
$$
\begin{split}
(q_1 + q_2i)e^{im\theta} &= (q_1 + q_2i)(\cos m\theta + i\sin m\theta)\newline
&= (q_1\cdot\cos m\theta - q_2\cdot\sin m\theta) + (q_1\cdot\sin m\theta + q_2\cdot\cos m\theta)i.
\end{split}
$$
The real and imaginary parts of this complex number are <em>exactly</em> the first and second entry of the transformed query vector above (🌟)! So to get the rotary embedding for vector $q$, we first view it as a complex number, multiply it by $e^{im\theta}$, then assemble the real and imaginary parts as the position encoded vector.</p><hr><p>So, here are the implementation steps for one single 2d query/key vector $q$ with position $m$:</p><ol><li>View the 2d vector $q=(q_1, q_2)^T$ as a complex number $q_1 + q_2i$, where the first entry is the real part, and the second entry is the imaginary part. In PyTorch, this is</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>q</span>  <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_complex</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ol start=2><li>Multiply the vector by $e^{im\theta}$. In PyTorch, $e^{im\theta}$ is <code>torch.polar(1, mθ)</code>. So we do</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>q</span> <span class=o>=</span> <span class=n>q</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>polar</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>mθ</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ol start=3><li>Get the real part and the imaginary part of this complex number. In PyTorch, this is</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>q_out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_real</span><span class=p>(</span><span class=n>q</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><p>The 2d implementation above showed the essence of RoPE. If you already understand the 2d case, then it is now easy to understand the code for general case. Here are the steps for $d$ dimensional query/key vectors , for all positions $m=0,\ldots,N-1$, with reference to Llama source code. The input will have dimension <code>(batch_size, seqlen, n_heads, head_dim)</code>.</p><table><thead><tr><th>dim</th><th>meaning</th></tr></thead><tbody><tr><td>0</td><td>batch size</td></tr><tr><td>1</td><td>sequence length</td></tr><tr><td>2</td><td>number of heads</td></tr><tr><td>3</td><td>head dimension</td></tr></tbody></table><ol><li>View each vector as a group of $d/2$ complex numbers. View dimension $(1, 2)$ as a complex number, dimension $(3, 4)$ as a complex number&mldr;&mldr;and so on. <code>torch.view_as_complex</code> expects the last dimension of its input to be of size 2, so we first reshape the input tensor as a list of 2 number pairs</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>q</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>q</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>then get the list of complex numbers via</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>q_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_complex</span><span class=p>(</span><span class=n>q</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>q</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><ol start=2><li>Multiply the first complex number by $e^{im\theta_1}$, multiply the second complex number by $e^{im\theta_2}$, and so on, until multiply the final complex number by $e^{im\theta_{d/2}}$. In llama source code, the complex numbers $\{e^{im\theta_1},e^{im\theta_2},\ldots,e^{im\theta_{d/2}}\}$ for all $m=0,\ldots,N-1$ are prepared by</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_exps</span><span class=p>(</span><span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>N</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>thetas</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>10000.0</span> <span class=o>**</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>m</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>thetas</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>angles</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=n>thetas</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>exps</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>polar</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>angles</span><span class=p>),</span> <span class=n>angles</span><span class=p>)</span>  <span class=c1># complex64</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>exps</span>
</span></span></code></pre></td></tr></table></div></div><p>To avoid potential confusions, I have renamed several variables so as to match the corresponding variable names in the original paper. Now let&rsquo;s examine the this function line by line:</p><p>(1)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>thetas</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=p>(</span><span class=mf>10000.0</span> <span class=o>**</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>dim</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>This line prepares all the thetas $\{\theta_i = 10000^{-2(i-1)/d}, i=1,\ldots,d/2\}$. The dimension of the output tensor <code>thetas</code> will be $d/2$, half the dimension of input $x$.</p><p>(2)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>end</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>thetas</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>This line prepares all the positions $(0, 1, \ldots, N-1)$.</p><p>(3)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>angles</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=n>thetas</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>This line prepares all the angles $m\theta_i$, for all $m$ and all $i$. For two tensors $a$ and $b$ with size $N$ and $d/2$, <code>torch.outer(a, b)</code> is $a^Tb$, so the output has dimension $N\times(d/2)$.</p><p>(4)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>exps</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>polar</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>angles</span><span class=p>),</span> <span class=n>angles</span><span class=p>)</span>  <span class=c1># complex64</span>
</span></span></code></pre></td></tr></table></div></div><p>This line computes $\{e^{im\theta_i}\}$, for all $m$ and all $\theta_i$. Thus the output of the <code>compute_exps</code> function is a matrix, where each row is $\{e^{im\theta_1},e^{im\theta_2},\ldots,e^{im\theta_{d/2}}\}$ for a particular position $m$, for all positions $m=0,\ldots,N-1$.</p><p>Suppose now we have prepared a query vector <code>q_</code> as a list of complex numbers and we also have complex exponentials as <code>exps</code>. We simply do</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>q_</span> <span class=o>*</span> <span class=n>exps</span>
</span></span></code></pre></td></tr></table></div></div><p>to multiply them together.</p><ol start=3><li>For each 2d dimension pair $(1, 2), (3,4), \ldots, (d-1, d)$, we have computed a complex number $q\cdot e^{im\theta}$, for a total number of $d/2$. Now we get the real and imaginary part of each complex number, and stack all of them together to get the output. <code>torch.view_as_real</code> is the inverse operation of <code>torch.view_as_complex</code>. That means the last dimension of the output from <code>torch.view_as_real</code> will be 2. That&rsquo;s why we need to call <code>.flatten(3)</code> to flatten the last dimension (remember from the input dimension table above that 3 is the last input shape position).</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>apply_rotary_emb</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>k</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>exps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>q_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_complex</span><span class=p>(</span><span class=n>q</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>q</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>k_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_complex</span><span class=p>(</span><span class=n>k</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>k</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>exps</span> <span class=o>=</span> <span class=n>reshape_for_broadcast</span><span class=p>(</span><span class=n>exps</span><span class=p>,</span> <span class=n>q_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>q_out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_real</span><span class=p>(</span><span class=n>q_</span> <span class=o>*</span> <span class=n>exps</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>k_out</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>view_as_real</span><span class=p>(</span><span class=n>k_</span> <span class=o>*</span> <span class=n>exps</span><span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>q_out</span><span class=p>,</span> <span class=n>k_out</span>
</span></span></code></pre></td></tr></table></div></div><hr><p>We mention that, although RoPE is claimed to be a relative position embedding method, meaning that the similarity measure $\hat{q}_m\cdot\hat{k}_n$ between the position encoded query $\hat{q}_m$ (with position $m$) and position encoded key $\hat{k}_n$ (with position $n$) only depends on $m-n$, their relative position, not their absolute positions in a sentence, we see that when implementing RoPE we still have to apply rotations to each query vector and each key vector in a sequence according to their <em>absolute</em> positions $m$ and $n$ in the sequence, obtaining $\hat{q}_m$ and $\hat{k}_n$, <em>before</em> computing $\hat{q}_m\cdot\hat{k}_n$. This is because, query and key vectors are needed to compute the attention matrix. In this sense, RoPE is not <em>true</em> relative position embedding.</p><p>Also, RoPE has long term decay property: the dot product of two position encoded vectors approaches zero as their distance increases. The assumption that a pair of long distance tokens should have less connection is not desirable in light of LLM applications to long texts, e.g. contracts, legal documents, and financial reports. For example, users could give an instruction at the end of a prompt to extract some information from a long piece of text, where such information could be located right at the very beginning of the text. The long term decay property could lead to degraded performance in such cases.</p><h3 id=alibi-position-embedding>ALiBi position embedding<a hidden class=anchor aria-hidden=true href=#alibi-position-embedding>#</a></h3><p>Attention with Linear Biases (ALiBi)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#alibi><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Ofir"><span itemprop=familyName>Press</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Noah A"><span itemprop=familyName>Smith</span></span>
<em>& al.</em>, <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Press</span>, <meta itemprop=givenName content="Ofir">O.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Smith</span>, <meta itemprop=givenName content="Noah A">N.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lewis</span>, <meta itemprop=givenName content="Mike">M.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Train short, test long: Attention with linear biases enables input length extrapolation</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2108.12409</span></i>.</span>
</span></span>)</span>
is an even simpler method to encode position information.</p><figure class=align-center><img loading=lazy src=/imgs/transformers/alibi.png#center alt="ALiBi positional embedding. Image source: paper"><figcaption><p>ALiBi positional embedding. Image source: paper</p></figcaption></figure><p>$m$ is a fixed scalar and is not learned. Ignoring $m$ for a second, the above figure has clear illustration of the method: biases $(-(N-1), \ldots, -2, -1, 0)$ are added to query and key dot products before applying softmax. For example, $-5$ is added to $q_5\cdot k_1$, $-1$ is added to $q_5\cdot k_4$, no bias is added to $q_5\cdot k_5$, and so on. There is no other positional encoding added elsewhere. Clearly, as distance gets larger, the negative bias will get large, and $\exp\{q\cdot k - N\}$ will decay to $0$ very fast as $N\to\infty$, resulting in 0 score and thus no lookups to distant tokens when generating the next token. As for RoPE, this property may not be desirable in real applications where one wants an LLM to process professional, long and complicated documents.</p><h3 id=flash-attention>Flash Attention<a hidden class=anchor aria-hidden=true href=#flash-attention>#</a></h3><p>Flash Attention
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#flashattention><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Tri"><span itemprop=familyName>Dao</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Dan"><span itemprop=familyName>Fu</span></span>
<em>& al.</em>, <span itemprop=datePublished>2022</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dao</span>, <meta itemprop=givenName content="Tri">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Fu</span>, <meta itemprop=givenName content="Dan">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ermon</span>, <meta itemprop=givenName content="Stefano">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rudra</span>, <meta itemprop=givenName content="Atri">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ré</span>, <meta itemprop=givenName content="Christopher">C.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name>FlashAttention: Fast and memory-efficient exact attention with io-awareness</span>.<i>
<span itemprop=about>Advances in Neural Information Processing Systems</span>, 35</i>. <span itemprop=pagination>16344–16359</span>.</span>
</span></span>)</span>
is a technique for speeding up attention computation on Nvidia GPUs for long sequences. There exists a memory <em>heirarchy</em> on GPU: high bandwidth memory (HBM) takes the bulk of GPU memory, but it is slower compared to SRAM, which is far smaller in size but is a dozen times faster than
HBM.</p><figure class=align-center><img loading=lazy src=/imgs/transformers/gpu-memory.png#center alt="GPU memory hierarchy. Image source: Fei Li (author)" height=400><figcaption><p>GPU memory hierarchy. Image source: Fei Li (author)</p></figcaption></figure><p>For a sequence with length $N$, the attention computation defined by the authors is
$$
\mathrm{O} = \mathrm{dropout}(\mathrm{softmax}(\mathrm{mask}(QK^T)))V
$$</p><figure class=align-center><img loading=lazy src=/imgs/transformers/attention-fa-illu.jpeg#center alt="Attention is Bottlenecked by Memory Reads/Writes. Source: https://www.youtube.com/live/gMOAud7hZg4"><figcaption><p>Attention is Bottlenecked by Memory Reads/Writes. Source: <a href=https://www.youtube.com/live/gMOAud7hZg4>https://www.youtube.com/live/gMOAud7hZg4</a></p></figcaption></figure><p>Memory usage grows quadratically with the sequence length $N$. As $N$ goes to thousands and beyond, the attention matrix $QK^T$ will occupy a lot of space in GPU memory. Masking, softmax and dropout computation all require reading and writing the $N\times N$ attention matrix. It turns out that those memory reads/writes take up most of the computation time, much more than the actual matrix multiplication $QK^T$.</p><p>To reduce memory i/o, and to take advantage of SRAM&rsquo;s fast speed, the authors proposed two techniques:</p><ol><li><p>Tiling. This refers to loading <em>partial</em> blocks of the attention matrix from HBM to SRAM, computing attention on SRAM for such blocks, then finally concatenating the results by the correct scaling factors. This technique utilizes that fact that for a matrix $A = [A_1, A_2]$ that consists of two sub-matrices $A_1$ and $A_2$, the softmax of $A$ can be written as
$$
\mathrm{softmax}([A_1, A_2]) = [\alpha\cdot\mathrm{softmax}(A_1), \beta\cdot\mathrm{softmax}(A_2)]
$$
for some scaling factors $\alpha$ and $\beta$, and
$$
\mathrm{softmax}([A_1, A_2])\begin{bmatrix}V_1\newline V_2\end{bmatrix} = \alpha\cdot\mathrm{softmax}(A_1)V_1 + \beta\cdot\mathrm{softmax}(A_2)V_2,
$$</p></li><li><p>Recomputation. During the backward pass, the attention matrix is needed for computing gradients of the weights, and again, reading this large matrix from HBM memory would be slow. The recomputation technique is straightforward: do not store the $N\times N$ attention matrix from forward pass (only store $N$ softmax normalizing factors), but recompute it in the backward pass. This incurs additional floating point operations (flops), but the runtime is reduced even with increased flops.</p><table><thead><tr><th>Attention</th><th>Standard</th><th>FlashAttention</th></tr></thead><tbody><tr><td>GFLOPs</td><td>66.6</td><td>75.2</td></tr><tr><td>HBM R/W (GB)</td><td>40.3</td><td>4.4</td></tr><tr><td>Runtime (ms)</td><td>41.7</td><td>7.3</td></tr></tbody></table><p><em>Source:
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#flashattention><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Tri"><span itemprop=familyName>Dao</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Dan"><span itemprop=familyName>Fu</span></span>
<em>& al.</em>, <span itemprop=datePublished>2022</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dao</span>, <meta itemprop=givenName content="Tri">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Fu</span>, <meta itemprop=givenName content="Dan">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ermon</span>, <meta itemprop=givenName content="Stefano">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rudra</span>, <meta itemprop=givenName content="Atri">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ré</span>, <meta itemprop=givenName content="Christopher">C.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name>FlashAttention: Fast and memory-efficient exact attention with io-awareness</span>.<i>
<span itemprop=about>Advances in Neural Information Processing Systems</span>, 35</i>. <span itemprop=pagination>16344–16359</span>.</span>
</span></span>)</span></em></p></li></ol><p>For the same sequence length $N$, Flash Attention allows faster computation compared with traditional attention. In other words, this means that for the same training time budget, one can train models that can deal with longer contexts. In this aspect, Flash Attention is very useful, because such models are very much needed in industrial applications.</p><h2 id=llama-source-code>Llama source code<a hidden class=anchor aria-hidden=true href=#llama-source-code>#</a></h2><p>We have already walked through some of Llama&rsquo;s source code, including the <a href=#swiglu>feedforward layer</a>, the <a href=#rmsnorm>RMSNorm layer</a>, and the <a href=#rotary-position-embedding-rope><code>apply_rotary_emb</code></a> function. In this last section, I will walk through the rest of Llama&rsquo;s source code. First let&rsquo;s look at the <code>model.py</code> file. We are left with three classes:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Attention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Transformer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span></code></pre></td></tr></table></div></div><p>Now let&rsquo;s study them one by one.</p><h3 id=the-attention-layer-1>The Attention layer<a hidden class=anchor aria-hidden=true href=#the-attention-layer-1>#</a></h3><p>The attention layer is implemented as follows. Again, I remind the reader that I have refactored the code and removed non-essential parts. What the Attention layer does is to multiply the query matrix with the key matrix, apply softmax to get the scores, then multiply the scores with values, and finally apply a linear layer to get the output.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Attention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>args</span><span class=p>:</span> <span class=n>ModelArgs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>args</span><span class=o>.</span><span class=n>dim</span> <span class=o>//</span> <span class=n>args</span><span class=o>.</span><span class=n>n_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wq</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>dim</span><span class=p>,</span> <span class=n>args</span><span class=o>.</span><span class=n>n_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wk</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>dim</span><span class=p>,</span> <span class=n>args</span><span class=o>.</span><span class=n>n_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>dim</span><span class=p>,</span> <span class=n>args</span><span class=o>.</span><span class=n>n_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wo</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>n_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>args</span><span class=o>.</span><span class=n>dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>start_pos</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>exps</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>xq</span><span class=p>,</span> <span class=n>xk</span><span class=p>,</span> <span class=n>xv</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>wq</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>wk</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>wv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>xq</span><span class=p>,</span> <span class=n>xk</span> <span class=o>=</span> <span class=n>apply_rotary_emb</span><span class=p>(</span><span class=n>xq</span><span class=p>,</span> <span class=n>xk</span><span class=p>,</span> <span class=n>exps</span><span class=o>=</span><span class=n>exps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cache_k</span><span class=p>[:</span><span class=n>bsz</span><span class=p>,</span> <span class=n>start_pos</span> <span class=p>:</span> <span class=n>start_pos</span> <span class=o>+</span> <span class=n>seqlen</span><span class=p>]</span> <span class=o>=</span> <span class=n>xk</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cache_v</span><span class=p>[:</span><span class=n>bsz</span><span class=p>,</span> <span class=n>start_pos</span> <span class=p>:</span> <span class=n>start_pos</span> <span class=o>+</span> <span class=n>seqlen</span><span class=p>]</span> <span class=o>=</span> <span class=n>xv</span>
</span></span><span class=line><span class=cl>        <span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_k</span><span class=p>[:</span><span class=n>bsz</span><span class=p>,</span> <span class=p>:</span> <span class=n>start_pos</span> <span class=o>+</span> <span class=n>seqlen</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cache_v</span><span class=p>[:</span><span class=n>bsz</span><span class=p>,</span> <span class=p>:</span> <span class=n>start_pos</span> <span class=o>+</span> <span class=n>seqlen</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>xq</span><span class=p>,</span> <span class=n>keys</span><span class=p>)</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span> <span class=o>+</span> <span class=n>mask</span>  <span class=c1># (bs, n_heads, seqlen, cache_len + seqlen)</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=o>.</span><span class=n>float</span><span class=p>(),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>values</span><span class=p>)</span>  <span class=c1># (bs, n_heads, seqlen, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>wo</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=the-transformerblock-layer>The TransformerBlock layer<a hidden class=anchor aria-hidden=true href=#the-transformerblock-layer>#</a></h3><p>A <code>TransformerBlock</code> layer in Llama is:</p><pre tabindex=0><code>x ---&gt;RMSNorm ---&gt;Attention ------&gt;RMSNorm ---&gt;FeedForward ---&gt;out
|                           |    |                           |
|                           |    |                           |
·------------ + ------------·    ·------------ + ------------·
</code></pre><p>Different from the original transformer architecture, the normalization layer is placed at the beginning, rather than the end. Below is the simplified pseudocode implementation. The <code>self.attention</code> attribute is Attention layer, <code>self.feed_forward</code> is SwiGLU layer. <code>self.ffn_norm</code> and <code>self.attention_norm</code> are both RMSNorm layers.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>layer_id</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>args</span><span class=p>:</span> <span class=n>ModelArgs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_id</span> <span class=o>=</span> <span class=n>layer_id</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>args</span><span class=o>.</span><span class=n>dim</span> <span class=o>//</span> <span class=n>args</span><span class=o>.</span><span class=n>n_heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>start_pos</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>attention_norm</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=n>start_pos</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>h</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ffn_norm</span><span class=p>(</span><span class=n>h</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=the-transformer-layer>The Transformer layer<a hidden class=anchor aria-hidden=true href=#the-transformer-layer>#</a></h3><p>Finally, the Llama model is defined as a <code>Transformer</code> class. It is a for loop of <code>TransformerBlock</code>s. The last dimension of <code>Transformer</code> output is un-normalized probability scores over the vocabulary. It has size <code>vocab_size</code>. Different sampling methods make different use of the scores. The basic greedy sampling chooses the one with the largest output score. Top-k sampling samples from top k highest score tokens.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Transformer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>params</span><span class=p>:</span> <span class=n>ModelArgs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tok_embeddings</span> <span class=o>=</span> <span class=n>Embedding</span><span class=p>(</span><span class=n>params</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>params</span><span class=o>.</span><span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer_id</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>params</span><span class=o>.</span><span class=n>n_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>TransformerBlock</span><span class=p>(</span><span class=n>layer_id</span><span class=p>,</span> <span class=n>params</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>RMSNorm</span><span class=p>(</span><span class=n>params</span><span class=o>.</span><span class=n>dim</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=n>params</span><span class=o>.</span><span class=n>norm_eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>params</span><span class=o>.</span><span class=n>dim</span><span class=p>,</span> <span class=n>params</span><span class=o>.</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@torch.inference_mode</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tokens</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>start_pos</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tok_embeddings</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>h</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>start_pos</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output</span><span class=p>(</span><span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span></code></pre></td></tr></table></div></div><p>The <code>Transformer</code> class above is the Llama model, which is pretty concise and clean. Having looked at the Llama model, now let&rsquo;s look at how inference is implemented.</p><h3 id=generation>Generation<a hidden class=anchor aria-hidden=true href=#generation>#</a></h3><p>There is one <code>Llama</code> class defined in <code>generation.py</code>. First, the <code>build</code> method loads weights from local paths, and initializes the Llama model. It is used in official examples <code>example_chat_completion.py</code> and <code>example_text_completion.py</code> to build a <code>generator</code>. The <code>generator</code> exposes two interfaces, <code>text_completion</code> and <code>chat_completion</code>. Both of the two are variants of the <code>generate</code> method.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama.model</span> <span class=kn>import</span> <span class=n>ModelArgs</span><span class=p>,</span> <span class=n>Transformer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>llama.tokenizer</span> <span class=kn>import</span> <span class=n>Tokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Llama</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>build</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>ckpt_dir</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer_path</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_seq_len</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_batch_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=s2>&#34;Llama&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>(</span><span class=n>model_path</span><span class=o>=</span><span class=n>tokenizer_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>Transformer</span><span class=p>(</span><span class=n>model_args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>,</span> <span class=n>strict</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>Llama</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>:</span> <span class=n>Transformer</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>:</span> <span class=n>Tokenizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>...</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>pass</span>
</span></span></code></pre></td></tr></table></div></div><p>The core part of the <code>generate</code> method is the following lines of code: get the logits from model&rsquo;s <code>forward</code> method, then sample the next token from the logits. As mentioned before, greedy sampling chooses the token with the largest output score; top-k sampling samples from tokens with k largest scores; and top-p sampling is top-k sampling where k summed probabilities exceeds a certain threshold, so that k is dynamically adjusted.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ......</span>
</span></span><span class=line><span class=cl><span class=nd>@torch.inference_mode</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt_tokens</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=nb>int</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=n>max_gen_len</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_p</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>logprobs</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>echo</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=nb>int</span><span class=p>]],</span> <span class=o>...</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>params</span>
</span></span><span class=line><span class=cl>    <span class=n>bsz</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>prompt_tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>tokens</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>full</span><span class=p>((</span><span class=n>bsz</span><span class=p>,</span> <span class=n>total_len</span><span class=p>),</span> <span class=n>pad_id</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>cur_pos</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>min_prompt_len</span><span class=p>,</span> <span class=n>total_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>tokens</span><span class=p>[:,</span> <span class=n>prev_pos</span><span class=p>:</span><span class=n>cur_pos</span><span class=p>],</span> <span class=n>prev_pos</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>temperature</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>/</span> <span class=n>temperature</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>next_token</span> <span class=o>=</span> <span class=n>sample_top_p</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>top_p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Note that the temperature parameter appears as the denominator of the softmax function input. If temperature is zero, then we simply have greedy sampling. If temperature is small, this would magnify those logits that are already large, so that other tokens will have even slimmer chance of being selected. <code>1.0</code> is a neutral choice. At values larger than <code>1.0</code>, temperature will reduce all the logits, reducing their differences after exponentials, so that sampling will be more random.</p><p>Text completion:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ......</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>text_completion</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>prompts</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_p</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_gen_len</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>List</span><span class=p>[</span><span class=nb>str</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt_tokens</span> <span class=o>=</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>bos</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>eos</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>prompts</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>generation_tokens</span><span class=p>,</span> <span class=n>generation_logprobs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>prompt_tokens</span><span class=o>=</span><span class=n>prompt_tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>max_gen_len</span><span class=o>=</span><span class=n>max_gen_len</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>temperature</span><span class=o>=</span><span class=n>temperature</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>top_p</span><span class=o>=</span><span class=n>top_p</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[{</span><span class=s2>&#34;generation&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>t</span><span class=p>)}</span> <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>generation_tokens</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>Chat completion:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Message</span><span class=p>(</span><span class=n>TypedDict</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>role</span><span class=p>:</span> <span class=n>Role</span>
</span></span><span class=line><span class=cl>    <span class=n>content</span><span class=p>:</span> <span class=nb>str</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Dialog</span> <span class=o>=</span> <span class=n>List</span><span class=p>[</span><span class=n>Message</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>B_INST</span><span class=p>,</span> <span class=n>E_INST</span> <span class=o>=</span> <span class=s2>&#34;[INST]&#34;</span><span class=p>,</span> <span class=s2>&#34;[/INST]&#34;</span>
</span></span><span class=line><span class=cl><span class=n>B_SYS</span><span class=p>,</span> <span class=n>E_SYS</span> <span class=o>=</span> <span class=s2>&#34;&lt;&lt;SYS&gt;&gt;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&lt;&lt;/SYS&gt;&gt;</span><span class=se>\n\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=n>DEFAULT_SYSTEM_PROMPT</span> <span class=o>=</span> <span class=s2>&#34;You are a helpful, respectful and honest assistant...&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ......</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>chat_completion</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dialogs</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=n>Dialog</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_p</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_gen_len</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>logprobs</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>List</span><span class=p>[</span><span class=n>Message</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>max_gen_len</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>max_gen_len</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>params</span><span class=o>.</span><span class=n>max_seq_len</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt_tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>dialog</span> <span class=ow>in</span> <span class=n>dialogs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>dialog</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;role&#34;</span><span class=p>]</span> <span class=o>!=</span> <span class=s2>&#34;system&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>dialog</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>DEFAULT_SYSTEM_PROMPT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=p>]</span> <span class=o>+</span> <span class=n>dialog</span>
</span></span><span class=line><span class=cl>        <span class=n>dialog</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=n>dialog</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=s2>&#34;role&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>B_SYS</span>
</span></span><span class=line><span class=cl>                <span class=o>+</span> <span class=n>dialog</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s2>&#34;content&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=o>+</span> <span class=n>E_SYS</span>
</span></span><span class=line><span class=cl>                <span class=o>+</span> <span class=n>dialog</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=s2>&#34;content&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span> <span class=o>+</span> <span class=n>dialog</span><span class=p>[</span><span class=mi>2</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=nb>all</span><span class=p>([</span><span class=n>msg</span><span class=p>[</span><span class=s2>&#34;role&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;user&#34;</span> <span class=k>for</span> <span class=n>msg</span> <span class=ow>in</span> <span class=n>dialog</span><span class=p>[::</span><span class=mi>2</span><span class=p>]])</span> <span class=ow>and</span> <span class=nb>all</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>msg</span><span class=p>[</span><span class=s2>&#34;role&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;assistant&#34;</span> <span class=k>for</span> <span class=n>msg</span> <span class=ow>in</span> <span class=n>dialog</span><span class=p>[</span><span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;model only supports &#39;system&#39;, &#39;user&#39; and &#39;assistant&#39; roles, &#34;</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;starting with &#39;system&#39;, then &#39;user&#39; and alternating (u/a/u/a/u...)&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dialog_tokens</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>B_INST</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=p>(</span><span class=n>prompt</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>E_INST</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=p>(</span><span class=n>answer</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span><span class=si>}</span><span class=s2> &#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>bos</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>eos</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=n>prompt</span><span class=p>,</span> <span class=n>answer</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>dialog</span><span class=p>[::</span><span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                    <span class=n>dialog</span><span class=p>[</span><span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>[],</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>dialog</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=s2>&#34;role&#34;</span><span class=p>]</span> <span class=o>==</span> <span class=s2>&#34;user&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span> <span class=sa>f</span><span class=s2>&#34;Last message must be from user, got </span><span class=si>{</span><span class=n>dialog</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=s1>&#39;role&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>dialog_tokens</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>B_INST</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=p>(</span><span class=n>dialog</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>E_INST</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>bos</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>eos</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>dialog_tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>generation_tokens</span><span class=p>,</span> <span class=n>generation_logprobs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt_tokens</span><span class=o>=</span><span class=n>prompt_tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_gen_len</span><span class=o>=</span><span class=n>max_gen_len</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>temperature</span><span class=o>=</span><span class=n>temperature</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>top_p</span><span class=o>=</span><span class=n>top_p</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>logprobs</span><span class=o>=</span><span class=n>logprobs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;generation&#34;</span><span class=p>:</span> <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;assistant&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>t</span><span class=p>)}}</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>generation_tokens</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>As with <code>text_completion</code>, <code>chat_completion</code> is also a wrapper around the <code>generate</code> method. Chat histories are concatenated to a string, before feeding into the <code>generate</code> method. To distinguish between users&rsquo; prompts and assistant&rsquo;s answers, special tokens like <code>[INST]</code> and <code>[/INST]</code> are inserted:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>B_INST</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=p>(</span><span class=n>prompt</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>E_INST</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=p>(</span><span class=n>answer</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span><span class=si>}</span><span class=s2> &#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>A pre-trained model is fine-tuned on chat data with such format, so that it should &ldquo;recognize&rdquo; such format in inference. Here is the description from the Llama 2 paper
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#llama2><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Hugo"><span itemprop=familyName>Touvron</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Louis"><span itemprop=familyName>Martin</span></span>
<em>& al.</em>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Touvron</span>, <meta itemprop=givenName content="Hugo">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Martin</span>, <meta itemprop=givenName content="Louis">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Stone</span>, <meta itemprop=givenName content="Kevin">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Albert</span>, <meta itemprop=givenName content="Peter">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Almahairi</span>, <meta itemprop=givenName content="Amjad">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Babaei</span>, <meta itemprop=givenName content="Yasmine">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Bashlykov</span>, <meta itemprop=givenName content="Nikolay">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Batra</span>, <meta itemprop=givenName content="Soumya">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Bhargava</span>, <meta itemprop=givenName content="Prajjwal">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Bhosale</span>, <meta itemprop=givenName content="Shruti">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Llama 2: Open foundation and fine-tuned chat models</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2307.09288</span></i>.</span>
</span></span>)</span>
:</p><blockquote><p>For the fine-tuning process, each sample consists of a prompt and an answer. To ensure the model sequence length is properly filled, we concatenate all the prompts and answers from the training set. A special token is utilized to separate the prompt and answer segments. We utilize an autoregressive objective and zero-out the loss on tokens from the user prompt, so as a result, we backpropagate only on answer tokens. Finally, we fine-tune the model for 2 epochs.</p></blockquote><p><code>self.tokenizer.encode</code> will encode the input string to a list of integers. So the type of</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>B_INST</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=p>(</span><span class=n>prompt</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>E_INST</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=p>(</span><span class=n>answer</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>])</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span><span class=si>}</span><span class=s2> &#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>bos</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>eos</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>prompt</span><span class=p>,</span> <span class=n>answer</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>dialog</span><span class=p>[::</span><span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>dialog</span><span class=p>[</span><span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>will be <code>list[list[int]]</code>, a list of integer lists. <code>sum(lst, [])</code> where <code>lst</code> is of type <code>list[list[int]]</code> will flatten the list (remove all inner lists), so the output of that will be <code>list[int]</code>. For example,</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>In</span> <span class=p>[</span><span class=mi>1</span><span class=p>]:</span> <span class=nb>sum</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span><span class=mi>5</span><span class=p>,</span><span class=mi>6</span><span class=p>],</span> <span class=p>[</span><span class=mi>7</span><span class=p>,</span><span class=mi>8</span><span class=p>,</span><span class=mi>9</span><span class=p>]],</span> <span class=p>[])</span>
</span></span><span class=line><span class=cl><span class=n>Out</span><span class=p>[</span><span class=mi>1</span><span class=p>]:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>9</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>This <code>dialog_tokens</code> list (type <code>list[int]</code>) is then appended to <code>prompt_tokens</code> (type <code>list[list[int]]</code>), and finally this list is fed to <code>self.generate</code> method to get the generated tokens. This is how <code>text_completion</code> and <code>chat_completion</code> work under the hood.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>In this post, I talked about various techniques for improving the transformer architecture, including SwiGLU, RMSNorm, Rotary Position Embedding (RoPE), ALiBi and Flash Attention. I have also walked through Llama&rsquo;s source code. My post could help AI practitioners better understand LLMs&rsquo; behaviors as well as how to use them properly. With more open source LLMs coming out, this post could also help provide a direction for finding models that best suit various application scenarios.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><section class=hugo-cite-bibliography><div style=margin-bottom:2pt id=alibi>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Press</span>, <meta itemprop=givenName content="Ofir">O.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Smith</span>, <meta itemprop=givenName content="Noah A">N.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lewis</span>, <meta itemprop=givenName content="Mike">M.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Train short, test long: Attention with linear biases enables input length extrapolation</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2108.12409</span></i>.</span></div><div style=margin-bottom:2pt id=batchnorm>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ioffe</span>, <meta itemprop=givenName content="Sergey">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Szegedy</span>, <meta itemprop=givenName content="Christian">C.</span>
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name><i>Batch normalization: Accelerating deep network training by reducing internal covariate shift</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>pmlr</span></span>.</span></div><div style=margin-bottom:2pt id=flashattention>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dao</span>, <meta itemprop=givenName content="Tri">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Fu</span>, <meta itemprop=givenName content="Dan">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ermon</span>, <meta itemprop=givenName content="Stefano">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rudra</span>, <meta itemprop=givenName content="Atri">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ré</span>, <meta itemprop=givenName content="Christopher">C.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name>FlashAttention: Fast and memory-efficient exact attention with io-awareness</span>.<i>
<span itemprop=about>Advances in Neural Information Processing Systems</span>, 35</i>. <span itemprop=pagination>16344–16359</span>.</span></div><div style=margin-bottom:2pt id=layernorm>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ba</span>, <meta itemprop=givenName content="Jimmy Lei">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kiros</span>, <meta itemprop=givenName content="Jamie Ryan">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Hinton</span>, <meta itemprop=givenName content="Geoffrey E">G.</span>
 
(<span itemprop=datePublished>2016</span>).
 <span itemprop=name>Layer normalization</span>.<i>
<span itemprop=about>arXiv preprint arXiv:1607.06450</span></i>.</span></div><div style=margin-bottom:2pt id=llama2>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Touvron</span>, <meta itemprop=givenName content="Hugo">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Martin</span>, <meta itemprop=givenName content="Louis">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Stone</span>, <meta itemprop=givenName content="Kevin">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Albert</span>, <meta itemprop=givenName content="Peter">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Almahairi</span>, <meta itemprop=givenName content="Amjad">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Babaei</span>, <meta itemprop=givenName content="Yasmine">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Bashlykov</span>, <meta itemprop=givenName content="Nikolay">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Batra</span>, <meta itemprop=givenName content="Soumya">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Bhargava</span>, <meta itemprop=givenName content="Prajjwal">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Bhosale</span>, <meta itemprop=givenName content="Shruti">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Llama 2: Open foundation and fine-tuned chat models</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2307.09288</span></i>.</span></div><div style=margin-bottom:2pt id=rmsnorm>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Biao">B.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sennrich</span>, <meta itemprop=givenName content="Rico">R.</span>
 
(<span itemprop=datePublished>2019</span>).
 <span itemprop=name>Root mean square layer normalization</span>.<i>
<span itemprop=about>Advances in Neural Information Processing Systems</span>, 32</i>.</span></div><div style=margin-bottom:2pt id=rope>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Su</span>, <meta itemprop=givenName content="Jianlin">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lu</span>, <meta itemprop=givenName content="Yu">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Pan</span>, <meta itemprop=givenName content="Shengfeng">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Murtadha</span>, <meta itemprop=givenName content="Ahmed">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wen</span>, <meta itemprop=givenName content="Bo">B.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Liu</span>, <meta itemprop=givenName content="Yunfeng">Y.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name>Roformer: Enhanced transformer with rotary position embedding</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2104.09864</span></i>.</span></div><div style=margin-bottom:2pt id=swiglu>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shazeer</span>, <meta itemprop=givenName content="Noam">N.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>Glu variants improve transformer</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2002.05202</span></i>.</span></div><div style=margin-bottom:2pt id=swish>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ramachandran</span>, <meta itemprop=givenName content="Prajit">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zoph</span>, <meta itemprop=givenName content="Barret">B.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Le</span>, <meta itemprop=givenName content="Quoc V">Q.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Searching for activation functions</span>.<i>
<span itemprop=about>arXiv preprint arXiv:1710.05941</span></i>.</span></div><div style=margin-bottom:2pt id=transformer>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Vaswani</span>, <meta itemprop=givenName content="Ashish">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shazeer</span>, <meta itemprop=givenName content="Noam">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Parmar</span>, <meta itemprop=givenName content="Niki">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Uszkoreit</span>, <meta itemprop=givenName content="Jakob">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jones</span>, <meta itemprop=givenName content="Llion">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gomez</span>, <meta itemprop=givenName content="Aidan N">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kaiser</span>, <meta itemprop=givenName content="Łukasz">Ł.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Polosukhin</span>, <meta itemprop=givenName content="Illia">I.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Attention is all you need</span>.<i>
<span itemprop=about>Advances in neural information processing systems</span>, 30</i>.</span></div></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://lifei.ai/tags/llm/>llm</a></li><li><a href=https://lifei.ai/tags/attention/>attention</a></li><li><a href=https://lifei.ai/tags/transformer/>transformer</a></li><li><a href=https://lifei.ai/tags/llama/>llama</a></li></ul><nav class=paginav><a class=prev href=https://lifei.ai/posts/2024-02-12-vision-models/><span class=title>« Prev</span><br><span>Vision Models</span></a>
<a class=next href=https://lifei.ai/posts/2023-04-06-chatgpt/><span class=title>Next »</span><br><span>ChatGPT has taken the world by storm. What's Next?</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Better Transformers on x" href="https://x.com/intent/tweet/?text=Better%20Transformers&amp;url=https%3a%2f%2flifei.ai%2fposts%2f2023-08-31-better-transformers%2f&amp;hashtags=llm%2cattention%2ctransformer%2cllama"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Better Transformers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flifei.ai%2fposts%2f2023-08-31-better-transformers%2f&amp;title=Better%20Transformers&amp;summary=Better%20Transformers&amp;source=https%3a%2f%2flifei.ai%2fposts%2f2023-08-31-better-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Better Transformers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flifei.ai%2fposts%2f2023-08-31-better-transformers%2f&title=Better%20Transformers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Better Transformers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flifei.ai%2fposts%2f2023-08-31-better-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Better Transformers on whatsapp" href="https://api.whatsapp.com/send?text=Better%20Transformers%20-%20https%3a%2f%2flifei.ai%2fposts%2f2023-08-31-better-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Better Transformers on telegram" href="https://telegram.me/share/url?text=Better%20Transformers&amp;url=https%3a%2f%2flifei.ai%2fposts%2f2023-08-31-better-transformers%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Better Transformers on ycombinator" href="https://news.ycombinator.com/submitlink?t=Better%20Transformers&u=https%3a%2f%2flifei.ai%2fposts%2f2023-08-31-better-transformers%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://lifei.ai/>Fei Li</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"auto"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>