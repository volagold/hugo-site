<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Vision Models | lifei.ai | AI blogs</title><meta name=keywords content="vision,diffusion,transformer"><meta name=description content="At the same time LLMs are changing the world, we have witnessed fast progress in AI image and video generation. AI models are now able to synthesis high quality, high fidelity and high resolution images via text prompting. Mainstream generative model architectures have shifted from VAEs, flows and GANs to diffusions and transformers. In this post I take notes of several vision models. It may be regularly updated to reflect latest research development."><meta name=author content="Fei Li"><link rel=canonical href=https://lifei.ai/posts/2024-02-12-vision-models/><link crossorigin=anonymous href=/assets/css/stylesheet.415e3a8d51db2ff1dd1daeb6ed47da1d29ea7215172df9a95da9a546c6c5ba84.css integrity="sha256-QV46jVHbL/HdHa627UfaHSnqchUXLfmpXamlRsbFuoQ=" rel="preload stylesheet" as=style><link rel=icon href=https://lifei.ai/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lifei.ai/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lifei.ai/favicon-32x32.png><link rel=apple-touch-icon href=https://lifei.ai/apple-touch-icon.png><link rel=mask-icon href=https://lifei.ai/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet type=text/css href=/hugo-cite.css><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js integrity="sha512-sHSNLECRJSK+BFs7E8DiFc6pf6n90bLI85Emiw1jhreduZhK3VXgq9l4kAt9eTIHYAcuQBKHL01QKs4/3Xgl8g==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.css integrity="sha512-mQwom8Ns4op+H29oDkD/LXO/OsXPvCFfkgZkFAVrhhePzRLU8NUI3Nkm43NhWUSmj3p5Cca2HTEkMQmXQRwDQQ==" crossorigin=anonymous referrerpolicy=no-referrer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/mhchem.min.js integrity=sha384-F2ptQFZqNJuqfGGl28mIXyQ5kXH48spn7rcoS0Y9psqIKAcZPLd1NzwFlm/bl1mH crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin=anonymous referrerpolicy=no-referrer onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-QV43D2GXK2"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QV43D2GXK2",{anonymize_ip:!1})}</script><meta property="og:title" content="Vision Models"><meta property="og:description" content="At the same time LLMs are changing the world, we have witnessed fast progress in AI image and video generation. AI models are now able to synthesis high quality, high fidelity and high resolution images via text prompting. Mainstream generative model architectures have shifted from VAEs, flows and GANs to diffusions and transformers. In this post I take notes of several vision models. It may be regularly updated to reflect latest research development."><meta property="og:type" content="article"><meta property="og:url" content="https://lifei.ai/posts/2024-02-12-vision-models/"><meta property="og:image" content="https://lifei.ai/imgs/vision/cover.jpeg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-12T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-12T00:00:00+00:00"><meta property="og:site_name" content="lifei.ai | AI blogs"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://lifei.ai/imgs/vision/cover.jpeg"><meta name=twitter:title content="Vision Models"><meta name=twitter:description content="At the same time LLMs are changing the world, we have witnessed fast progress in AI image and video generation. AI models are now able to synthesis high quality, high fidelity and high resolution images via text prompting. Mainstream generative model architectures have shifted from VAEs, flows and GANs to diffusions and transformers. In this post I take notes of several vision models. It may be regularly updated to reflect latest research development."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lifei.ai/posts/"},{"@type":"ListItem","position":2,"name":"Vision Models","item":"https://lifei.ai/posts/2024-02-12-vision-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Vision Models","name":"Vision Models","description":"At the same time LLMs are changing the world, we have witnessed fast progress in AI image and video generation. AI models are now able to synthesis high quality, high fidelity and high resolution images via text prompting. Mainstream generative model architectures have shifted from VAEs, flows and GANs to diffusions and transformers. In this post I take notes of several vision models. It may be regularly updated to reflect latest research development.","keywords":["vision","diffusion","transformer"],"articleBody":"At the same time LLMs are changing the world, we have witnessed fast progress in AI image and video generation. AI models are now able to synthesis high quality, high fidelity and high resolution images via text prompting. Mainstream generative model architectures have shifted from VAEs, flows and GANs to diffusions and transformers. In this post I take notes of several vision models. It may be regularly updated to reflect latest research development.\nVision Understanding ViT Vision Transformer (ViT) ( Citation: Dosovitskiy, Beyer \u0026 al., 2020 Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. \u0026 (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. ) applies the transformer architecture to vision inputs. Images are segmented to patches, and the patches are embedded as sequence of vectors, with patch ordering as position embedding. The figure below illustrates the architecture. The model achieves superior performance than convolutional neural networks (CNNs) in many tasks, and has become a basic component in other subsequent vision models. Refer to @lucidrains/vit-pytorch for a simple implementation of ViT.\nViT architecture. Source: Machine Learning Mastery\nCLIP CLIP ( Citation: Radford, Kim \u0026 al., 2021 Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. \u0026 (2021). Learning transferable visual models from natural language supervision. PMLR. ) is a foundational model developed by OpenAI that set the stage for multi-modality modeling. Traditionally, image classification models were trained to map input to a fixed set of classes, which is inflexible. By jointly training text and image embeddings on massive natural text and image pairs, CLIP models learn associations of images with texts, which allows it to perform open-set classification.\nArchitecture The CLIP model consists of a text encoder and an image encoder, trained on large collection of (text, image) pairs. For image encoder, two architectures are experimented, ResNet and ViT, and ViT was found to perform better than ResNet. The text encoder is a Transformer, the base one has 12 layers and 512 embedding dimensions, for a total of 63M parameters, with a vocab size 49152, and sequence length capped at 76.\nDot product between text embedding and image embedding of a pair $(T_i, I_i)$ is maximized, and dot product for pairs $$ \\{(T_i, I_j)\\}_{j\\neq i} $$ and $$ \\{(T_j, I_i)\\}_{j\\neq i} $$ are minimized, where $I_j$ and $T_j$ are other texts and images in the same batch. A large batch size of 32768 is used in the paper. In the figure below, this means maximizing cross entropy of the first row ➕ cross entropy of the first column, cross entropy of the second row ➕ cross entropy of the second column, and so on.\n[Further Detail] The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space.\nCLIP model architecture. Source: paper\nTraining Dataset. 400 million non-open (text, image) pairs. Loss. The loss is cross entropy loss that maximizes probability for (text, image) pairs in the training data and minimizes probability for not-paired texts and images in training batches. The paper provided the following pseudocode: 1 2 3 4 5 logits = np.dot(I_e, T_e.T) * np.exp(t) # t is temperature labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t) / 2 Code. huggingface - CLIP Note that CLIP is a discriminative model. It can be used for image search and zero-shot image classification, but it is not a generative model, and it cannot be directly used to generate images. It is often served as a text encoder component in many image generation models.\nMAE Masked Autoencoder (MAE) ( Citation: He, Chen \u0026 al., 2022 He, K., Chen, X., Xie, S., Li, Y., Dollár, P. \u0026 Girshick, R. (2022). Masked autoencoders are scalable vision learners. ) is a ViT-based denoising autoencoder that learns image representations. 75% patches of training images are masked and the objective is to reconstruct the masked pixel values, with MSE between predicted vector of pixels and original pixel values as loss. The encoder is a ViT model that is only applied to unmasked patches. The decoder is a smaller ViT model that is only used for reconstruction during training, but is discarded afterwards since only encoder outputs are used in downstream tasks.\nMAE model architecture. Source: paper\nSAM Segment Anything (SAM) is an interactive image segmentation model released by Meta AI. The model can be used for image and video editing, and can also be integrated into AR/VR headset to select objects.\nSource: paper\nArchitecture SAM consists of a one-time image encoder, a prompt encoder and a lightweight mask decoder. The image encoder is a ViT model that produces embedding for an input image. It has 632M parameters, which takes ~0.15 seconds to run on an NVIDIA A100 GPU. The prompt encoder maps points, boxes and texts via CLIP and CNNs. The decoder is a transformer that predicts object masks from image embeddings and prompt embeddings. The prompt encoder and mask decoder have 4M parameters, and take ~50ms on CPU in the browser.\nNote that the model only predicts binary masks, but not labels (chairs, cars, etc.). SAM model architecture. Image embedding can be computed once per image in the backend on GPUs. After the embedding is obtained, masking prediction can run very fast in the browser, which enables real-time interactive prompting. Source: https://segment-anything.com\nTraining Dataset. The model is trained on 11M images with 1B+ masks. The dataset called SA-1B is released here along with the model. Loss. Mask prediction is supervised with linear combination of focal loss ( Citation: Lin, Goyal \u0026 al., 2017 Lin, T., Goyal, P., Girshick, R., He, K. \u0026 Dollár, P. (2017). Focal loss for dense object detection. ) and dice loss ( Citation: Milletari, Navab \u0026 al., 2016 Milletari, F., Navab, N. \u0026 Ahmadi, S. (2016). V-net: Fully convolutional neural networks for volumetric medical image segmentation. Ieee. ) . The focal loss is a modulated cross entropy loss: $$ \\mathrm{FL}(p) = -\\alpha(1-p)^\\gamma\\log(p). $$ The dice loss measures overlap between predicted and ground-truth labels. Let $y_i$ denote label, the formula is $$ \\mathrm{Dice}(p) = 1 - \\frac{2\\sum y_i\\cdot p_i}{y_i^2 + p_i^2+1} $$ Hardware. The model was trained for 3~5 days on 256 A100 GPUs. More details can be found at https://segment-anything.com/.\nImage Generation via Discrete Tokenization VQ-VAE VQ-VAE ( Citation: Van Den Oord, Vinyals \u0026 al., 2017 Van Den Oord, A., Vinyals, O. \u0026 (2017). Neural discrete representation learning. Advances in neural information processing systems, 30. ) is a seminal paper published by Google DeepMind researchers in 2017. VQ-VAE is a way to regularize the latents, by forcing the decoder to only allocate probability mass to a finite set of learned vectors. The model was proposed to address the “posterior collapse” issue in VAEs, which is the phenomenon that the decoder often ignores the latents, producing blurry and monotonous images.\nVQ-VAE Architecture. Source: ResearchGate\nVQ-VAE consists of two parts: an autoencoder that learns discrete representation of images, and an autoregressive model that learns to generate latents. The encoder maps input images of shape $H\\times W\\times C$ to tensor $z$ of shape $h\\times w\\times c$. The downsampling factor $f=H/h=W/w$ is often set to $8$ or $16$. The latent codes in codebook, which serve as input to the decoder, are also designed to have the same channel dimension $c$. As clearly illustrated in the figure above, in forward pass, each spatial element in $z$ is mapped to its closest latent $e_i$ from the codebook $\\{e_1,\\ldots,e_K\\}$ by computing Euclidean distances. However, in backward pass, we need to consider how to back-propagate gradients through this discrete operation. The proposal is simple: just copy the gradient of $e_i$ from the right side to the left side. Since we want the encoder outputs to be close to their learned embeddings, this strategy should be a useful way to adjust the the encoder outputs.\nThe loss function for training the autoencoder is\n$$ \\ell = \\log p(x\\mid z_q(x)) + \\|sg[z_e(x)] - e\\|^2 + \\beta\\cdot\\|z_e(x) - sg[e]\\|^2, $$\nwhere sg stands for “stop gradient” and it is the .detach() operation in PyTorch. This means, for the second term, we only update the embedding vector $e$, treating $z_e(x)$ as a constant; for the third term, we only update $z_e(x)$, treating $e$ as a constant.\nHere is a breakdown of the three terms:\nThe decoder optimizes $\\log p(x\\mid z_q(x))$ only. To make sure the encoder commits to an embedding and its output does not grow, a commitment loss is added as the third term. The encoder optimizes $$ \\log p(x\\mid z_q(x)) + \\beta\\cdot\\|z_e(x) - sg[e]\\|^2. $$ The embeddings are optimized by the middle term $\\|sg[z_e(x)] - e\\|^2$. Note that under the assumption that the decoder output is Gaussian: $$ p(x\\mid z_q(x)) \\sim e^{-\\|x-\\mu\\|^2}, $$ the first loss term reduces to mean squared error (MSE) loss. So when it comes down to implementation, the loss is directly implemented as $$ \\ell_{\\mathrm{VQ-VAE}} = \\ell_{recon} + \\ell_{codebook} + \\beta\\cdot\\ell_{commit} $$ with $$ \\ell_{recon} = \\|x-\\hat{x}\\|^2,\\quad \\ell_{codebook} = \\|sg[z_e(x)] - e\\|^2,\\quad \\ell_{commit} = \\|z_e(x) - sg[e]\\|^2. $$\nThe encoder and decoder used in the original VQ-VAE paper are CNNs, and after the autoencoder is trained, an autoregressive model (PixelCNN) is learned to autoregressively generate tokens in the latent space. Nowadays, the PixelCNN model has been substituted with transformers. In the original experiment, x = 128 x 128 x 3 images are mapped to z = 32 × 32 × 1 tensors, and the codebook consists of K=512 vectors each of dimension 1. This means that each each of the 32 x 32 = 1024 latent value comes from one of 512 values.\nBelow are two open-source PyTorch implementations of VQ-VAE:\nkarpathy/deep-vector-quantization lucidrains/vector-quantize-pytorch VQ-VAE can be used to generate images: first generate tokens in latent space by the trained generative model, then use the decoder to map generated latents to pixel space. An important architectural improvement, the VQ-GAN model, successfully substituted PixelCNN with transformers, and helped further popularize the approach.\nVQ-GAN VQ-GAN ( Citation: Esser, Rombach \u0026 al., 2021 Esser, P., Rombach, R. \u0026 Ommer, B. (2021). Taming transformers for high-resolution image synthesis. ) is an improvement over the VQ-VAE model, at a time when the Transformer architecture became popular. Here are the differences:\nThe PixelCNN model used in the VQ-VAE paper to generate tokens in latent space is replaced by a Transformer, though the training objective is still autoregressive, i.e. the latent tensor is flattened to a 1D vector and the model is trained to predict the next token given all previous tokens. The encoder and the decoder are still CNNs. The MSE loss $\\|x-\\hat{x}\\|^2$ used in VQ-VAE is replaced with a “perceptual loss”, meaning that MSE is calculated between feature maps $\\|f(x)-f(\\hat{x})\\|^2$ for some CNN model $f$, instead of between original images in pixel space. Further, a GAN objective is added to the original VQ-VAE objective: $$ \\ell_{GAN} = \\log D(x) + \\log(1 - D(\\hat{x})) $$ where $D$ is a patch-based discriminator. The complete objective is $$ \\ell = \\ell_{\\mathrm{VQ-VAE}} + \\lambda\\cdot\\ell_{GAN} $$ where $\\lambda$ is a dynamically-adjusted weight (refer to paper for detail). VQ-GAN Architecture. Source: paper\nRecently, Google’s series of work that are built upon the VQ-GAN model demonstrated the potential of masked token modeling approach to visual content generation. The approach is to learn an autoencoder tokenizer to map images or videos to discrete tokens, then train a transformer model for masked token prediction. Here we briefly mention some of them.\nTwo different approaches to image generation. Source: author\nMaskGIT MaskGIT ( Citation: Chang, Zhang \u0026 al., 2022 Chang, H., Zhang, H., Jiang, L., Liu, C. \u0026 Freeman, W. (2022). Maskgit: Masked generative image transformer. ) is an improvement over VQ-GAN. In VQ-GAN, tokens are modeled using a GPT-style autoregressive Transformer, which treats visual tokens as a 1D sequence and predicts the next token given previously generated tokens. In MaskGIT, this GPT model is substituted with a BERT model that predicts masked tokens. Note that MaskGIT is a foundational model trained mainly for unconditional image generation, which allows text-to-image models to be built upon it for incorporating more modalities and image editing applications.\nArchitecture The model follows the same two-stage design as with VQ-GAN. First, an autoencoder is trained to learn to represent images with discrete tokens. Second, a BERT model is learnt to predict masked tokens. The purpose of the paper is to mainly investigate the second stage, so the first stage follows the same setup as in the VQ-GAN. MaskGIT Architecture. Source: paper\nIn theory, the model is able to infer all tokens and generate the entire image in a single pass. However, the authors found that this doesn’t work, so a gradual sampling procedure is employed, where at each step only predictions with high probabilities are retained, and others tokens are masked again.\nA decreasing mask function $\\gamma(r)\\in(0,1]$ is used to mask tokens during training and inference. A number of functions are experimented, and linear and concave ones like cosine ($\\cos x$), square ($1-x^2$), and cubic ($1-x^3$) are found to be the best ones. During training, a ratio $r$ is sampled from 0 to 1, then $\\gamma(r)\\cdot N$ tokens in $Y$ are uniformly selected for masking.\nThe decoding algorithm synthesizes an image in $T$ steps. At each iteration, the model predicts all tokens simultaneously but only keeps the most confident ones. The remaining tokens are masked out and re-predicted in the next iteration. The mask ratio is made decreasing until all tokens are generated within $T$ iterations. $T=8$ to $12$ iterations are found to yield best results.\nTraining Loss. After the tokenizer is trained, a BERT model is trained to predict masked tokens. The training loss is cross entropy loss on masked tokens. $$ \\mathcal{L} = -\\mathbb{E}_{Y\\in\\mathcal{D}}\\left[\\sum_{\\forall i\\in[1, N], m_i=1}\\log p\\left(y_i\\mid Y_{\\overline{M}}\\right)\\right] $$ where $\\mathcal{D}$ is dataset, $Y=[y_i]_1^N$ is image tokens with length $N$, $M=[m_i]_1^N$ is the corresponding binary mask.\nDataset. The model is trained on ImageNet 256x256 \u0026 512x512 and Places2 dataset. For each dataset, a single autoencoder along with a codebook of 1024 tokens are trained.\nHardware. All models are trained on 4x4 TPU devices with a batch size of 256.\nCode. List of implementations is available on paperswithcode.\nMuse Muse ( Citation: Chang, Zhang \u0026 al., 2023 Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M., Murphy, K., Freeman, W., Rubinstein, M. \u0026 (2023). Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704. ) is a text-conditioned MaskGIT model for real-world image generation application with numerous modifications over the original MaskGIT architecture. A 4.6B-parameter text encoder, T5-XXL, is used to map text prompts to 4096 dim embedding vectors that will be fused with token embeddings via cross attention. The image generation pipeline is split into two parts with a base module and a super-resolution module.\nMuse Architecture. Source: paper\nThe reason for adding a super-resolution pipeline is that, directly predicting 512 × 512 resolution images is found to lead the model to focus on low-level details over large-scale semantics, yielding sup-optimal results. So a base model is first used to generate 16 × 16 latent embeddings (corresponding to 256 × 256 images), followed by a super-resolution model that upsamples base latent embeddings to 64 × 64 latent embeddings (corresponding to 512 × 512 images).\nLow-resolution embeddings are passed into a series of self-attention transformer layers. The updated embeddings are concatenated with text embeddings. Then it is fused with high-resolution embeddings via cross-attention. In this way, the predicted high-resolution tokens take into account both its masked input as well as low-resolution tokens and text embeddings.\nMuse super-resolution model. Source: paper\nBesides super-resolution module, there are many other engineering designs over MaskGIT.\nDecoder finetuning. To further improve the model’s ability to generate fine details, the VQ-GAN decoder is added with more layers and fine-tuned, while keeping the VQ-GAN encoder, codebook and transformers fixed.\nMasking rate. As was done in MaskGIT, the model is trained with a variable masking rate, however the masking rate used in Muse is from a truncated $\\arccos$ distribution with density function $p(r) = \\frac{2}{\\pi}(1-r^2)^{-1/2}$. The expectation of this distribution is $0.64$, with a bias towards higher masking rates. The authors claim that masking is not only critical for the sampling algorithm, but it also enables a number of zero-shot, out-of-the-box editing capabilities.\nClassifier Free Guidance. At training time, text prompts in 10% of samples chosen randomly are removed, with image embedding self-attention substituted for text-image cross-attention. At inference time, a conditional logit $p_c$ and an unconditional logit $p_u$ are computed for each masked token. The final logits $p_g$ are formed by moving away from the unconditional logits by an amount $t$, the guidance scale: $$ p_g = (1 + t)\\cdot p_c - tp_u. $$\nThis can also enable negative prompting: the user (or system) inputs negative prompts, then logit associated with the negative prompts $p_n$ replaces $p_u$ in above equation. This can be used to dissociate generated image tokens with negative prompts.\nTraining Dataset. 460 million text-image pairs from Imagen dataset. Model size. The tokenizer is a CNN model with 19 ResNet blocks with codebook of size 8192. A number of base transformer models at different sizes are trained, ranging from 600M to 3B parameters. Loss. The loss follows the same setup with MaskGIT. First, a VQ-GAN autoencoder is trained, with perception loss + GAN loss. Second, BERT is trained with NLL on masked tokens. The super-resolution model is trained after the base model has been trained. Hardware. The model is trained on 512-core TPU-v4 chips with a batch size of 512 for 1M steps, which takes about 1 week. Code. lucidrains/muse-maskgit-pytorch huggingface/amused Image Generation via Diffusions Diffusion model was originally proposed in a paper titled “Deep unsupervised learning using nonequilibrium thermodynamics” ( Citation: Sohl-Dickstein, Weiss \u0026 al., 2015 Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N. \u0026 Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. PMLR. ) , though the method per se does not depend on techniques in statistical mechanics.\nTo learn image generation, the method adds Gaussian noises to training data step by step, until they become almost complete noise at step $T$. The model is trained to predict denoised version of its input at any given step. For this reason, diffusion models are also called “denoising autoencoders”. Some methods make predictions of added noises only, instead of denoising their inputs.\nNote that because there is an analytic formula for normal distribution, we don’t have to generate noisy training samples sequentially. We can directly compute noises needed to add to a training image at any step $t\\in[0,\\ldots,T]$ ( Citation: Ho, Jain \u0026 al., 2020 Ho, J., Jain, A. \u0026 Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33. 6840–6851. ; Citation: Song, Sohl-Dickstein \u0026 al., 2020 Song, Y., Sohl-Dickstein, J., Kingma, D., Kumar, A., Ermon, S. \u0026 Poole, B. (2020). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456. ) .\nWe don’t have to add small noises sequentially. Training data can be prepared in parallel and each sample can be computed in one step. Illustration from Song (2020).\nGiven the above explanation, the training objective is to minimize\n$$ \\ell = \\mathbb{E}_{x, \\epsilon\\sim N(0,1), t}\\| \\epsilon_\\theta(x_t, t) - \\epsilon\\|_2^2 $$\nwhere $x$ are training samples, and $t$ is uniformly sampled from steps ${1, . . . , T }$.\nImagen The Imangen model ( Citation: Saharia, Chan \u0026 al., 2022 Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T. \u0026 (2022). Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35. 36479–36494. ) is a pixel-space diffusion model. It is conceptually simple, with a frozen T5-XXL encoder to map text prompts to embeddings and a 64×64 image diffusion model, followed by two super-resolution diffusion models for generating 256×256 and 1024×1024 images. The U-Net backbone predicts denoised version of its input. All diffusion models are conditioned on text embeddings and use classifier-free guidance.\nThe 64×64 model has 2B parameters, the 256×256 model has 600M paramters, and the 1024×1024 model has 400M parameters. The models are trained with a batch size of 2048 on 256 TPU-v4 chips for 2.5M steps.\nThe authors reported that Imagen depends critically on classifier-free guidance for effective text conditioning.\nIn early days of diffusion models, to improve sample quality, a separate image classifier $p(c\\mid z)$ is trained. Classifier-free guidance is an alternative technique that avoids this pretrained model by instead jointly training a single diffusion model on conditional and unconditional objectives via randomly dropping $c$ during training (e.g. with 10% probability). Sampling is performed using $$ \\tilde{\\epsilon}_\\theta(z_t, c) = w\\cdot\\tilde{\\epsilon}_\\theta(z_t, c) + (1 − w)\\cdot\\tilde{\\epsilon}_\\theta(z_t). $$\nImagen model architecture. Source: official website\nHere are some open-source implementations of Imagen:\nlucidrains/imagen-pytorch deep-floyd/if AssemblyAI-Examples/MinImagen Conditional_Diffusion_MNIST Latent Diffusion Model Diffusion in pixel space is too expensive, because there are too many dimensions to keep track of: 512x512 color images have 512x512x3 = 786,432 dimensions, and 1024x1024 images have more than 3 million dimensions. In addition, information in pixel space is often very redundant, containing details that are not semantically important.\nLatent Diffusion Models (LDM) ( Citation: Rombach, Blattmann \u0026 al., 2022 Rombach, R., Blattmann, A., Lorenz, D., Esser, P. \u0026 Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. ) , also known as stable diffusion, map images to latent space with a trained autoencoder VQ-GAN $(\\mathcal{E}, \\mathcal{D})$, and learn U-Net backbones to reverse diffusions (noise-adding) in latent space. This has the benefit of reducing computational cost, and letting the model focus on semantics over details.\nNote that the first author of LDM, Robin Rombach, is also the second author of VQ-GAN.\nU-Net ( Citation: Ronneberger, Fischer \u0026 al., 2015 Ronneberger, O., Fischer, P. \u0026 Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. Springer. ) is a convolutional neural network model that was first proposed to solve medical image segmentation task, where data size is small. For example, in the original paper, it was trained on a dataset of 30 images (512x512 pixels), and another dataset with 35 images. Recently, however, the U-Net backbone used in diffusion models is being replaced by the Transformer architecture.\nAn $H\\times W\\times 3$ image $x\\in\\mathbb{R}^{H\\times W\\times 3}$ will be mapped to $h\\times w\\times c$ latent tensor $z\\in\\mathbb{R}^{h\\times w\\times c}$. Some specific examples of $h\\times w\\times c$ are\n64 x 64 x 3 (12,288 dims in total), 32 x 32 x 4 (4096 dims in total), 16 x 16 x 8 (2048 dims in total). For conditional generation, text and image prompts are projected to vectors via an encoder $\\tau_\\theta$, then fused with U-Net intermediate layers via cross-attention. The loss is now\n$$\\ell = \\mathbb{E}_{y, \\epsilon\\sim N(0,1), t}\\| \\epsilon_\\theta(z_t, t, \\tau_\\theta(y)) - \\epsilon\\|_2^2.$$\nLatent Diffusion Model (LDM) architecture. Source: paper\nControlNet ControlNet ( Citation: Zhang, Rao \u0026 al., 2023 Zhang, L., Rao, A. \u0026 Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. ) is an adapter-tuned stable diffusion model for generating images via image guidance like edge maps, human pose skeletons, segmentation maps, depth, and normals. For each of some U-Net blocks, specifically those encoder blocks, a trainable copy is added with zero convolution to process conditioning images and add the adapter output to the original block output.\nControlNet architecture. Source: paper\nThe model is trained on (conditioning image, image) datasets, for example 3M canny edge image-caption pairs, 80K pose-image-caption pairs, 20K segmentation-image-caption pairs, 3M depth-image-caption pairs and more.\nWeights \u0026 biases of the zero convolution (1x1 convolution) layers are initialized to zero, so at the start of training, the adapters output zero and the model outputs similar high quality images as the pre-trained SD model. The authors noted an interesting phenomenon of “sudden convergence”: the model suddenly learns to follow the input condition only at a certain training step.\nSudden convergence phenomenon in ControlNet training. Source: paper\nAnydoor Anydoor ( Citation: Chen, Huang \u0026 al., 2023 Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D. \u0026 Zhao, H. (2023). Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481. ) is a fine-tuned stable diffusion model developed by researchers from Alibaba for placing objects in an image into another image at a specified location. It targets e-commerce applications:\nIt can be used for placing a product into some desired background. Given an apparel as the object, and a model picture as the scene image, the model can place the cloth onto the model, generating try-on images. Anydoor can be applied to e-commerce tryon scenario\nArchitecture Anydoor model architecture. Source: paper\nThe architectural design is as follows: for the object image, first use a (freezed) segmentor model to remove the background, align the object to the image center, then feed the image to an ID extractor (freezed DINO-v2 + trainable linear layer) to extract the features as a $$ T^{257\\times1024} = \\mathrm{Linear}(T_g^{1\\times1536}, T_p^{256\\times1536}) $$ tensor, where $T_g^{1\\times1536}$ is a global feature, and $T_p^{256\\times1536}$ is a patch feature.\nThe extracted ID embeddings lack detail. To preserve high fidelity while also allow for diversity like gesture, lighting, and orientation, a high frequency map is used to extract fine details of the ID image: $$ I_h = (I\\otimes K_h + I\\otimes K_v) \\odot I \\odot M_{\\mathrm{erode}} $$ where $K_h$ and $K_v$ are horizontal and vertical Sobel kernels, $\\otimes$ is convolution, $\\odot$ is Hadamard product (element-wise matrix multiplication), $M_{\\mathrm{erode}}$ is an eroded mask used to filter out the information near the outer contour of the target object. See below for visualization of this transformation.\nVisualization of HF maps\nThe HF-map is then stitched to the scene image at specified location, and a trainable detail extractor extracts features from the stitched image. The ID embeddings and the stitched image embeddings are then fused to the U-Net backbone via cross-attention. The model then output images that should be a harmonic composition of the object at the scene. The ID embeddings are injected into each U-Net layer, while the detail maps are concatenated with U-Net decoder features at each resolution. The U-Net encoder is freezed and only the U-Net decoder is trained.\nTraining Dataset. The training data consists of 23,783 videos, 137,105 multi-view images, and 249,606 single-view images. The ideal training samples are image pairs for “the same object in different scenes”, which are difficult to obtain. To deal with this problem, the authors utilize video datasets to capture different frames containing the same object. Given a clip, the authors first sample two frames and segment the instances within each frame. Then, one instance from one frame is selected as the target object, and the same instance on the other frame is treated as the training supervision (i.e., the desired model output).\nit is observed that initial denoising steps of a diffusion model often focus on generating the overall structure, while later steps cover fine details like textures and colors. Thus, video data is sampled with more chance in early denoising steps (large T) to better learn the appearance changes, and image data is sampled with more chance in late denoising steps (small T) to learn fine details.\nCode. ali-vilab/AnyDoor\nIP-Adapter Given an input human face image and a prompt, the IP-Adapter model ( Citation: Ye, Zhang \u0026 al., 2023 Ye, H., Zhang, J., Liu, S., Han, X. \u0026 Yang, W. (2023). Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721. ) generates output image of that character under the scene described by the prompt. It is an adapter fine-tuned stable diffusion model. An Image encoder is added as trainable layers to process input images. Input image embeddings and text embeddings are fused via cross attention to the denoising U-Net backbone. The model has around 22M parameters.\nArchitecture IP-Adapter model architecture. Source: paper\nTraining Loss. Same with stable diffusion, with classifier-free guidance. Dataset. 10M text-image pairs from two open source datasets, LAION-2B and COYO-700M. Images are resized to 512 × 512 resolution. Hardware. Trained on a single machine with 8 V100 GPUs for 1M steps. Code. tencent-ailab/IP-Adapter PhotoMaker As with IP-Adapter, the goal of the PhotoMaker model ( Citation: Li, Cao \u0026 al., 2023 Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M. \u0026 Shan, Y. (2023). PhotoMaker: Customizing realistic human photos via stacked ID embedding. arXiv preprint arXiv:2312.04461. ) is to re-draw the background and/or style described by text prompts while preserving the facial identity of a set of reference human images. Design of the model architecture comes from the observation that pre-trained CLIP models may associate words like “woman”, “man”, “girl” and “boy” to humans, and thus fusing those words’ embeddings with the image could guide the diffusion model to preserve the subject in the image.\nAfter training, those models can output personalized image given an input reference image at reference time, without having to perform fine-tuning for each input instance. This is in contrast with earlier models like DreamBooth ( Citation: Ruiz, Li \u0026 al., 2023 Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M. \u0026 Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. ) , which fine-tunes diffusion model on each batch of reference images for personalization, which is expensive.\nArchitecture A CLIP image encoder $\\mathcal{E}_{img}$ is used to map $N$ human images supplied by the user to image embeddings $\\{e^i\\in\\mathbb{R}^D\\}_{i=1\\ldots N}$, and a CLIP text encoder is used to map text prompt tokens of length $L$ to text embeddings $t\\in\\mathbb{R}^{L\\times D}$. Non-human parts of input images are filled with noises to remove influence of the background, and additional trainable layers are added to the image encoder to fine-tune for such non-natural inputs.\nPhotoMaker model architecture\nDuring inference, a “class” word from a set like {woman, man, girl, boy, young, old} is required in the text prompt. The text embedding $t_c\\in\\mathbb{R}^D$ that corresponds to the class word will be extracted and fused with each image embedding $e^i, i=1\\ldots N$ through MLPs to get updated image embeddings $\\{\\hat{e}^i\\in\\mathbb{R}^D\\}_{i=1\\ldots N}$. We then concatenate them to get the concatenated embedding (called “stacked id embedding” in the paper) $$ s = (\\hat{e}^1,\\ldots,\\hat{e}^N)\\in\\mathbb{R}^{N\\times D}. $$\nThe conditional used to perform cross-attentions with the U-Net is defined as\n$$ t^* = [t_1,\\ldots, s, \\ldots, t_L]\\in\\mathbb{R}^{(L-1 + N)\\times D}. $$\nWhere $s$ replaces the vector corresponding to the class word $t_c$. Specifically, the cross-attention calculation is $$ Q = W_Q\\cdot\\phi(z_t) $$ $$ K = W_K\\cdot t^* $$ $$ V = W_V \\cdot t^* $$ $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)\\cdot V $$ Attention matrices are fine-tuned by LoRA. The rest of the diffusion model remains unchanged.\nTraining Dataset. A dataset of 112K+ (image, caption, ID mask) triples of 13,000 people is collected, for an average of 8.6 pictures per person. The diffusion model used is SDXL ( Citation: Podell, English \u0026 al., 2023 Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J. \u0026 Rombach, R. (2023). SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. ) , resolution of training data is 1024x1024. Hardware. The authors reported training on 8 Nvidia A100 GPUs for two weeks with a batch size of 48. For inference, a minimum of 15GB GPU memory is required. Test. The model shows impressive ability to preserve features presented in input image, like general facial shapes. But quite often the generated person is “different”. Rather, it looks like some kind of avarage of training images. In addition, there are noticeable artifacts in teeth and ears. PhotoMaker test. The left one is the input image (Huiwen Chang). The middle one is generated with the prompt ‘an asian woman img sitting at the beach, with purple sunset’. The third one is generated with the prompt ‘An asian woman in front of a fountain at a garden img’. Source: author\nCode. TencentARC/PhotoMaker References ◎ Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D. \u0026 Zhao, H. (2023). Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481. ◎ Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. \u0026 (2021). Learning transferable visual models from natural language supervision. PMLR. ◎ Zhang, L., Rao, A. \u0026 Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. ◎ Ho, J., Jain, A. \u0026 Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33. 6840–6851. ◎ Milletari, F., Navab, N. \u0026 Ahmadi, S. (2016). V-net: Fully convolutional neural networks for volumetric medical image segmentation. Ieee. ◎ Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N. \u0026 Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. PMLR. ◎ Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M. \u0026 Aberman, K. (2023). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. ◎ Lin, T., Goyal, P., Girshick, R., He, K. \u0026 Dollár, P. (2017). Focal loss for dense object detection. ◎ Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T. \u0026 (2022). Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35. 36479–36494. ◎ Ye, H., Zhang, J., Liu, S., Han, X. \u0026 Yang, W. (2023). Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721. ◎ Rombach, R., Blattmann, A., Lorenz, D., Esser, P. \u0026 Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. ◎ He, K., Chen, X., Xie, S., Li, Y., Dollár, P. \u0026 Girshick, R. (2022). Masked autoencoders are scalable vision learners. ◎ Chang, H., Zhang, H., Jiang, L., Liu, C. \u0026 Freeman, W. (2022). Maskgit: Masked generative image transformer. ◎ Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M., Murphy, K., Freeman, W., Rubinstein, M. \u0026 (2023). Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704. ◎ Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M. \u0026 Shan, Y. (2023). PhotoMaker: Customizing realistic human photos via stacked ID embedding. arXiv preprint arXiv:2312.04461. ◎ Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J. \u0026 Rombach, R. (2023). SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. ◎ Song, Y., Sohl-Dickstein, J., Kingma, D., Kumar, A., Ermon, S. \u0026 Poole, B. (2020). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456. ◎ Ronneberger, O., Fischer, P. \u0026 Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. Springer. ◎ Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. \u0026 (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. ◎ Esser, P., Rombach, R. \u0026 Ommer, B. (2021). Taming transformers for high-resolution image synthesis. ◎ Van Den Oord, A., Vinyals, O. \u0026 (2017). Neural discrete representation learning. Advances in neural information processing systems, 30. ","wordCount":"5784","inLanguage":"en","image":"https://lifei.ai/imgs/vision/cover.jpeg","datePublished":"2024-02-12T00:00:00Z","dateModified":"2024-02-12T00:00:00Z","author":{"@type":"Person","name":"Fei Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://lifei.ai/posts/2024-02-12-vision-models/"},"publisher":{"@type":"Organization","name":"lifei.ai | AI blogs","logo":{"@type":"ImageObject","url":"https://lifei.ai/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lifei.ai/ accesskey=h title="lifei.ai (Alt + H)"><img src=https://lifei.ai/f.png alt aria-label=logo height=40>lifei.ai</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://lifei.ai/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://lifei.ai/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://lifei.ai/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://lifei.ai/>Home</a>&nbsp;»&nbsp;<a href=https://lifei.ai/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Vision Models</h1><div class=post-meta><span title='2024-02-12 00:00:00 +0000 UTC'>February 12, 2024</span>&nbsp;·&nbsp;28 min&nbsp;·&nbsp;Fei Li</div></header><figure class=entry-cover><img loading=eager src=https://lifei.ai/imgs/vision/cover.jpeg alt="post cover"><p>source: pinterest</p></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#vision-understanding>Vision Understanding</a><ul><li><a href=#vit>ViT</a></li><li><a href=#clip>CLIP</a></li><li><a href=#mae>MAE</a></li><li><a href=#sam>SAM</a></li></ul></li><li><a href=#image-generation-via-discrete-tokenization>Image Generation via Discrete Tokenization</a><ul><li><a href=#vq-vae>VQ-VAE</a></li><li><a href=#vq-gan>VQ-GAN</a></li><li><a href=#maskgit>MaskGIT</a></li><li><a href=#muse>Muse</a></li></ul></li><li><a href=#image-generation-via-diffusions>Image Generation via Diffusions</a><ul><li><a href=#imagen>Imagen</a></li><li><a href=#latent-diffusion-model>Latent Diffusion Model</a></li><li><a href=#controlnet>ControlNet</a></li><li><a href=#anydoor>Anydoor</a></li><li><a href=#ip-adapter>IP-Adapter</a></li><li><a href=#photomaker>PhotoMaker</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>At the same time LLMs are changing the world, we have witnessed fast progress in AI image and video generation. AI models are now able to synthesis high quality, high fidelity and high resolution images via text prompting. Mainstream generative model architectures have shifted from VAEs, flows and GANs to diffusions and transformers. In this post I take notes of several vision models. It may be regularly updated to reflect latest research development.</p><h2 id=vision-understanding>Vision Understanding<a hidden class=anchor aria-hidden=true href=#vision-understanding>#</a></h2><h3 id=vit>ViT<a hidden class=anchor aria-hidden=true href=#vit>#</a></h3><p>Vision Transformer (ViT)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#vit><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Alexey"><span itemprop=familyName>Dosovitskiy</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Lucas"><span itemprop=familyName>Beyer</span></span>
<em>& al.</em>, <span itemprop=datePublished>2020</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dosovitskiy</span>, <meta itemprop=givenName content="Alexey">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Beyer</span>, <meta itemprop=givenName content="Lucas">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kolesnikov</span>, <meta itemprop=givenName content="Alexander">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Weissenborn</span>, <meta itemprop=givenName content="Dirk">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhai</span>, <meta itemprop=givenName content="Xiaohua">X.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Unterthiner</span>, <meta itemprop=givenName content="Thomas">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dehghani</span>, <meta itemprop=givenName content="Mostafa">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Minderer</span>, <meta itemprop=givenName content="Matthias">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Heigold</span>, <meta itemprop=givenName content="Georg">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gelly</span>, <meta itemprop=givenName content="Sylvain">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>An image is worth 16x16 words: Transformers for image recognition at scale</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2010.11929</span></i>.</span>
</span></span>)</span>
applies the transformer architecture to vision inputs. Images are segmented to patches, and the patches are embedded as sequence of vectors, with patch ordering as position embedding. The figure below illustrates the architecture. The model achieves superior performance than convolutional neural networks (CNNs) in many tasks, and has become a basic component in other subsequent vision models. Refer to <a href=https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/simple_vit.py>@lucidrains/vit-pytorch</a> for a simple implementation of ViT.</p><figure class=align-center><img loading=lazy src=/imgs/vision/vit.webp#center alt="ViT architecture. Source: Machine Learning Mastery"><figcaption><p>ViT architecture. Source: <a href=https://machinelearningmastery.com/the-vision-transformer-model/>Machine Learning Mastery</a></p></figcaption></figure><h3 id=clip>CLIP<a hidden class=anchor aria-hidden=true href=#clip>#</a></h3><p>CLIP
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#clip><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Alec"><span itemprop=familyName>Radford</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jong Wook"><span itemprop=familyName>Kim</span></span>
<em>& al.</em>, <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Radford</span>, <meta itemprop=givenName content="Alec">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kim</span>, <meta itemprop=givenName content="Jong Wook">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Hallacy</span>, <meta itemprop=givenName content="Chris">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ramesh</span>, <meta itemprop=givenName content="Aditya">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goh</span>, <meta itemprop=givenName content="Gabriel">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Agarwal</span>, <meta itemprop=givenName content="Sandhini">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sastry</span>, <meta itemprop=givenName content="Girish">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Askell</span>, <meta itemprop=givenName content="Amanda">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Mishkin</span>, <meta itemprop=givenName content="Pamela">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Clark</span>, <meta itemprop=givenName content="Jack">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name><i>Learning transferable visual models from natural language supervision</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>PMLR</span></span>.</span>
</span></span>)</span>
is a foundational model developed by OpenAI that set the stage for multi-modality modeling. Traditionally, image classification models were trained to map input to a fixed set of classes, which is inflexible. By jointly training text and image embeddings on massive natural text and image pairs, CLIP models learn associations of images with texts, which allows it to perform open-set classification.</p><h4 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h4><p>The CLIP model consists of a text encoder and an image encoder, trained on large collection of (text, image) pairs. For image encoder, two architectures are experimented, ResNet and ViT, and ViT was found to perform better than ResNet. The text encoder is a Transformer, the base one has 12 layers and 512 embedding dimensions, for a total of 63M parameters, with a vocab size 49152, and sequence length capped at 76.</p><p>Dot product between text embedding and image embedding of a pair $(T_i, I_i)$ is maximized, and dot product for pairs
$$
\{(T_i, I_j)\}_{j\neq i}
$$
and
$$
\{(T_j, I_i)\}_{j\neq i}
$$
are minimized, where $I_j$ and $T_j$ are other texts and images in the same batch. A large batch size of 32768 is used in the paper. In the figure below, this means maximizing cross entropy of the first row ➕ cross entropy of the first column, cross entropy of the second row ➕ cross entropy of the second column, and so on.</p><blockquote><p>[Further Detail] The text sequence is bracketed with <code>[SOS]</code> and <code>[EOS]</code> tokens and the activations of the highest layer of the transformer at the <code>[EOS]</code> token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space.</p></blockquote><figure class=align-center><img loading=lazy src=/imgs/vision/clip.png#center alt="CLIP model architecture. Source: paper"><figcaption><p>CLIP model architecture. Source: paper</p></figcaption></figure><h4 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h4><ol><li><strong>Dataset.</strong> 400 million non-open (text, image) pairs.</li><li><strong>Loss.</strong> The loss is cross entropy loss that maximizes probability for (text, image) pairs in the training data and minimizes probability for not-paired texts and images in training batches. The paper provided the following pseudocode:</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>I_e</span><span class=p>,</span> <span class=n>T_e</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>t</span><span class=p>)</span>  <span class=c1># t is temperature</span>
</span></span><span class=line><span class=cl><span class=n>labels</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss_i</span> <span class=o>=</span> <span class=n>cross_entropy_loss</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss_t</span> <span class=o>=</span> <span class=n>cross_entropy_loss</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>loss_i</span> <span class=o>+</span> <span class=n>loss_t</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span>
</span></span></code></pre></td></tr></table></div></div><ol start=3><li><strong>Code.</strong> <a href=https://huggingface.co/docs/transformers/model_doc/clip>huggingface - CLIP</a></li></ol><p>Note that CLIP is a discriminative model. It can be used for image search and zero-shot image classification, but it is not a generative model, and it cannot be directly used to generate images. It is often served as a text encoder component in many image generation models.</p><h3 id=mae>MAE<a hidden class=anchor aria-hidden=true href=#mae>#</a></h3><p>Masked Autoencoder (MAE)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#mae><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Kaiming"><span itemprop=familyName>He</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Xinlei"><span itemprop=familyName>Chen</span></span>
<em>& al.</em>, <span itemprop=datePublished>2022</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>He</span>, <meta itemprop=givenName content="Kaiming">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Xinlei">X.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Saining">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Yanghao">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dollár</span>, <meta itemprop=givenName content="Piotr">P.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Girshick</span>, <meta itemprop=givenName content="Ross">R.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name><i>Masked autoencoders are scalable vision learners</i></span>.</span>
</span></span>)</span>
is a ViT-based denoising autoencoder that learns image representations. 75% patches of training images are masked and the objective is to reconstruct the masked pixel values, with MSE between predicted vector of pixels and original pixel values as loss. The encoder is a ViT model that is only applied to unmasked patches. The decoder is a smaller ViT model that is only used for reconstruction during training, but is discarded afterwards since only encoder outputs are used in downstream tasks.</p><figure class=align-center><img loading=lazy src=/imgs/vision/mae.png#center alt="MAE model architecture. Source: paper"><figcaption><p>MAE model architecture. Source: paper</p></figcaption></figure><h3 id=sam>SAM<a hidden class=anchor aria-hidden=true href=#sam>#</a></h3><p>Segment Anything (SAM) is an interactive image segmentation model released by Meta AI. The model can be used for image and video editing, and can also be integrated into AR/VR headset to select objects.</p><figure class=align-center><img loading=lazy src=/imgs/vision/sam-cover.png#center alt="Source: paper"><figcaption><p>Source: paper</p></figcaption></figure><h4 id=architecture-1>Architecture<a hidden class=anchor aria-hidden=true href=#architecture-1>#</a></h4><p>SAM consists of a one-time image encoder, a prompt encoder and a lightweight mask decoder. The image encoder is a ViT model that produces embedding for an input image. It has 632M parameters, which takes ~0.15 seconds to run on an NVIDIA A100 GPU. The prompt encoder maps points, boxes and texts via CLIP and CNNs. The decoder is a transformer that predicts object masks from image embeddings and prompt embeddings. The prompt encoder and mask decoder have 4M parameters, and take ~50ms on CPU in the browser.</p><p>Note that the model only predicts binary masks, but not labels (chairs, cars, etc.).<figure class=align-center><img loading=lazy src=/imgs/vision/sam.png#center alt="SAM model architecture. Image embedding can be computed once per image in the backend on GPUs. After the embedding is obtained, masking prediction can run very fast in the browser, which enables real-time interactive prompting. Source: https://segment-anything.com"><figcaption><p>SAM model architecture. Image embedding can be computed once per image in the backend on GPUs. After the embedding is obtained, masking prediction can run very fast in the browser, which enables real-time interactive prompting. Source: <a href=https://segment-anything.com>https://segment-anything.com</a></p></figcaption></figure></p><h4 id=training-1>Training<a hidden class=anchor aria-hidden=true href=#training-1>#</a></h4><ol><li><strong>Dataset.</strong> The model is trained on 11M images with 1B+ masks. The dataset called SA-1B is released <a href=https://segment-anything.com/>here</a> along with the model.</li><li><strong>Loss.</strong> Mask prediction is supervised with linear combination of focal loss
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#focal><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Tsung-Yi"><span itemprop=familyName>Lin</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Priya"><span itemprop=familyName>Goyal</span></span>
<em>& al.</em>, <span itemprop=datePublished>2017</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lin</span>, <meta itemprop=givenName content="Tsung-Yi">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goyal</span>, <meta itemprop=givenName content="Priya">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Girshick</span>, <meta itemprop=givenName content="Ross">R.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>He</span>, <meta itemprop=givenName content="Kaiming">K.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dollár</span>, <meta itemprop=givenName content="Piotr">P.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name><i>Focal loss for dense object detection</i></span>.</span>
</span></span>)</span>
and dice loss
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#dice><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Fausto"><span itemprop=familyName>Milletari</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Nassir"><span itemprop=familyName>Navab</span></span>
<em>& al.</em>, <span itemprop=datePublished>2016</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Milletari</span>, <meta itemprop=givenName content="Fausto">F.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Navab</span>, <meta itemprop=givenName content="Nassir">N.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ahmadi</span>, <meta itemprop=givenName content="Seyed-Ahmad">S.</span>
 
(<span itemprop=datePublished>2016</span>).
 <span itemprop=name><i>V-net: Fully convolutional neural networks for volumetric medical image segmentation</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>Ieee</span></span>.</span>
</span></span>)</span>
. The focal loss is a modulated cross entropy loss:
$$
\mathrm{FL}(p) = -\alpha(1-p)^\gamma\log(p).
$$
The dice loss measures overlap between predicted and ground-truth labels. Let $y_i$ denote label, the formula is
$$
\mathrm{Dice}(p) = 1 - \frac{2\sum y_i\cdot p_i}{y_i^2 + p_i^2+1}
$$</li><li><strong>Hardware.</strong> The model was trained for 3~5 days on 256 A100 GPUs.</li></ol><p>More details can be found at <a href=https://segment-anything.com/>https://segment-anything.com/</a>.</p><h2 id=image-generation-via-discrete-tokenization>Image Generation via Discrete Tokenization<a hidden class=anchor aria-hidden=true href=#image-generation-via-discrete-tokenization>#</a></h2><h3 id=vq-vae>VQ-VAE<a hidden class=anchor aria-hidden=true href=#vq-vae>#</a></h3><p>VQ-VAE
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#vq-vae><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Aaron"><span itemprop=familyName>Van Den Oord</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Oriol"><span itemprop=familyName>Vinyals</span></span>
<em>& al.</em>, <span itemprop=datePublished>2017</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Van Den Oord</span>, <meta itemprop=givenName content="Aaron">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Vinyals</span>, <meta itemprop=givenName content="Oriol">O.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Neural discrete representation learning</span>.<i>
<span itemprop=about>Advances in neural information processing systems</span>, 30</i>.</span>
</span></span>)</span>
is a seminal paper published by Google DeepMind researchers in 2017. VQ-VAE is a way to regularize the latents, by forcing the decoder to only allocate probability mass to a finite set of learned vectors. The model was proposed to address the &ldquo;posterior collapse&rdquo; issue in VAEs, which is the phenomenon that the decoder often ignores the latents, producing blurry and monotonous images.</p><figure class=align-center><img loading=lazy src=/imgs/vision/vq-vae.png#center alt="VQ-VAE Architecture. Source: ResearchGate"><figcaption><p>VQ-VAE Architecture. Source: <a href=https://www.researchgate.net/figure/Our-method-uses-a-VQ-VAE-to-learn-the-latent-discrete-representation-of-brain-data-This_fig1_360377288>ResearchGate</a></p></figcaption></figure><p>VQ-VAE consists of two parts: an autoencoder that learns discrete representation of images, and an autoregressive model that learns to generate latents. The encoder maps input images of shape $H\times W\times C$ to tensor $z$ of shape $h\times w\times c$. The downsampling factor $f=H/h=W/w$ is often set to $8$ or $16$. The latent codes in codebook, which serve as input to the decoder, are also designed to have the same channel dimension $c$. As clearly illustrated in the figure above, in forward pass, each spatial element in $z$ is mapped to its closest latent $e_i$ from the codebook $\{e_1,\ldots,e_K\}$ by computing Euclidean distances. However, in backward pass, we need to consider how to back-propagate gradients through this discrete operation. The proposal is simple: just copy the gradient of $e_i$ from the right side to the left side. Since we want the encoder outputs to be close to their learned embeddings, this strategy should be a useful way to adjust the the encoder outputs.</p><p>The loss function for training the autoencoder is</p><p>$$
\ell = \log p(x\mid z_q(x)) + \|sg[z_e(x)] - e\|^2 + \beta\cdot\|z_e(x) - sg[e]\|^2,
$$</p><p>where sg stands for &ldquo;stop gradient&rdquo; and it is the <code>.detach()</code> operation in PyTorch. This means, for the second term, we only update the embedding vector $e$, treating $z_e(x)$ as a constant; for the third term, we only update $z_e(x)$, treating $e$ as a constant.</p><p>Here is a breakdown of the three terms:</p><ol><li>The <strong>decoder</strong> optimizes $\log p(x\mid z_q(x))$ only.</li><li>To make sure the encoder commits to an embedding and its output does not grow, a commitment loss is added as the third term. The <strong>encoder</strong> optimizes
$$
\log p(x\mid z_q(x)) + \beta\cdot\|z_e(x) - sg[e]\|^2.
$$</li><li>The <strong>embeddings</strong> are optimized by the middle term $\|sg[z_e(x)] - e\|^2$.</li></ol><p>Note that under the assumption that the decoder output is Gaussian:
$$
p(x\mid z_q(x)) \sim e^{-\|x-\mu\|^2},
$$
the first loss term reduces to mean squared error (MSE) loss. So when it comes down to implementation, the loss is directly implemented as
$$
\ell_{\mathrm{VQ-VAE}} = \ell_{recon} + \ell_{codebook} + \beta\cdot\ell_{commit}
$$
with
$$
\ell_{recon} = \|x-\hat{x}\|^2,\quad \ell_{codebook} = \|sg[z_e(x)] - e\|^2,\quad \ell_{commit} = \|z_e(x) - sg[e]\|^2.
$$</p><p>The encoder and decoder used in the original VQ-VAE paper are CNNs, and after the autoencoder is trained, an <a href=https://lifeitech.github.io/posts/generative-models/#autoregressive-flows>autoregressive model (PixelCNN)</a> is learned to autoregressively generate tokens in the latent space. Nowadays, the PixelCNN model has been substituted with transformers. In the original experiment, x = 128 x 128 x 3 images are mapped to z = 32 × 32 × 1 tensors, and the codebook consists of K=512 vectors each of dimension 1. This means that each each of the 32 x 32 = 1024 latent value comes from one of 512 values.</p><p>Below are two open-source PyTorch implementations of VQ-VAE:</p><ul><li><a href=https://github.com/karpathy/deep-vector-quantization>karpathy/deep-vector-quantization</a></li><li><a href=https://github.com/lucidrains/vector-quantize-pytorch>lucidrains/vector-quantize-pytorch</a></li></ul><p>VQ-VAE can be used to generate images: first generate tokens in latent space by the trained generative model, then use the decoder to map generated latents to pixel space. An important architectural improvement, the VQ-GAN model, successfully substituted PixelCNN with transformers, and helped further popularize the approach.</p><h3 id=vq-gan>VQ-GAN<a hidden class=anchor aria-hidden=true href=#vq-gan>#</a></h3><p>VQ-GAN
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#vq-gan><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Patrick"><span itemprop=familyName>Esser</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Robin"><span itemprop=familyName>Rombach</span></span>
<em>& al.</em>, <span itemprop=datePublished>2021</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Esser</span>, <meta itemprop=givenName content="Patrick">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rombach</span>, <meta itemprop=givenName content="Robin">R.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ommer</span>, <meta itemprop=givenName content="Bjorn">B.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name><i>Taming transformers for high-resolution image synthesis</i></span>.</span>
</span></span>)</span>
is an improvement over the VQ-VAE model, at a time when the Transformer architecture became popular. Here are the differences:</p><ol><li>The PixelCNN model used in the VQ-VAE paper to generate tokens in latent space is replaced by a <strong>Transformer</strong>, though the training objective is still autoregressive, i.e. the latent tensor is flattened to a 1D vector and the model is trained to predict the next token given all previous tokens. The encoder and the decoder are still CNNs.</li><li>The MSE loss $\|x-\hat{x}\|^2$ used in VQ-VAE is replaced with a &ldquo;perceptual loss&rdquo;, meaning that MSE is calculated between feature maps $\|f(x)-f(\hat{x})\|^2$ for some CNN model $f$, instead of between original images in pixel space. Further, a GAN objective is added to the original VQ-VAE objective:
$$
\ell_{GAN} = \log D(x) + \log(1 - D(\hat{x}))
$$
where $D$ is a patch-based discriminator. The complete objective is
$$
\ell = \ell_{\mathrm{VQ-VAE}} + \lambda\cdot\ell_{GAN}
$$
where $\lambda$ is a dynamically-adjusted weight (refer to paper for detail).</li></ol><figure class=align-center><img loading=lazy src=/imgs/vision/vq-gan.png#center alt="VQ-GAN Architecture. Source: paper"><figcaption><p>VQ-GAN Architecture. Source: paper</p></figcaption></figure><p>Recently, Google&rsquo;s series of work that are built upon the VQ-GAN model demonstrated the potential of masked token modeling approach to visual content generation. The approach is to learn an autoencoder tokenizer to map images or videos to discrete tokens, then train a transformer model for masked token prediction. Here we briefly mention some of them.</p><figure class=align-center><img loading=lazy src=/imgs/vision/masked-google.png#center alt="Two different approaches to image generation. Source: author"><figcaption><p>Two different approaches to image generation. Source: author</p></figcaption></figure><h3 id=maskgit>MaskGIT<a hidden class=anchor aria-hidden=true href=#maskgit>#</a></h3><p>MaskGIT
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#maskgit><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Huiwen"><span itemprop=familyName>Chang</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Han"><span itemprop=familyName>Zhang</span></span>
<em>& al.</em>, <span itemprop=datePublished>2022</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chang</span>, <meta itemprop=givenName content="Huiwen">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Han">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jiang</span>, <meta itemprop=givenName content="Lu">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Liu</span>, <meta itemprop=givenName content="Ce">C.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Freeman</span>, <meta itemprop=givenName content="William T">W.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name><i>Maskgit: Masked generative image transformer</i></span>.</span>
</span></span>)</span>
is an improvement over VQ-GAN. In VQ-GAN, tokens are modeled using a GPT-style autoregressive Transformer, which treats visual tokens as a 1D sequence and predicts the next token given previously generated tokens. In MaskGIT, this GPT model is substituted with a BERT model that predicts masked tokens. Note that MaskGIT is a foundational model trained mainly for unconditional image generation, which allows text-to-image models to be built upon it for incorporating more modalities and image editing applications.</p><h4 id=architecture-2>Architecture<a hidden class=anchor aria-hidden=true href=#architecture-2>#</a></h4><p>The model follows the same two-stage design as with VQ-GAN. First, an autoencoder is trained to learn to represent images with discrete tokens. Second, a BERT model is learnt to predict masked tokens. The purpose of the paper is to mainly investigate the <strong>second stage</strong>, so the first stage follows the same setup as in the VQ-GAN.<figure class=align-center><img loading=lazy src=/imgs/vision/maskgit.png#center alt="MaskGIT Architecture. Source: paper"><figcaption><p>MaskGIT Architecture. Source: paper</p></figcaption></figure></p><p>In theory, the model is able to infer all tokens and generate the entire image in a single pass. However, the authors found that this doesn&rsquo;t work, so a gradual sampling procedure is employed, where at each step only predictions with high probabilities are retained, and others tokens are masked again.</p><p>A decreasing mask function $\gamma(r)\in(0,1]$ is used to mask tokens during training and inference. A number of functions are experimented, and linear and concave ones like cosine ($\cos x$), square ($1-x^2$), and cubic ($1-x^3$) are found to be the best ones. During training, a ratio $r$ is sampled from 0 to 1, then $\gamma(r)\cdot N$ tokens in $Y$ are uniformly selected for masking.</p><p>The decoding algorithm synthesizes an image in $T$ steps. At each iteration, the model predicts all tokens simultaneously but only keeps the most confident ones. The remaining tokens are masked out and re-predicted in the next iteration. The mask ratio is made decreasing until all tokens are generated within $T$ iterations. $T=8$ to $12$ iterations are found to yield best results.</p><h4 id=training-2>Training<a hidden class=anchor aria-hidden=true href=#training-2>#</a></h4><ol><li><p><strong>Loss.</strong> After the tokenizer is trained, a BERT model is trained to predict masked tokens. The training loss is cross entropy loss on masked tokens.
$$
\mathcal{L} = -\mathbb{E}_{Y\in\mathcal{D}}\left[\sum_{\forall i\in[1, N], m_i=1}\log p\left(y_i\mid Y_{\overline{M}}\right)\right]
$$
where $\mathcal{D}$ is dataset, $Y=[y_i]_1^N$ is image tokens with length $N$, $M=[m_i]_1^N$ is the corresponding binary mask.</p></li><li><p><strong>Dataset.</strong> The model is trained on ImageNet 256x256 & 512x512 and <a href=https://www.kaggle.com/datasets/nickj26/places2-mit-dataset>Places2</a> dataset. For each dataset, a single autoencoder along with a codebook of 1024 tokens are trained.</p></li><li><p><strong>Hardware.</strong> All models are trained on 4x4 TPU devices with a batch size of 256.</p></li><li><p><strong>Code.</strong> List of implementations is available on <a href=https://paperswithcode.com/paper/maskgit-masked-generative-image-transformer>paperswithcode</a>.</p></li></ol><h3 id=muse>Muse<a hidden class=anchor aria-hidden=true href=#muse>#</a></h3><p>Muse
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#muse><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Huiwen"><span itemprop=familyName>Chang</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Han"><span itemprop=familyName>Zhang</span></span>
<em>& al.</em>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chang</span>, <meta itemprop=givenName content="Huiwen">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Han">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Barber</span>, <meta itemprop=givenName content="Jarred">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Maschinot</span>, <meta itemprop=givenName content="AJ">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lezama</span>, <meta itemprop=givenName content="Jose">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jiang</span>, <meta itemprop=givenName content="Lu">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span>, <meta itemprop=givenName content="Ming-Hsuan">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Murphy</span>, <meta itemprop=givenName content="Kevin">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Freeman</span>, <meta itemprop=givenName content="William T">W.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rubinstein</span>, <meta itemprop=givenName content="Michael">M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Muse: Text-to-image generation via masked generative transformers</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2301.00704</span></i>.</span>
</span></span>)</span>
is a text-conditioned MaskGIT model for real-world image generation application with numerous modifications over the original MaskGIT architecture. A 4.6B-parameter text encoder, T5-XXL, is used to map text prompts to 4096 dim embedding vectors that will be fused with token embeddings via cross attention. The image generation pipeline is split into two parts with a base module and a super-resolution module.</p><figure class=align-center><img loading=lazy src=/imgs/vision/muse.jpeg#center alt="Muse Architecture. Source: paper"><figcaption><p>Muse Architecture. Source: paper</p></figcaption></figure><p>The reason for adding a super-resolution pipeline is that, directly predicting 512 × 512 resolution images is found to lead the model to focus on low-level details over large-scale semantics, yielding sup-optimal results. So a base model is first used to generate 16 × 16 latent embeddings (corresponding to 256 × 256 images), followed by a super-resolution model that upsamples base latent embeddings to 64 × 64 latent embeddings (corresponding to 512 × 512 images).</p><p>Low-resolution embeddings are passed into a series of self-attention transformer layers. The updated embeddings are concatenated with text embeddings. Then it is fused with high-resolution embeddings via cross-attention. In this way, the predicted high-resolution tokens take into account <strong>both</strong> its masked input <strong>as well as</strong> low-resolution tokens and text embeddings.</p><figure class=align-center><img loading=lazy src=/imgs/vision/muse-sr.png#center alt="Muse super-resolution model. Source: paper"><figcaption><p>Muse super-resolution model. Source: paper</p></figcaption></figure><p>Besides super-resolution module, there are many other engineering designs over MaskGIT.</p><p><strong>Decoder finetuning.</strong> To further improve the model’s ability to generate fine details, the VQ-GAN decoder is added with more layers and fine-tuned, while keeping the VQ-GAN encoder, codebook and transformers fixed.</p><p><strong>Masking rate.</strong> As was done in MaskGIT, the model is trained with a variable masking rate, however the masking rate used in Muse is from a truncated $\arccos$ distribution with density function $p(r) = \frac{2}{\pi}(1-r^2)^{-1/2}$. The expectation of this distribution is $0.64$, with a bias towards higher masking rates. The authors claim that masking is not only critical for the sampling algorithm, but it also enables a number of zero-shot, out-of-the-box editing capabilities.</p><p><strong>Classifier Free Guidance.</strong> At training time, text prompts in 10% of samples chosen randomly are removed, with image embedding self-attention substituted for text-image cross-attention. At inference time, a conditional logit $p_c$ and an unconditional logit $p_u$ are computed for each masked token. The final logits $p_g$ are formed by moving away from the unconditional logits by an amount $t$, the <em>guidance scale</em>:
$$
p_g = (1 + t)\cdot p_c - tp_u.
$$</p><p>This can also enable <em>negative prompting</em>: the user (or system) inputs negative prompts, then logit associated with the negative prompts $p_n$ replaces $p_u$ in above equation. This can be used to dissociate generated image tokens with negative prompts.</p><h4 id=training-3>Training<a hidden class=anchor aria-hidden=true href=#training-3>#</a></h4><ol><li><strong>Dataset.</strong> 460 million text-image pairs from Imagen dataset.</li><li><strong>Model size.</strong> The tokenizer is a CNN model with 19 ResNet blocks with codebook of size 8192. A number of base transformer models at different sizes are trained, ranging from 600M to 3B parameters.</li><li><strong>Loss.</strong> The loss follows the same setup with MaskGIT. First, a VQ-GAN autoencoder is trained, with perception loss + GAN loss. Second, BERT is trained with NLL on masked tokens. The super-resolution model is trained after the base model has been trained.</li><li><strong>Hardware.</strong> The model is trained on 512-core TPU-v4 chips with a batch size of 512 for 1M steps, which takes about 1 week.</li><li><strong>Code.</strong><ul><li><a href=https://github.com/lucidrains/muse-maskgit-pytorch>lucidrains/muse-maskgit-pytorch</a></li><li><a href=https://github.com/huggingface/amused>huggingface/amused</a></li></ul></li></ol><h2 id=image-generation-via-diffusions>Image Generation via Diffusions<a hidden class=anchor aria-hidden=true href=#image-generation-via-diffusions>#</a></h2><p>Diffusion model was originally proposed in a paper titled &ldquo;<em>Deep unsupervised learning using nonequilibrium thermodynamics</em>&rdquo;
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#diffusion><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jascha"><span itemprop=familyName>Sohl-Dickstein</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Eric"><span itemprop=familyName>Weiss</span></span>
<em>& al.</em>, <span itemprop=datePublished>2015</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sohl-Dickstein</span>, <meta itemprop=givenName content="Jascha">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Weiss</span>, <meta itemprop=givenName content="Eric">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Maheswaranathan</span>, <meta itemprop=givenName content="Niru">N.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ganguli</span>, <meta itemprop=givenName content="Surya">S.</span>
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name><i>Deep unsupervised learning using nonequilibrium thermodynamics</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>PMLR</span></span>.</span>
</span></span>)</span>
, though the method per se does not depend on techniques in statistical mechanics.</p><p>To learn image generation, the method adds Gaussian noises to training data step by step, until they become almost complete noise at step $T$. The model is trained to predict denoised version of its input at any given step. For this reason, diffusion models are also called &ldquo;denoising autoencoders&rdquo;. Some methods make predictions of added noises only, instead of denoising their inputs.</p><p>Note that because there is an analytic formula for normal distribution, we don&rsquo;t have to generate noisy training samples sequentially. We can directly compute noises needed to add to a training image at any step $t\in[0,\ldots,T]$
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#ddpm><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jonathan"><span itemprop=familyName>Ho</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Ajay"><span itemprop=familyName>Jain</span></span>
<em>& al.</em>, <span itemprop=datePublished>2020</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ho</span>, <meta itemprop=givenName content="Jonathan">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jain</span>, <meta itemprop=givenName content="Ajay">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Abbeel</span>, <meta itemprop=givenName content="Pieter">P.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>Denoising diffusion probabilistic models</span>.<i>
<span itemprop=about>Advances in neural information processing systems</span>, 33</i>. <span itemprop=pagination>6840–6851</span>.</span>
</span></span>; <span class=hugo-cite-group><a href=#song><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Yang"><span itemprop=familyName>Song</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jascha"><span itemprop=familyName>Sohl-Dickstein</span></span>
<em>& al.</em>, <span itemprop=datePublished>2020</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Song</span>, <meta itemprop=givenName content="Yang">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sohl-Dickstein</span>, <meta itemprop=givenName content="Jascha">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kingma</span>, <meta itemprop=givenName content="Diederik P">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kumar</span>, <meta itemprop=givenName content="Abhishek">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ermon</span>, <meta itemprop=givenName content="Stefano">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Poole</span>, <meta itemprop=givenName content="Ben">B.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>Score-based generative modeling through stochastic differential equations</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2011.13456</span></i>.</span>
</span></span>)</span>
.</p><figure class=align-center><img loading=lazy src=/imgs/vision/diffusion-noise.jpeg#center alt="We don&amp;rsquo;t have to add small noises sequentially. Training data can be prepared in parallel and each sample can be computed in one step. Illustration from Song (2020)."><figcaption><p>We don&rsquo;t have to add small noises sequentially. Training data can be prepared in parallel and each sample can be computed in one step. Illustration from Song (2020).</p></figcaption></figure><p>Given the above explanation, the training objective is to minimize</p><p>$$
\ell = \mathbb{E}_{x, \epsilon\sim N(0,1), t}\| \epsilon_\theta(x_t, t) - \epsilon\|_2^2
$$</p><p>where $x$ are training samples, and $t$ is uniformly sampled from steps ${1, . . . , T }$.</p><h3 id=imagen>Imagen<a hidden class=anchor aria-hidden=true href=#imagen>#</a></h3><p>The Imangen model
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#imagen><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Chitwan"><span itemprop=familyName>Saharia</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="William"><span itemprop=familyName>Chan</span></span>
<em>& al.</em>, <span itemprop=datePublished>2022</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Saharia</span>, <meta itemprop=givenName content="Chitwan">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chan</span>, <meta itemprop=givenName content="William">W.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Saxena</span>, <meta itemprop=givenName content="Saurabh">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Lala">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Whang</span>, <meta itemprop=givenName content="Jay">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Denton</span>, <meta itemprop=givenName content="Emily L">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ghasemipour</span>, <meta itemprop=givenName content="Kamyar">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gontijo Lopes</span>, <meta itemprop=givenName content="Raphael">R.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Karagol Ayan</span>, <meta itemprop=givenName content="Burcu">B.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Salimans</span>, <meta itemprop=givenName content="Tim">T.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name>Photorealistic text-to-image diffusion models with deep language understanding</span>.<i>
<span itemprop=about>Advances in Neural Information Processing Systems</span>, 35</i>. <span itemprop=pagination>36479–36494</span>.</span>
</span></span>)</span>
is a pixel-space diffusion model. It is conceptually simple, with a frozen T5-XXL encoder to map text prompts to embeddings and a 64×64 image diffusion model, followed by two super-resolution diffusion models for generating 256×256 and 1024×1024 images. The U-Net backbone predicts denoised version of its input. All diffusion models are conditioned on text embeddings and use classifier-free guidance.</p><p>The 64×64 model has 2B parameters, the 256×256 model has 600M paramters, and the 1024×1024 model has 400M parameters. The models are trained with a batch size of 2048 on 256 TPU-v4 chips for 2.5M steps.</p><p>The authors reported that Imagen depends critically on classifier-free guidance for effective text conditioning.</p><blockquote><p>In early days of diffusion models, to improve sample quality, a separate image classifier $p(c\mid z)$ is trained. Classifier-free guidance is an alternative technique that avoids this pretrained model by instead jointly training a single diffusion model on conditional and unconditional objectives via randomly dropping $c$ during training (e.g. with 10% probability). Sampling is performed using
$$
\tilde{\epsilon}_\theta(z_t, c) = w\cdot\tilde{\epsilon}_\theta(z_t, c) + (1 − w)\cdot\tilde{\epsilon}_\theta(z_t).
$$</p></blockquote><figure class=align-center><img loading=lazy src=/imgs/vision/imagen.jpeg#center alt="Imagen model architecture. Source: official website"><figcaption><p>Imagen model architecture. Source: <a href=https://imagen.research.google/>official website</a></p></figcaption></figure><p>Here are some open-source implementations of Imagen:</p><ul><li><a href=https://github.com/lucidrains/imagen-pytorch>lucidrains/imagen-pytorch</a></li><li><a href=https://github.com/deep-floyd/if>deep-floyd/if</a></li><li><a href=https://github.com/AssemblyAI-Examples/MinImagen>AssemblyAI-Examples/MinImagen</a></li><li><a href=https://github.com/TeaPearce/Conditional_Diffusion_MNIST>Conditional_Diffusion_MNIST</a></li></ul><h3 id=latent-diffusion-model>Latent Diffusion Model<a hidden class=anchor aria-hidden=true href=#latent-diffusion-model>#</a></h3><p>Diffusion in pixel space is too expensive, because there are too many dimensions to keep track of: 512x512 color images have 512x512x3 = 786,432 dimensions, and 1024x1024 images have more than 3 million dimensions. In addition, information in pixel space is often very redundant, containing details that are not semantically important.</p><p>Latent Diffusion Models (LDM)
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#ldm><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Robin"><span itemprop=familyName>Rombach</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Andreas"><span itemprop=familyName>Blattmann</span></span>
<em>& al.</em>, <span itemprop=datePublished>2022</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rombach</span>, <meta itemprop=givenName content="Robin">R.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Blattmann</span>, <meta itemprop=givenName content="Andreas">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lorenz</span>, <meta itemprop=givenName content="Dominik">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Esser</span>, <meta itemprop=givenName content="Patrick">P.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ommer</span>, <meta itemprop=givenName content="Björn">B.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name><i>High-resolution image synthesis with latent diffusion models</i></span>.</span>
</span></span>)</span>
, also known as <em>stable diffusion</em>, map images to latent space with a <strong>trained</strong> autoencoder <strong>VQ-GAN</strong> $(\mathcal{E}, \mathcal{D})$, and learn U-Net backbones to reverse diffusions (noise-adding) in latent space. This has the benefit of reducing computational cost, and letting the model focus on semantics over details.</p><blockquote><p>Note that the first author of LDM, Robin Rombach, is also the second author of VQ-GAN.</p></blockquote><blockquote><p>U-Net
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#u-net><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Olaf"><span itemprop=familyName>Ronneberger</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Philipp"><span itemprop=familyName>Fischer</span></span>
<em>& al.</em>, <span itemprop=datePublished>2015</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ronneberger</span>, <meta itemprop=givenName content="Olaf">O.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Fischer</span>, <meta itemprop=givenName content="Philipp">P.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Brox</span>, <meta itemprop=givenName content="Thomas">T.</span>
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name><i>U-net: Convolutional networks for biomedical image segmentation</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>Springer</span></span>.</span>
</span></span>)</span>
is a convolutional neural network model that was first proposed to solve medical image segmentation task, where data size is small. For example, in the original paper, it was trained on a dataset of 30 images (512x512 pixels), and another dataset with 35 images. Recently, however, the U-Net backbone used in diffusion models is being replaced by the Transformer architecture.</p></blockquote><p>An $H\times W\times 3$ image $x\in\mathbb{R}^{H\times W\times 3}$ will be mapped to $h\times w\times c$ latent tensor $z\in\mathbb{R}^{h\times w\times c}$. Some specific examples of $h\times w\times c$ are</p><ul><li>64 x 64 x 3 (12,288 dims in total),</li><li>32 x 32 x 4 (4096 dims in total),</li><li>16 x 16 x 8 (2048 dims in total).</li></ul><p>For conditional generation, text and image prompts are projected to vectors via an encoder $\tau_\theta$, then fused with U-Net intermediate layers via cross-attention. The loss is now</p><p>$$\ell = \mathbb{E}_{y, \epsilon\sim N(0,1), t}\| \epsilon_\theta(z_t, t, \tau_\theta(y)) - \epsilon\|_2^2.$$</p><figure class=align-center><img loading=lazy src=/imgs/vision/ldm.png#center alt="Latent Diffusion Model (LDM)  architecture. Source: paper"><figcaption><p>Latent Diffusion Model (LDM) architecture. Source: paper</p></figcaption></figure><h3 id=controlnet>ControlNet<a hidden class=anchor aria-hidden=true href=#controlnet>#</a></h3><p>ControlNet
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#controlnet><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Lvmin"><span itemprop=familyName>Zhang</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Anyi"><span itemprop=familyName>Rao</span></span>
<em>& al.</em>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Lvmin">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rao</span>, <meta itemprop=givenName content="Anyi">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Agrawala</span>, <meta itemprop=givenName content="Maneesh">M.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name><i>Adding conditional control to text-to-image diffusion models</i></span>.</span>
</span></span>)</span>
is an adapter-tuned stable diffusion model for generating images via <strong>image</strong> guidance like edge maps, human pose skeletons, segmentation maps, depth, and normals. For each of some U-Net blocks, specifically those encoder blocks, a trainable copy is added with zero convolution to process conditioning images and add the adapter output to the original block output.</p><figure class=align-center><img loading=lazy src=/imgs/vision/controlnet.png#center alt="ControlNet architecture. Source: paper"><figcaption><p>ControlNet architecture. Source: paper</p></figcaption></figure><p>The model is trained on (conditioning image, image) datasets, for example 3M canny edge image-caption pairs, 80K pose-image-caption pairs, 20K segmentation-image-caption pairs, 3M depth-image-caption pairs and more.</p><p>Weights & biases of the zero convolution (1x1 convolution) layers are initialized to zero, so at the start of training, the adapters output zero and the model outputs similar high quality images as the pre-trained SD model. The authors noted an interesting phenomenon of &ldquo;sudden convergence&rdquo;: the model suddenly learns to follow the input condition only at a certain training step.</p><figure class=align-center><img loading=lazy src=/imgs/vision/sudden-convergence.png#center alt="Sudden convergence phenomenon in ControlNet training. Source: paper"><figcaption><p>Sudden convergence phenomenon in ControlNet training. Source: paper</p></figcaption></figure><h3 id=anydoor>Anydoor<a hidden class=anchor aria-hidden=true href=#anydoor>#</a></h3><p>Anydoor
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#anydoor><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Xi"><span itemprop=familyName>Chen</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Lianghua"><span itemprop=familyName>Huang</span></span>
<em>& al.</em>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Xi">X.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Huang</span>, <meta itemprop=givenName content="Lianghua">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Liu</span>, <meta itemprop=givenName content="Yu">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shen</span>, <meta itemprop=givenName content="Yujun">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhao</span>, <meta itemprop=givenName content="Deli">D.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhao</span>, <meta itemprop=givenName content="Hengshuang">H.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Anydoor: Zero-shot object-level image customization</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2307.09481</span></i>.</span>
</span></span>)</span>
is a fine-tuned stable diffusion model developed by researchers from Alibaba for placing objects in an image into another image at a specified location. It targets e-commerce applications:</p><ul><li>It can be used for placing a product into some desired background.</li><li>Given an apparel as the object, and a model picture as the scene image, the model can place the cloth onto the model, generating <strong>try-on</strong> images.</li></ul><figure class=align-center><img loading=lazy src=/imgs/vision/tryon.png#center alt="Anydoor can be applied to e-commerce tryon scenario"><figcaption><p>Anydoor can be applied to e-commerce tryon scenario</p></figcaption></figure><h4 id=architecture-3>Architecture<a hidden class=anchor aria-hidden=true href=#architecture-3>#</a></h4><figure class=align-center><img loading=lazy src=/imgs/vision/anydoor.jpeg#center alt="Anydoor model architecture. Source: paper"><figcaption><p>Anydoor model architecture. Source: paper</p></figcaption></figure><p>The architectural design is as follows: for the object image, first use a (freezed) segmentor model to remove the background, align the object to the image center, then feed the image to an ID extractor (freezed <a href=https://dinov2.metademolab.com/>DINO-v2</a> + trainable linear layer) to extract the features as a
$$
T^{257\times1024} = \mathrm{Linear}(T_g^{1\times1536}, T_p^{256\times1536})
$$
tensor, where $T_g^{1\times1536}$ is a global feature, and $T_p^{256\times1536}$ is a patch feature.</p><p>The extracted ID embeddings lack detail. To preserve high fidelity while also allow for diversity like gesture, lighting, and orientation, a high frequency map is used to extract fine details of the ID image:
$$
I_h = (I\otimes K_h + I\otimes K_v) \odot I \odot M_{\mathrm{erode}}
$$
where $K_h$ and $K_v$ are horizontal and vertical <a href=https://en.wikipedia.org/wiki/Sobel_operator>Sobel kernels</a>, $\otimes$ is convolution, $\odot$ is <a href=https://en.wikipedia.org/wiki/Hadamard_product_(matrices)>Hadamard product</a> (element-wise matrix multiplication), $M_{\mathrm{erode}}$ is an eroded mask used to filter out the information near the outer contour of the target object. See below for visualization of this transformation.</p><figure class=align-center><img loading=lazy src=/imgs/vision/anydoor-hf.jpeg#center alt="Visualization of HF maps"><figcaption><p>Visualization of HF maps</p></figcaption></figure><p>The HF-map is then stitched to the scene image at specified location, and a trainable detail extractor extracts features from the stitched image. The ID embeddings and the stitched image embeddings are then fused to the U-Net backbone via cross-attention. The model then output images that should be a harmonic composition of the object at the scene. The ID embeddings are injected into each U-Net layer, while the detail maps are concatenated with U-Net decoder features at each resolution. The U-Net encoder is freezed and only the U-Net decoder is trained.</p><h4 id=training-4>Training<a hidden class=anchor aria-hidden=true href=#training-4>#</a></h4><ol><li><p><strong>Dataset.</strong> The training data consists of 23,783 videos, 137,105 multi-view images, and 249,606 single-view images. The ideal training samples are image pairs for “the same object in different scenes”, which are difficult to obtain. To deal with this problem, the authors utilize video datasets to capture different frames containing the same object. Given a clip, the authors first sample two frames and segment the instances within each frame. Then, one instance from one frame is selected as the target object, and the same instance on the other frame is treated as the training supervision (i.e., the desired model output).</p><p>it is observed that initial denoising steps of a diffusion model often focus on generating the overall structure, while later steps cover fine details like textures and colors. Thus, video data is sampled with more chance in early denoising steps (large T) to better learn the appearance changes, and image data is sampled with more chance in late denoising steps (small T) to learn fine details.</p></li><li><p><strong>Code.</strong> <a href=https://github.com/ali-vilab/AnyDoor>ali-vilab/AnyDoor</a></p></li></ol><h3 id=ip-adapter>IP-Adapter<a hidden class=anchor aria-hidden=true href=#ip-adapter>#</a></h3><p>Given an input human face image and a prompt, the IP-Adapter model
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#ip-adapter><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Hu"><span itemprop=familyName>Ye</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Jun"><span itemprop=familyName>Zhang</span></span>
<em>& al.</em>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ye</span>, <meta itemprop=givenName content="Hu">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Jun">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Liu</span>, <meta itemprop=givenName content="Sibo">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Han</span>, <meta itemprop=givenName content="Xiao">X.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span>, <meta itemprop=givenName content="Wei">W.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2308.06721</span></i>.</span>
</span></span>)</span>
generates output image of that character under the scene described by the prompt. It is an adapter fine-tuned stable diffusion model. An Image encoder is added as trainable layers to process input images. Input image embeddings and text embeddings are fused via cross attention to the denoising U-Net backbone. The model has around 22M parameters.</p><h4 id=architecture-4>Architecture<a hidden class=anchor aria-hidden=true href=#architecture-4>#</a></h4><figure class=align-center><img loading=lazy src=/imgs/vision/ip-adapter.jpeg#center alt="IP-Adapter model architecture. Source: paper"><figcaption><p>IP-Adapter model architecture. Source: paper</p></figcaption></figure><h4 id=training-5>Training<a hidden class=anchor aria-hidden=true href=#training-5>#</a></h4><ol><li><strong>Loss.</strong> Same with stable diffusion, with classifier-free guidance.</li><li><strong>Dataset.</strong> 10M text-image pairs from two open source datasets, LAION-2B and COYO-700M. Images are resized to 512 × 512 resolution.</li><li><strong>Hardware.</strong> Trained on a single machine with 8 V100 GPUs for 1M steps.</li><li><strong>Code.</strong> <a href=https://github.com/tencent-ailab/IP-Adapter>tencent-ailab/IP-Adapter</a></li></ol><h3 id=photomaker>PhotoMaker<a hidden class=anchor aria-hidden=true href=#photomaker>#</a></h3><p>As with IP-Adapter, the goal of the PhotoMaker model
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#photomaker><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Zhen"><span itemprop=familyName>Li</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Mingdeng"><span itemprop=familyName>Cao</span></span>
<em>& al.</em>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Zhen">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cao</span>, <meta itemprop=givenName content="Mingdeng">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wang</span>, <meta itemprop=givenName content="Xintao">X.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Qi</span>, <meta itemprop=givenName content="Zhongang">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cheng</span>, <meta itemprop=givenName content="Ming-Ming">M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shan</span>, <meta itemprop=givenName content="Ying">Y.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>PhotoMaker: Customizing realistic human photos via stacked ID embedding</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2312.04461</span></i>.</span>
</span></span>)</span>
is to re-draw the background and/or style described by text prompts while preserving the facial identity of a set of reference human images. Design of the model architecture comes from the observation that pre-trained CLIP models may associate words like &ldquo;woman&rdquo;, &ldquo;man&rdquo;, &ldquo;girl&rdquo; and &ldquo;boy&rdquo; to humans, and thus fusing those words&rsquo; embeddings with the image could guide the diffusion model to preserve the subject in the image.</p><p>After training, those models can output personalized image given an input reference image at reference time, without having to perform fine-tuning for each input instance. This is in contrast with earlier models like DreamBooth
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#dreambooth><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Nataniel"><span itemprop=familyName>Ruiz</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Yuanzhen"><span itemprop=familyName>Li</span></span>
<em>& al.</em>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ruiz</span>, <meta itemprop=givenName content="Nataniel">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Yuanzhen">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jampani</span>, <meta itemprop=givenName content="Varun">V.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Pritch</span>, <meta itemprop=givenName content="Yael">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rubinstein</span>, <meta itemprop=givenName content="Michael">M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Aberman</span>, <meta itemprop=givenName content="Kfir">K.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name><i>Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation</i></span>.</span>
</span></span>)</span>
, which fine-tunes diffusion model on each batch of reference images for personalization, which is expensive.</p><h4 id=architecture-5>Architecture<a hidden class=anchor aria-hidden=true href=#architecture-5>#</a></h4><p>A CLIP image encoder $\mathcal{E}_{img}$ is used to map $N$ human images supplied by the user to image embeddings $\{e^i\in\mathbb{R}^D\}_{i=1\ldots N}$, and a CLIP text encoder is used to map text prompt tokens of length $L$ to text embeddings $t\in\mathbb{R}^{L\times D}$. Non-human parts of input images are filled with noises to remove influence of the background, and additional trainable layers are added to the image encoder to fine-tune for such non-natural inputs.</p><figure class=align-center><img loading=lazy src=/imgs/vision/photomaker.jpeg#center alt="PhotoMaker model architecture"><figcaption><p>PhotoMaker model architecture</p></figcaption></figure><p>During inference, a &ldquo;class&rdquo; word from a set like {woman, man, girl, boy, young, old} is required in the text prompt. The text embedding $t_c\in\mathbb{R}^D$ that corresponds to the class word will be extracted and fused with each image embedding $e^i, i=1\ldots N$ through MLPs to get updated image embeddings $\{\hat{e}^i\in\mathbb{R}^D\}_{i=1\ldots N}$. We then concatenate them to get the concatenated embedding (called &ldquo;stacked id embedding&rdquo; in the paper)
$$
s = (\hat{e}^1,\ldots,\hat{e}^N)\in\mathbb{R}^{N\times D}.
$$</p><p>The conditional used to perform cross-attentions with the U-Net is defined as</p><p>$$
t^* = [t_1,\ldots, s, \ldots, t_L]\in\mathbb{R}^{(L-1 + N)\times D}.
$$</p><p>Where $s$ replaces the vector corresponding to the class word $t_c$. Specifically, the cross-attention calculation is
$$
Q = W_Q\cdot\phi(z_t)
$$
$$
K = W_K\cdot t^*
$$
$$
V = W_V \cdot t^*
$$
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)\cdot V
$$
Attention matrices are fine-tuned by LoRA. The rest of the diffusion model remains unchanged.</p><h4 id=training-6>Training<a hidden class=anchor aria-hidden=true href=#training-6>#</a></h4><ol><li><strong>Dataset.</strong> A dataset of 112K+ (image, caption, ID mask) triples of 13,000 people is collected, for an average of 8.6 pictures per person.
The diffusion model used is SDXL
<span class=hugo-cite-intext itemprop=citation>(<span class=hugo-cite-group>
<a href=#sdxl><span class=visually-hidden>Citation: </span><span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Dustin"><span itemprop=familyName>Podell</span></span>, <span itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=givenName content="Zion"><span itemprop=familyName>English</span></span>
<em>& al.</em>, <span itemprop=datePublished>2023</span></a><span class=hugo-cite-citation>
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Podell</span>, <meta itemprop=givenName content="Dustin">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>English</span>, <meta itemprop=givenName content="Zion">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lacey</span>, <meta itemprop=givenName content="Kyle">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Blattmann</span>, <meta itemprop=givenName content="Andreas">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dockhorn</span>, <meta itemprop=givenName content="Tim">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Müller</span>, <meta itemprop=givenName content="Jonas">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Penna</span>, <meta itemprop=givenName content="Joe">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rombach</span>, <meta itemprop=givenName content="Robin">R.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>SDXL: Improving latent diffusion models for high-resolution image synthesis</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2307.01952</span></i>.</span>
</span></span>)</span>
, resolution of training data is 1024x1024.</li><li><strong>Hardware.</strong> The authors reported training on 8 Nvidia A100 GPUs for two weeks with a batch size of 48. For inference, a minimum of 15GB GPU memory is required.</li><li><strong>Test.</strong> The model shows impressive ability to preserve features presented in input image, like general facial shapes. But quite often the generated person is &ldquo;different&rdquo;. Rather, it looks like some kind of avarage of training images. In addition, there are noticeable artifacts in teeth and ears.<figure class=align-center><img loading=lazy src=/imgs/vision/huiwen-chang.jpg#center alt="PhotoMaker test. The left one is the input image (Huiwen Chang). The middle one is generated with the prompt &amp;lsquo;an asian woman img sitting at the beach, with purple sunset&amp;rsquo;. The third one is generated with the prompt &amp;lsquo;An asian woman in front of a fountain at a garden img&amp;rsquo;. Source: author"><figcaption><p>PhotoMaker test. The left one is the input image (Huiwen Chang). The middle one is generated with the prompt &lsquo;an asian woman img sitting at the beach, with purple sunset&rsquo;. The third one is generated with the prompt &lsquo;An asian woman in front of a fountain at a garden img&rsquo;. Source: author</p></figcaption></figure></li><li><strong>Code.</strong> <a href=https://github.com/TencentARC/PhotoMaker>TencentARC/PhotoMaker</a></li></ol><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><section class=hugo-cite-bibliography><div style=margin-bottom:2pt id=anydoor>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Xi">X.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Huang</span>, <meta itemprop=givenName content="Lianghua">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Liu</span>, <meta itemprop=givenName content="Yu">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shen</span>, <meta itemprop=givenName content="Yujun">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhao</span>, <meta itemprop=givenName content="Deli">D.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhao</span>, <meta itemprop=givenName content="Hengshuang">H.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Anydoor: Zero-shot object-level image customization</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2307.09481</span></i>.</span></div><div style=margin-bottom:2pt id=clip>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Radford</span>, <meta itemprop=givenName content="Alec">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kim</span>, <meta itemprop=givenName content="Jong Wook">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Hallacy</span>, <meta itemprop=givenName content="Chris">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ramesh</span>, <meta itemprop=givenName content="Aditya">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goh</span>, <meta itemprop=givenName content="Gabriel">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Agarwal</span>, <meta itemprop=givenName content="Sandhini">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sastry</span>, <meta itemprop=givenName content="Girish">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Askell</span>, <meta itemprop=givenName content="Amanda">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Mishkin</span>, <meta itemprop=givenName content="Pamela">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Clark</span>, <meta itemprop=givenName content="Jack">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name><i>Learning transferable visual models from natural language supervision</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>PMLR</span></span>.</span></div><div style=margin-bottom:2pt id=controlnet>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Lvmin">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rao</span>, <meta itemprop=givenName content="Anyi">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Agrawala</span>, <meta itemprop=givenName content="Maneesh">M.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name><i>Adding conditional control to text-to-image diffusion models</i></span>.</span></div><div style=margin-bottom:2pt id=ddpm>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ho</span>, <meta itemprop=givenName content="Jonathan">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jain</span>, <meta itemprop=givenName content="Ajay">A.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Abbeel</span>, <meta itemprop=givenName content="Pieter">P.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>Denoising diffusion probabilistic models</span>.<i>
<span itemprop=about>Advances in neural information processing systems</span>, 33</i>. <span itemprop=pagination>6840–6851</span>.</span></div><div style=margin-bottom:2pt id=dice>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Milletari</span>, <meta itemprop=givenName content="Fausto">F.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Navab</span>, <meta itemprop=givenName content="Nassir">N.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ahmadi</span>, <meta itemprop=givenName content="Seyed-Ahmad">S.</span>
 
(<span itemprop=datePublished>2016</span>).
 <span itemprop=name><i>V-net: Fully convolutional neural networks for volumetric medical image segmentation</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>Ieee</span></span>.</span></div><div style=margin-bottom:2pt id=diffusion>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sohl-Dickstein</span>, <meta itemprop=givenName content="Jascha">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Weiss</span>, <meta itemprop=givenName content="Eric">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Maheswaranathan</span>, <meta itemprop=givenName content="Niru">N.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ganguli</span>, <meta itemprop=givenName content="Surya">S.</span>
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name><i>Deep unsupervised learning using nonequilibrium thermodynamics</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>PMLR</span></span>.</span></div><div style=margin-bottom:2pt id=dreambooth>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ruiz</span>, <meta itemprop=givenName content="Nataniel">N.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Yuanzhen">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jampani</span>, <meta itemprop=givenName content="Varun">V.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Pritch</span>, <meta itemprop=givenName content="Yael">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rubinstein</span>, <meta itemprop=givenName content="Michael">M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Aberman</span>, <meta itemprop=givenName content="Kfir">K.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name><i>Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation</i></span>.</span></div><div style=margin-bottom:2pt id=focal>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lin</span>, <meta itemprop=givenName content="Tsung-Yi">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Goyal</span>, <meta itemprop=givenName content="Priya">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Girshick</span>, <meta itemprop=givenName content="Ross">R.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>He</span>, <meta itemprop=givenName content="Kaiming">K.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dollár</span>, <meta itemprop=givenName content="Piotr">P.</span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name><i>Focal loss for dense object detection</i></span>.</span></div><div style=margin-bottom:2pt id=imagen>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Saharia</span>, <meta itemprop=givenName content="Chitwan">C.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chan</span>, <meta itemprop=givenName content="William">W.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Saxena</span>, <meta itemprop=givenName content="Saurabh">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Lala">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Whang</span>, <meta itemprop=givenName content="Jay">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Denton</span>, <meta itemprop=givenName content="Emily L">E.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ghasemipour</span>, <meta itemprop=givenName content="Kamyar">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gontijo Lopes</span>, <meta itemprop=givenName content="Raphael">R.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Karagol Ayan</span>, <meta itemprop=givenName content="Burcu">B.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Salimans</span>, <meta itemprop=givenName content="Tim">T.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name>Photorealistic text-to-image diffusion models with deep language understanding</span>.<i>
<span itemprop=about>Advances in Neural Information Processing Systems</span>, 35</i>. <span itemprop=pagination>36479–36494</span>.</span></div><div style=margin-bottom:2pt id=ip-adapter>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ye</span>, <meta itemprop=givenName content="Hu">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Jun">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Liu</span>, <meta itemprop=givenName content="Sibo">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Han</span>, <meta itemprop=givenName content="Xiao">X.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span>, <meta itemprop=givenName content="Wei">W.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2308.06721</span></i>.</span></div><div style=margin-bottom:2pt id=ldm>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rombach</span>, <meta itemprop=givenName content="Robin">R.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Blattmann</span>, <meta itemprop=givenName content="Andreas">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lorenz</span>, <meta itemprop=givenName content="Dominik">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Esser</span>, <meta itemprop=givenName content="Patrick">P.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ommer</span>, <meta itemprop=givenName content="Björn">B.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name><i>High-resolution image synthesis with latent diffusion models</i></span>.</span></div><div style=margin-bottom:2pt id=mae>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>He</span>, <meta itemprop=givenName content="Kaiming">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chen</span>, <meta itemprop=givenName content="Xinlei">X.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Xie</span>, <meta itemprop=givenName content="Saining">S.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Yanghao">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dollár</span>, <meta itemprop=givenName content="Piotr">P.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Girshick</span>, <meta itemprop=givenName content="Ross">R.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name><i>Masked autoencoders are scalable vision learners</i></span>.</span></div><div style=margin-bottom:2pt id=maskgit>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chang</span>, <meta itemprop=givenName content="Huiwen">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Han">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jiang</span>, <meta itemprop=givenName content="Lu">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Liu</span>, <meta itemprop=givenName content="Ce">C.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Freeman</span>, <meta itemprop=givenName content="William T">W.</span>
 
(<span itemprop=datePublished>2022</span>).
 <span itemprop=name><i>Maskgit: Masked generative image transformer</i></span>.</span></div><div style=margin-bottom:2pt id=muse>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Chang</span>, <meta itemprop=givenName content="Huiwen">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhang</span>, <meta itemprop=givenName content="Han">H.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Barber</span>, <meta itemprop=givenName content="Jarred">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Maschinot</span>, <meta itemprop=givenName content="AJ">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lezama</span>, <meta itemprop=givenName content="Jose">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Jiang</span>, <meta itemprop=givenName content="Lu">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Yang</span>, <meta itemprop=givenName content="Ming-Hsuan">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Murphy</span>, <meta itemprop=givenName content="Kevin">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Freeman</span>, <meta itemprop=givenName content="William T">W.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rubinstein</span>, <meta itemprop=givenName content="Michael">M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>Muse: Text-to-image generation via masked generative transformers</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2301.00704</span></i>.</span></div><div style=margin-bottom:2pt id=photomaker>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Li</span>, <meta itemprop=givenName content="Zhen">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cao</span>, <meta itemprop=givenName content="Mingdeng">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Wang</span>, <meta itemprop=givenName content="Xintao">X.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Qi</span>, <meta itemprop=givenName content="Zhongang">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Cheng</span>, <meta itemprop=givenName content="Ming-Ming">M.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Shan</span>, <meta itemprop=givenName content="Ying">Y.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>PhotoMaker: Customizing realistic human photos via stacked ID embedding</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2312.04461</span></i>.</span></div><div style=margin-bottom:2pt id=sdxl>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Podell</span>, <meta itemprop=givenName content="Dustin">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>English</span>, <meta itemprop=givenName content="Zion">Z.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Lacey</span>, <meta itemprop=givenName content="Kyle">K.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Blattmann</span>, <meta itemprop=givenName content="Andreas">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dockhorn</span>, <meta itemprop=givenName content="Tim">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Müller</span>, <meta itemprop=givenName content="Jonas">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Penna</span>, <meta itemprop=givenName content="Joe">J.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rombach</span>, <meta itemprop=givenName content="Robin">R.</span>
 
(<span itemprop=datePublished>2023</span>).
 <span itemprop=name>SDXL: Improving latent diffusion models for high-resolution image synthesis</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2307.01952</span></i>.</span></div><div style=margin-bottom:2pt id=song>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Song</span>, <meta itemprop=givenName content="Yang">Y.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Sohl-Dickstein</span>, <meta itemprop=givenName content="Jascha">J.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kingma</span>, <meta itemprop=givenName content="Diederik P">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kumar</span>, <meta itemprop=givenName content="Abhishek">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ermon</span>, <meta itemprop=givenName content="Stefano">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Poole</span>, <meta itemprop=givenName content="Ben">B.</span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>Score-based generative modeling through stochastic differential equations</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2011.13456</span></i>.</span></div><div style=margin-bottom:2pt id=u-net>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ronneberger</span>, <meta itemprop=givenName content="Olaf">O.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Fischer</span>, <meta itemprop=givenName content="Philipp">P.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Brox</span>, <meta itemprop=givenName content="Thomas">T.</span>
 
(<span itemprop=datePublished>2015</span>).
 <span itemprop=name><i>U-net: Convolutional networks for biomedical image segmentation</i></span>.
 
<span itemprop=publisher itemtype=http://schema.org/Organization itemscope><span itemprop=name>Springer</span></span>.</span></div><div style=margin-bottom:2pt id=vit>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dosovitskiy</span>, <meta itemprop=givenName content="Alexey">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Beyer</span>, <meta itemprop=givenName content="Lucas">L.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Kolesnikov</span>, <meta itemprop=givenName content="Alexander">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Weissenborn</span>, <meta itemprop=givenName content="Dirk">D.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Zhai</span>, <meta itemprop=givenName content="Xiaohua">X.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Unterthiner</span>, <meta itemprop=givenName content="Thomas">T.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Dehghani</span>, <meta itemprop=givenName content="Mostafa">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Minderer</span>, <meta itemprop=givenName content="Matthias">M.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Heigold</span>, <meta itemprop=givenName content="Georg">G.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Gelly</span>, <meta itemprop=givenName content="Sylvain">S.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2020</span>).
 <span itemprop=name>An image is worth 16x16 words: Transformers for image recognition at scale</span>.<i>
<span itemprop=about>arXiv preprint arXiv:2010.11929</span></i>.</span></div><div style=margin-bottom:2pt id=vq-gan>◎
<span itemscope itemtype=https://schema.org/CreativeWork data-type=paper-conference><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Esser</span>, <meta itemprop=givenName content="Patrick">P.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Rombach</span>, <meta itemprop=givenName content="Robin">R.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Ommer</span>, <meta itemprop=givenName content="Bjorn">B.</span>
 
(<span itemprop=datePublished>2021</span>).
 <span itemprop=name><i>Taming transformers for high-resolution image synthesis</i></span>.</span></div><div style=margin-bottom:2pt id=vq-vae>◎
<span itemscope itemtype=https://schema.org/Article data-type=article><span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Van Den Oord</span>, <meta itemprop=givenName content="Aaron">A.</span>, 
<span itemprop=author itemscope itemtype=https://schema.org/Person><span itemprop=familyName>Vinyals</span>, <meta itemprop=givenName content="Oriol">O.</span> & <span itemprop=author itemscope itemtype=https://schema.org/Person></span>
 
(<span itemprop=datePublished>2017</span>).
 <span itemprop=name>Neural discrete representation learning</span>.<i>
<span itemprop=about>Advances in neural information processing systems</span>, 30</i>.</span></div></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://lifei.ai/tags/vision/>vision</a></li><li><a href=https://lifei.ai/tags/diffusion/>diffusion</a></li><li><a href=https://lifei.ai/tags/transformer/>transformer</a></li></ul><nav class=paginav><a class=prev href=https://lifei.ai/posts/2024-11-26-options/><span class=title>« Prev</span><br><span>Options Trading and the Black-Scholes Model</span></a>
<a class=next href=https://lifei.ai/posts/2023-08-31-better-transformers/><span class=title>Next »</span><br><span>Better Transformers</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Vision Models on x" href="https://x.com/intent/tweet/?text=Vision%20Models&amp;url=https%3a%2f%2flifei.ai%2fposts%2f2024-02-12-vision-models%2f&amp;hashtags=vision%2cdiffusion%2ctransformer"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vision Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flifei.ai%2fposts%2f2024-02-12-vision-models%2f&amp;title=Vision%20Models&amp;summary=Vision%20Models&amp;source=https%3a%2f%2flifei.ai%2fposts%2f2024-02-12-vision-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vision Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flifei.ai%2fposts%2f2024-02-12-vision-models%2f&title=Vision%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vision Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flifei.ai%2fposts%2f2024-02-12-vision-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vision Models on whatsapp" href="https://api.whatsapp.com/send?text=Vision%20Models%20-%20https%3a%2f%2flifei.ai%2fposts%2f2024-02-12-vision-models%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vision Models on telegram" href="https://telegram.me/share/url?text=Vision%20Models&amp;url=https%3a%2f%2flifei.ai%2fposts%2f2024-02-12-vision-models%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vision Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=Vision%20Models&u=https%3a%2f%2flifei.ai%2fposts%2f2024-02-12-vision-models%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://lifei.ai/>Fei Li</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"auto"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>